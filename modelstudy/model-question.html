<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>1. 说一下ChatGPT的优缺点 | 寒梦的博客</title>
    <meta name="generator" content="VuePress 1.9.4">
    <link rel="icon" href="/hmblog/logo.png">
    <meta name="description" content="宝剑锋从磨砺出，梅花香自苦寒来。">
    
    <link rel="preload" href="/hmblog/assets/css/0.styles.b92e6d01.css" as="style"><link rel="preload" href="/hmblog/assets/js/app.ba48173d.js" as="script"><link rel="preload" href="/hmblog/assets/js/3.ac02bfbd.js" as="script"><link rel="preload" href="/hmblog/assets/js/1.ba4d6411.js" as="script"><link rel="preload" href="/hmblog/assets/js/59.ec40cafa.js" as="script"><link rel="preload" href="/hmblog/assets/js/9.d5a05a45.js" as="script"><link rel="prefetch" href="/hmblog/assets/js/10.cdd9f1fd.js"><link rel="prefetch" href="/hmblog/assets/js/100.6bd782a8.js"><link rel="prefetch" href="/hmblog/assets/js/101.aa82f15d.js"><link rel="prefetch" href="/hmblog/assets/js/102.02a5e952.js"><link rel="prefetch" href="/hmblog/assets/js/103.f64270f5.js"><link rel="prefetch" href="/hmblog/assets/js/104.28c05f95.js"><link rel="prefetch" href="/hmblog/assets/js/105.4a3c3d99.js"><link rel="prefetch" href="/hmblog/assets/js/106.d7163a41.js"><link rel="prefetch" href="/hmblog/assets/js/107.03567d93.js"><link rel="prefetch" href="/hmblog/assets/js/108.9c57ec7a.js"><link rel="prefetch" href="/hmblog/assets/js/109.0b5cc849.js"><link rel="prefetch" href="/hmblog/assets/js/11.513bb120.js"><link rel="prefetch" href="/hmblog/assets/js/110.c000ed58.js"><link rel="prefetch" href="/hmblog/assets/js/111.288d96b3.js"><link rel="prefetch" href="/hmblog/assets/js/112.8a5358fb.js"><link rel="prefetch" href="/hmblog/assets/js/113.a1de3201.js"><link rel="prefetch" href="/hmblog/assets/js/114.d8618f6d.js"><link rel="prefetch" href="/hmblog/assets/js/115.17026eb3.js"><link rel="prefetch" href="/hmblog/assets/js/116.f0cb16ec.js"><link rel="prefetch" href="/hmblog/assets/js/117.66402938.js"><link rel="prefetch" href="/hmblog/assets/js/118.e16ec272.js"><link rel="prefetch" href="/hmblog/assets/js/119.53ee2325.js"><link rel="prefetch" href="/hmblog/assets/js/12.6eb5f340.js"><link rel="prefetch" href="/hmblog/assets/js/120.b2631c22.js"><link rel="prefetch" href="/hmblog/assets/js/121.9b598b7e.js"><link rel="prefetch" href="/hmblog/assets/js/122.05f086d4.js"><link rel="prefetch" href="/hmblog/assets/js/123.ee628a51.js"><link rel="prefetch" href="/hmblog/assets/js/124.3309b897.js"><link rel="prefetch" href="/hmblog/assets/js/125.8ba7f05f.js"><link rel="prefetch" href="/hmblog/assets/js/126.363ab065.js"><link rel="prefetch" href="/hmblog/assets/js/127.3dc43128.js"><link rel="prefetch" href="/hmblog/assets/js/128.ead549d4.js"><link rel="prefetch" href="/hmblog/assets/js/129.0219dd0d.js"><link rel="prefetch" href="/hmblog/assets/js/13.41ba4240.js"><link rel="prefetch" href="/hmblog/assets/js/130.50036dbd.js"><link rel="prefetch" href="/hmblog/assets/js/131.43d759e2.js"><link rel="prefetch" href="/hmblog/assets/js/132.73426bd0.js"><link rel="prefetch" href="/hmblog/assets/js/133.bf6d3d08.js"><link rel="prefetch" href="/hmblog/assets/js/134.2eb9f09e.js"><link rel="prefetch" href="/hmblog/assets/js/135.06734521.js"><link rel="prefetch" href="/hmblog/assets/js/136.82c90848.js"><link rel="prefetch" href="/hmblog/assets/js/137.8569b3cd.js"><link rel="prefetch" href="/hmblog/assets/js/138.8e5559db.js"><link rel="prefetch" href="/hmblog/assets/js/139.d7e6536d.js"><link rel="prefetch" href="/hmblog/assets/js/14.bd0b82cb.js"><link rel="prefetch" href="/hmblog/assets/js/140.8481dfb3.js"><link rel="prefetch" href="/hmblog/assets/js/15.491bfa3e.js"><link rel="prefetch" href="/hmblog/assets/js/16.00b7c2a7.js"><link rel="prefetch" href="/hmblog/assets/js/17.2edfa6dd.js"><link rel="prefetch" href="/hmblog/assets/js/18.c1ed5355.js"><link rel="prefetch" href="/hmblog/assets/js/19.127093b0.js"><link rel="prefetch" href="/hmblog/assets/js/20.13018e45.js"><link rel="prefetch" href="/hmblog/assets/js/21.af0d47de.js"><link rel="prefetch" href="/hmblog/assets/js/22.94430091.js"><link rel="prefetch" href="/hmblog/assets/js/23.7f4160d8.js"><link rel="prefetch" href="/hmblog/assets/js/24.53f66321.js"><link rel="prefetch" href="/hmblog/assets/js/25.70c7adf7.js"><link rel="prefetch" href="/hmblog/assets/js/26.ed970703.js"><link rel="prefetch" href="/hmblog/assets/js/27.61f08265.js"><link rel="prefetch" href="/hmblog/assets/js/28.47f448c8.js"><link rel="prefetch" href="/hmblog/assets/js/29.3a6d76f5.js"><link rel="prefetch" href="/hmblog/assets/js/30.fcf9b535.js"><link rel="prefetch" href="/hmblog/assets/js/31.8a27a1da.js"><link rel="prefetch" href="/hmblog/assets/js/32.e4efef75.js"><link rel="prefetch" href="/hmblog/assets/js/33.ea12175d.js"><link rel="prefetch" href="/hmblog/assets/js/34.bc951939.js"><link rel="prefetch" href="/hmblog/assets/js/35.adfc7113.js"><link rel="prefetch" href="/hmblog/assets/js/36.f4b05b10.js"><link rel="prefetch" href="/hmblog/assets/js/37.b160f43f.js"><link rel="prefetch" href="/hmblog/assets/js/38.182e0a40.js"><link rel="prefetch" href="/hmblog/assets/js/39.f6688462.js"><link rel="prefetch" href="/hmblog/assets/js/4.e51e222a.js"><link rel="prefetch" href="/hmblog/assets/js/40.9edf4cd4.js"><link rel="prefetch" href="/hmblog/assets/js/41.6ee87788.js"><link rel="prefetch" href="/hmblog/assets/js/42.e15f3316.js"><link rel="prefetch" href="/hmblog/assets/js/43.a703d986.js"><link rel="prefetch" href="/hmblog/assets/js/44.73bbf228.js"><link rel="prefetch" href="/hmblog/assets/js/45.c5ced175.js"><link rel="prefetch" href="/hmblog/assets/js/46.c039688e.js"><link rel="prefetch" href="/hmblog/assets/js/47.85473c5e.js"><link rel="prefetch" href="/hmblog/assets/js/48.011447cd.js"><link rel="prefetch" href="/hmblog/assets/js/49.64ffd976.js"><link rel="prefetch" href="/hmblog/assets/js/5.60398f15.js"><link rel="prefetch" href="/hmblog/assets/js/50.d4938794.js"><link rel="prefetch" href="/hmblog/assets/js/51.13d55290.js"><link rel="prefetch" href="/hmblog/assets/js/52.506ef0e3.js"><link rel="prefetch" href="/hmblog/assets/js/53.fbe62579.js"><link rel="prefetch" href="/hmblog/assets/js/54.72aa5c58.js"><link rel="prefetch" href="/hmblog/assets/js/55.8a3c2734.js"><link rel="prefetch" href="/hmblog/assets/js/56.4d420373.js"><link rel="prefetch" href="/hmblog/assets/js/57.57103446.js"><link rel="prefetch" href="/hmblog/assets/js/58.9aab8a53.js"><link rel="prefetch" href="/hmblog/assets/js/6.775d6c13.js"><link rel="prefetch" href="/hmblog/assets/js/60.05b1112b.js"><link rel="prefetch" href="/hmblog/assets/js/61.5dd88b4a.js"><link rel="prefetch" href="/hmblog/assets/js/62.4bf0f31f.js"><link rel="prefetch" href="/hmblog/assets/js/63.5fa24c27.js"><link rel="prefetch" href="/hmblog/assets/js/64.98804cd8.js"><link rel="prefetch" href="/hmblog/assets/js/65.9d6391ad.js"><link rel="prefetch" href="/hmblog/assets/js/66.afa16de6.js"><link rel="prefetch" href="/hmblog/assets/js/67.23a13c5e.js"><link rel="prefetch" href="/hmblog/assets/js/68.faffe00e.js"><link rel="prefetch" href="/hmblog/assets/js/69.8d2c347b.js"><link rel="prefetch" href="/hmblog/assets/js/7.e91fdee2.js"><link rel="prefetch" href="/hmblog/assets/js/70.bed5cdee.js"><link rel="prefetch" href="/hmblog/assets/js/71.712a04f2.js"><link rel="prefetch" href="/hmblog/assets/js/72.67728c96.js"><link rel="prefetch" href="/hmblog/assets/js/73.a49f5b5b.js"><link rel="prefetch" href="/hmblog/assets/js/74.fa293c83.js"><link rel="prefetch" href="/hmblog/assets/js/75.9452e054.js"><link rel="prefetch" href="/hmblog/assets/js/76.24ad0c90.js"><link rel="prefetch" href="/hmblog/assets/js/77.94b04afb.js"><link rel="prefetch" href="/hmblog/assets/js/78.97c9f7fb.js"><link rel="prefetch" href="/hmblog/assets/js/79.be8996a8.js"><link rel="prefetch" href="/hmblog/assets/js/8.968ba87b.js"><link rel="prefetch" href="/hmblog/assets/js/80.002c12e9.js"><link rel="prefetch" href="/hmblog/assets/js/81.724c88b9.js"><link rel="prefetch" href="/hmblog/assets/js/82.c17fc6b0.js"><link rel="prefetch" href="/hmblog/assets/js/83.b1e421f1.js"><link rel="prefetch" href="/hmblog/assets/js/84.94eacc74.js"><link rel="prefetch" href="/hmblog/assets/js/85.089b8ca8.js"><link rel="prefetch" href="/hmblog/assets/js/86.fb003f4f.js"><link rel="prefetch" href="/hmblog/assets/js/87.b6fd3eaa.js"><link rel="prefetch" href="/hmblog/assets/js/88.d8b79d91.js"><link rel="prefetch" href="/hmblog/assets/js/89.f6d32768.js"><link rel="prefetch" href="/hmblog/assets/js/90.89f31336.js"><link rel="prefetch" href="/hmblog/assets/js/91.47343b87.js"><link rel="prefetch" href="/hmblog/assets/js/92.2e8d45e3.js"><link rel="prefetch" href="/hmblog/assets/js/93.5e7328c6.js"><link rel="prefetch" href="/hmblog/assets/js/94.bde570da.js"><link rel="prefetch" href="/hmblog/assets/js/95.f0d4415e.js"><link rel="prefetch" href="/hmblog/assets/js/96.d48d924f.js"><link rel="prefetch" href="/hmblog/assets/js/97.3cea1091.js"><link rel="prefetch" href="/hmblog/assets/js/98.158fd54d.js"><link rel="prefetch" href="/hmblog/assets/js/99.ca68782b.js">
    <link rel="stylesheet" href="/hmblog/assets/css/0.styles.b92e6d01.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1c636796><div data-v-1c636796><div class="password-shadow password-wrapper-out" style="display:none;" data-v-2c3e9f55 data-v-1c636796 data-v-1c636796><h3 class="title" data-v-2c3e9f55>寒梦的博客</h3> <p class="description" data-v-2c3e9f55>宝剑锋从磨砺出，梅花香自苦寒来。</p> <label id="box" class="inputBox" data-v-2c3e9f55><input type="password" value="" data-v-2c3e9f55> <span data-v-2c3e9f55>Konck! Knock!</span> <button data-v-2c3e9f55>OK</button></label> <div class="footer" data-v-2c3e9f55><span data-v-2c3e9f55><i class="iconfont reco-theme" data-v-2c3e9f55></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-2c3e9f55>vuePress-theme-reco</a></span> <span data-v-2c3e9f55><i class="iconfont reco-copyright" data-v-2c3e9f55></i> <a data-v-2c3e9f55><span data-v-2c3e9f55>寒梦</span>
          
        <!---->
        2025
      </a></span></div></div> <div class="hide" data-v-1c636796><header class="navbar" data-v-1c636796><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hmblog/" class="home-link router-link-active"><!----> <span class="site-name">寒梦的博客</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      强大的MCP
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-knowledge.html" class="nav-link"><i class="undefined"></i>
  MCP 是什么
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/ide-mcp-server.html" class="nav-link"><i class="undefined"></i>
  IDE 使用MCP Server实操
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      Python
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/basic.html" class="nav-link"><i class="undefined"></i>
  Python 基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/data-structure.html" class="nav-link"><i class="undefined"></i>
  Python 数据结构
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/builtin-function.html" class="nav-link"><i class="undefined"></i>
  Python 内置函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/function.html" class="nav-link"><i class="undefined"></i>
  Python 函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/class.html" class="nav-link"><i class="undefined"></i>
  Python 类
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/module.html" class="nav-link"><i class="undefined"></i>
  Python 模块
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/package.html" class="nav-link"><i class="undefined"></i>
  Python 包
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/exception.html" class="nav-link"><i class="undefined"></i>
  Python 异常
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/file.html" class="nav-link"><i class="undefined"></i>
  Python 文件操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/regex.html" class="nav-link"><i class="undefined"></i>
  Python 正则表达式
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/thread.html" class="nav-link"><i class="undefined"></i>
  Python 多线程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/process.html" class="nav-link"><i class="undefined"></i>
  Python 多进程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/network.html" class="nav-link"><i class="undefined"></i>
  Python 网络编程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/database.html" class="nav-link"><i class="undefined"></i>
  Python 数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/numpy.html" class="nav-link"><i class="undefined"></i>
  Python numpy
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/array-operation.html" class="nav-link"><i class="undefined"></i>
  Python 数组操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-library.html" class="nav-link"><i class="undefined"></i>
  Python 常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/string-function.html" class="nav-link"><i class="undefined"></i>
  Python 字符串函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/practice.html" class="nav-link"><i class="undefined"></i>
  Python 日常练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-pip-install.html" class="nav-link"><i class="undefined"></i>
  pip 那些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/line-continue.html" class="nav-link"><i class="undefined"></i>
  Python 中的行续行符
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/technology-point.html" class="nav-link"><i class="undefined"></i>
  Python 中技术点练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/pandas-study.html" class="nav-link"><i class="undefined"></i>
  pandas 库的使用
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      大模型应用开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/whatIsModel.html" class="nav-link"><i class="undefined"></i>
  什么是大模型应用开发
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-basic.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发构建
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-deploy.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发部署
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vector-database.html" class="nav-link"><i class="undefined"></i>
  向量数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-pytorch.html" class="nav-link"><i class="undefined"></i>
  Fine-tuning框架PyTorch 和 Hugging face 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-code.html" class="nav-link"><i class="undefined"></i>
  手撕Transformer 代码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-knowledge.html" class="nav-link"><i class="undefined"></i>
  理解Transformer 代码必看
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-related.html" class="nav-link"><i class="undefined"></i>
  transformer 相关源码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-transformer.html" class="nav-link"><i class="undefined"></i>
  Transformers实战
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train.html" class="nav-link"><i class="undefined"></i>
  Transformers实战2
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-code.html" class="nav-link"><i class="undefined"></i>
  模型训练代码分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  Transformer 核心组件学习路线
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-question.html" class="nav-link"><i class="undefined"></i>
  Transformer 论文精读中的常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-english.html" class="nav-link"><i class="undefined"></i>
  大模型相关的英语词汇
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-case.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发案例
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-summary.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发总结
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-question.html" aria-current="page" class="nav-link router-link-exact-active router-link-active"><i class="undefined"></i>
  大模型应用常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/impl-transformer.html" class="nav-link"><i class="undefined"></i>
  手撕Transformer
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-RNN.html" class="nav-link"><i class="undefined"></i>
  RNN（循环神经网络）是什么？
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-evaluate.html" class="nav-link"><i class="undefined"></i>
  大模型评估指标
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain-rag.html" class="nav-link"><i class="undefined"></i>
  Langchain &amp; RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain.html" class="nav-link"><i class="undefined"></i>
  Langchain 核心知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag.html" class="nav-link"><i class="undefined"></i>
  RAG 知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-study.html" class="nav-link"><i class="undefined"></i>
  Langchain 学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag-pain.html" class="nav-link"><i class="undefined"></i>
  RAG 痛点分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-agent-practice.html" class="nav-link"><i class="undefined"></i>
  Langchain agent 实战作业二
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/extended-learning.html" class="nav-link"><i class="undefined"></i>
  扩展学习知识
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/self-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling 自己练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  模型微调需要的库或者方法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/lora-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  LoRA 微调
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/agent.html" class="nav-link"><i class="undefined"></i>
  Agent 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-prompt.html" class="nav-link"><i class="undefined"></i>
  练习langchain prompt的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/construct-dataset.html" class="nav-link"><i class="undefined"></i>
  如何构造数据集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/auto-model-desc.html" class="nav-link"><i class="undefined"></i>
  加载模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/compute-metrics.html" class="nav-link"><i class="undefined"></i>
  评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/ner-instruct-task.html" class="nav-link"><i class="undefined"></i>
  NER 任务指令数据构造
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-exercises-code.html" class="nav-link"><i class="undefined"></i>
  Fine-Tuning 实战作业三
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-error-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三错误收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-warn-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三警告收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-f1-scores.html" class="nav-link"><i class="undefined"></i>
  微调实战F1 分数打印
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-analyze.html" class="nav-link"><i class="undefined"></i>
  训练完结果分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step5.html" class="nav-link"><i class="undefined"></i>
  微调实战-step5--数据处理函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step9.html" class="nav-link"><i class="undefined"></i>
  微调实战-step9--评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vLLM-intro.html" class="nav-link"><i class="undefined"></i>
  vLLM
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      算法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/sort.html" class="nav-link"><i class="undefined"></i>
  排序算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/double-pointer.html" class="nav-link"><i class="undefined"></i>
  双指针算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-tree.html" class="nav-link"><i class="undefined"></i>
  二叉树
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/receive-rain.html" class="nav-link"><i class="undefined"></i>
  接雨水
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/dynamic-plan.html" class="nav-link"><i class="undefined"></i>
  动态规划
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/greedy.html" class="nav-link"><i class="undefined"></i>
  贪心算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/longasc-sequence.html" class="nav-link"><i class="undefined"></i>
  最长上升子序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-search.html" class="nav-link"><i class="undefined"></i>
  二分查找
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/reverse-list.html" class="nav-link"><i class="undefined"></i>
  反转链表
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/del-single-list.html" class="nav-link"><i class="undefined"></i>
  删除单链表-集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/other.html" class="nav-link"><i class="undefined"></i>
  其他
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/compare-al.html" class="nav-link"><i class="undefined"></i>
  m个数，最多用n次比较，找出第二大的数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/effect-bracket.html" class="nav-link"><i class="undefined"></i>
  有效的括号
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      其他
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/other/ai-agent.html" class="nav-link"><i class="undefined"></i>
  AI项目
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/python.html" class="nav-link"><i class="undefined"></i>
  python学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/conda.html" class="nav-link"><i class="undefined"></i>
  Conda 使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/using-packages.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/pytorch-know.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  transformer库中那些常用函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-collect.html" class="nav-link"><i class="undefined"></i>
  transformer库学习哪些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-1.html" class="nav-link"><i class="undefined"></i>
  transformer 使用T5模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-2.html" class="nav-link"><i class="undefined"></i>
  torch DataLoader
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-3.html" class="nav-link"><i class="undefined"></i>
  设置随机种子
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/model-refrence.html" class="nav-link"><i class="undefined"></i>
  优秀的参考文档
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/git-operate.html" class="nav-link"><i class="undefined"></i>
  git 操作命令
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/fine-tuning-adapters.html" class="nav-link"><i class="undefined"></i>
  PEFT 之Adapters
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/swanLab-info.html" class="nav-link"><i class="undefined"></i>
  深度学习之SwanLab
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/lora-0-1.html" class="nav-link"><i class="undefined"></i>
  从0到1手撕LoRA类
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      关于我
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/hmyjyghh" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/ghh_" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/cool-62-29/columns" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  知乎
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://hmyjyghh.github.io/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1c636796></div> <aside class="sidebar" data-v-1c636796><div class="personal-info-wrapper" data-v-6f92ba70 data-v-1c636796><!----> <h3 class="name" data-v-6f92ba70>
    寒梦
  </h3> <div class="num" data-v-6f92ba70><div data-v-6f92ba70><h3 data-v-6f92ba70>100</h3> <h6 data-v-6f92ba70>Articles</h6></div> <div data-v-6f92ba70><h3 data-v-6f92ba70>4</h3> <h6 data-v-6f92ba70>Tags</h6></div></div> <ul class="social-links" data-v-6f92ba70></ul> <hr data-v-6f92ba70></div> <nav class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      强大的MCP
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-knowledge.html" class="nav-link"><i class="undefined"></i>
  MCP 是什么
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/ide-mcp-server.html" class="nav-link"><i class="undefined"></i>
  IDE 使用MCP Server实操
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      Python
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/basic.html" class="nav-link"><i class="undefined"></i>
  Python 基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/data-structure.html" class="nav-link"><i class="undefined"></i>
  Python 数据结构
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/builtin-function.html" class="nav-link"><i class="undefined"></i>
  Python 内置函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/function.html" class="nav-link"><i class="undefined"></i>
  Python 函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/class.html" class="nav-link"><i class="undefined"></i>
  Python 类
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/module.html" class="nav-link"><i class="undefined"></i>
  Python 模块
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/package.html" class="nav-link"><i class="undefined"></i>
  Python 包
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/exception.html" class="nav-link"><i class="undefined"></i>
  Python 异常
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/file.html" class="nav-link"><i class="undefined"></i>
  Python 文件操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/regex.html" class="nav-link"><i class="undefined"></i>
  Python 正则表达式
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/thread.html" class="nav-link"><i class="undefined"></i>
  Python 多线程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/process.html" class="nav-link"><i class="undefined"></i>
  Python 多进程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/network.html" class="nav-link"><i class="undefined"></i>
  Python 网络编程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/database.html" class="nav-link"><i class="undefined"></i>
  Python 数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/numpy.html" class="nav-link"><i class="undefined"></i>
  Python numpy
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/array-operation.html" class="nav-link"><i class="undefined"></i>
  Python 数组操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-library.html" class="nav-link"><i class="undefined"></i>
  Python 常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/string-function.html" class="nav-link"><i class="undefined"></i>
  Python 字符串函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/practice.html" class="nav-link"><i class="undefined"></i>
  Python 日常练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-pip-install.html" class="nav-link"><i class="undefined"></i>
  pip 那些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/line-continue.html" class="nav-link"><i class="undefined"></i>
  Python 中的行续行符
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/technology-point.html" class="nav-link"><i class="undefined"></i>
  Python 中技术点练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/pandas-study.html" class="nav-link"><i class="undefined"></i>
  pandas 库的使用
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      大模型应用开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/whatIsModel.html" class="nav-link"><i class="undefined"></i>
  什么是大模型应用开发
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-basic.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发构建
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-deploy.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发部署
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vector-database.html" class="nav-link"><i class="undefined"></i>
  向量数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-pytorch.html" class="nav-link"><i class="undefined"></i>
  Fine-tuning框架PyTorch 和 Hugging face 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-code.html" class="nav-link"><i class="undefined"></i>
  手撕Transformer 代码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-knowledge.html" class="nav-link"><i class="undefined"></i>
  理解Transformer 代码必看
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-related.html" class="nav-link"><i class="undefined"></i>
  transformer 相关源码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-transformer.html" class="nav-link"><i class="undefined"></i>
  Transformers实战
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train.html" class="nav-link"><i class="undefined"></i>
  Transformers实战2
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-code.html" class="nav-link"><i class="undefined"></i>
  模型训练代码分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  Transformer 核心组件学习路线
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-question.html" class="nav-link"><i class="undefined"></i>
  Transformer 论文精读中的常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-english.html" class="nav-link"><i class="undefined"></i>
  大模型相关的英语词汇
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-case.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发案例
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-summary.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发总结
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-question.html" aria-current="page" class="nav-link router-link-exact-active router-link-active"><i class="undefined"></i>
  大模型应用常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/impl-transformer.html" class="nav-link"><i class="undefined"></i>
  手撕Transformer
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-RNN.html" class="nav-link"><i class="undefined"></i>
  RNN（循环神经网络）是什么？
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-evaluate.html" class="nav-link"><i class="undefined"></i>
  大模型评估指标
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain-rag.html" class="nav-link"><i class="undefined"></i>
  Langchain &amp; RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain.html" class="nav-link"><i class="undefined"></i>
  Langchain 核心知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag.html" class="nav-link"><i class="undefined"></i>
  RAG 知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-study.html" class="nav-link"><i class="undefined"></i>
  Langchain 学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag-pain.html" class="nav-link"><i class="undefined"></i>
  RAG 痛点分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-agent-practice.html" class="nav-link"><i class="undefined"></i>
  Langchain agent 实战作业二
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/extended-learning.html" class="nav-link"><i class="undefined"></i>
  扩展学习知识
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/self-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling 自己练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  模型微调需要的库或者方法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/lora-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  LoRA 微调
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/agent.html" class="nav-link"><i class="undefined"></i>
  Agent 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-prompt.html" class="nav-link"><i class="undefined"></i>
  练习langchain prompt的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/construct-dataset.html" class="nav-link"><i class="undefined"></i>
  如何构造数据集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/auto-model-desc.html" class="nav-link"><i class="undefined"></i>
  加载模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/compute-metrics.html" class="nav-link"><i class="undefined"></i>
  评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/ner-instruct-task.html" class="nav-link"><i class="undefined"></i>
  NER 任务指令数据构造
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-exercises-code.html" class="nav-link"><i class="undefined"></i>
  Fine-Tuning 实战作业三
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-error-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三错误收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-warn-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三警告收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-f1-scores.html" class="nav-link"><i class="undefined"></i>
  微调实战F1 分数打印
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-analyze.html" class="nav-link"><i class="undefined"></i>
  训练完结果分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step5.html" class="nav-link"><i class="undefined"></i>
  微调实战-step5--数据处理函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step9.html" class="nav-link"><i class="undefined"></i>
  微调实战-step9--评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vLLM-intro.html" class="nav-link"><i class="undefined"></i>
  vLLM
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      算法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/sort.html" class="nav-link"><i class="undefined"></i>
  排序算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/double-pointer.html" class="nav-link"><i class="undefined"></i>
  双指针算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-tree.html" class="nav-link"><i class="undefined"></i>
  二叉树
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/receive-rain.html" class="nav-link"><i class="undefined"></i>
  接雨水
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/dynamic-plan.html" class="nav-link"><i class="undefined"></i>
  动态规划
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/greedy.html" class="nav-link"><i class="undefined"></i>
  贪心算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/longasc-sequence.html" class="nav-link"><i class="undefined"></i>
  最长上升子序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-search.html" class="nav-link"><i class="undefined"></i>
  二分查找
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/reverse-list.html" class="nav-link"><i class="undefined"></i>
  反转链表
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/del-single-list.html" class="nav-link"><i class="undefined"></i>
  删除单链表-集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/other.html" class="nav-link"><i class="undefined"></i>
  其他
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/compare-al.html" class="nav-link"><i class="undefined"></i>
  m个数，最多用n次比较，找出第二大的数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/effect-bracket.html" class="nav-link"><i class="undefined"></i>
  有效的括号
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      其他
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/other/ai-agent.html" class="nav-link"><i class="undefined"></i>
  AI项目
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/python.html" class="nav-link"><i class="undefined"></i>
  python学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/conda.html" class="nav-link"><i class="undefined"></i>
  Conda 使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/using-packages.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/pytorch-know.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  transformer库中那些常用函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-collect.html" class="nav-link"><i class="undefined"></i>
  transformer库学习哪些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-1.html" class="nav-link"><i class="undefined"></i>
  transformer 使用T5模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-2.html" class="nav-link"><i class="undefined"></i>
  torch DataLoader
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-3.html" class="nav-link"><i class="undefined"></i>
  设置随机种子
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/model-refrence.html" class="nav-link"><i class="undefined"></i>
  优秀的参考文档
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/git-operate.html" class="nav-link"><i class="undefined"></i>
  git 操作命令
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/fine-tuning-adapters.html" class="nav-link"><i class="undefined"></i>
  PEFT 之Adapters
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/swanLab-info.html" class="nav-link"><i class="undefined"></i>
  深度学习之SwanLab
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/lora-0-1.html" class="nav-link"><i class="undefined"></i>
  从0到1手撕LoRA类
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      关于我
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/hmyjyghh" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/ghh_" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/cool-62-29/columns" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  知乎
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://hmyjyghh.github.io/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-2c3e9f55 data-v-1c636796><h3 class="title" data-v-2c3e9f55></h3> <!----> <label id="box" class="inputBox" data-v-2c3e9f55><input type="password" value="" data-v-2c3e9f55> <span data-v-2c3e9f55>Konck! Knock!</span> <button data-v-2c3e9f55>OK</button></label> <div class="footer" data-v-2c3e9f55><span data-v-2c3e9f55><i class="iconfont reco-theme" data-v-2c3e9f55></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-2c3e9f55>vuePress-theme-reco</a></span> <span data-v-2c3e9f55><i class="iconfont reco-copyright" data-v-2c3e9f55></i> <a data-v-2c3e9f55><span data-v-2c3e9f55>寒梦</span>
          
        <!---->
        2025
      </a></span></div></div> <div data-v-1c636796><div data-v-1c636796><main class="page"><section style="display:;"><div class="page-title"><h1 class="title">1. 说一下ChatGPT的优缺点</h1> <div data-v-6acedb3b><i class="iconfont reco-account" data-v-6acedb3b><span data-v-6acedb3b>寒梦</span></i> <!----> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h3 id="_1-说一下chatgpt的优缺点"><a href="#_1-说一下chatgpt的优缺点" class="header-anchor">#</a> 1. 说一下ChatGPT的优缺点</h3> <ol><li>优点：
<ul><li>强大的语言理解和生成能力</li> <li>广泛的应用场景</li> <li>出色的上下文理解和多轮对话能力</li> <li>多功能性与通用性</li> <li>大幅提升效率</li></ul></li> <li>缺点：
<ul><li>模型&quot;幻觉&quot;</li> <li>训练数据有截止日期、知识陈旧与缺乏实时性</li> <li>计算资源需求高</li> <li>可能产生偏见与有害内容</li> <li>在专业领域深度不足</li> <li>上下文长度限制</li> <li>会让人们对它产生依赖，渐渐地缺乏独立思考的能力</li></ul></li></ol> <h3 id="_2-请简述下transformer基本流程"><a href="#_2-请简述下transformer基本流程" class="header-anchor">#</a> 2. 请简述下Transformer基本流程</h3> <blockquote><p>简化流程: 输入 -&gt; 编码（通过自注意力理解源序列） -&gt; 解码（通过交叉注意力参考编码结果，并通过掩码自注意力生成目标序列） -&gt; 输出。</p></blockquote> <ul><li>输入层：将输入的文本序列转换为向量表示</li> <li>编码器层：将输入向量编码为上下文表示</li> <li>解码器层：根据上下文表示生成输出序列</li> <li>输出层：将输出向量转换为文本序列</li> <li>位置编码：为了保留序列中单词的位置信息，引入位置编码</li> <li>自注意力机制：模型通过自注意力机制来学习输入序列中不同位置之间的依赖关系</li> <li>多头注意力机制：模型通过多头注意力机制来学习输入序列中不同位置之间的依赖关系</li> <li>前馈神经网络：每个位置的表示通过前馈神经网络进行处理</li> <li>层归一化：在每个子层之间引入层归一化，提高模型的训练稳定性</li> <li>残差连接：引入残差连接，解决深度神经网络训练中的梯度消失问题</li></ul> <h3 id="_3-为什么基于transformer的架构需要多头注意力机制"><a href="#_3-为什么基于transformer的架构需要多头注意力机制" class="header-anchor">#</a> 3. 为什么基于Transformer的架构需要多头注意力机制？</h3> <ul><li>考察点，多头注意力机制，多头，多个专家，多个角度去分析和理解</li></ul> <blockquote><p>简单来说，<strong>多头机制允许模型同时从不同的“表示子空间”和不同“角度”关注输入信息，极大地增强了模型的表征能力和泛化能力。</strong></p></blockquote> <ol><li>克服单头注意力的局限性：增强模型的“视角”</li> <li>并行捕捉多种类型的关系</li> <li>增加模型的表征能力和稳健性</li></ol> <h4 id="技术实现简述"><a href="#技术实现简述" class="header-anchor">#</a> 技术实现简述</h4> <p>在技术上，多头注意力的实现非常优雅：</p> <ol><li><strong>线性投影</strong>：对于给定的输入，通过 <code>h</code> 个（头的数量）不同的线性投影矩阵，分别生成 <code>h</code> 套 <strong>Query、Key、Value</strong> 向量。</li> <li><strong>并行计算</strong>：在这 <code>h</code> 套投影上并行地执行缩放点积注意力计算，得到 <code>h</code> 个输出矩阵。</li> <li><strong>拼接与融合</strong>：将这 <code>h</code> 个输出矩阵拼接起来，最后通过一个线性层进行融合，将信息整合回原始的维度。</li></ol> <h3 id="_4-编码器-解码器-编解码llm的区别"><a href="#_4-编码器-解码器-编解码llm的区别" class="header-anchor">#</a> 4. 编码器，解码器，编解码LLM的区别？</h3> <ul><li>编码器：将输入序列编码为上下文表示</li> <li>解码器：根据上下文表示生成输出序列</li> <li>编解码LLM：同时进行编码和解码，生成输出序列</li></ul> <table><thead><tr><th style="text-align:left;">特性</th> <th style="text-align:left;">编码器-仅编码</th> <th style="text-align:left;">解码器-仅解码</th> <th style="text-align:left;">编码器-解码器</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>核心注意力</strong></td> <td style="text-align:left;">双向注意力</td> <td style="text-align:left;">因果/掩码注意力</td> <td style="text-align:left;">编码器：双向<br>解码器：因果 + <strong>交叉注意力</strong></td></tr> <tr><td style="text-align:left;"><strong>训练目标</strong></td> <td style="text-align:left;">掩码语言建模</td> <td style="text-align:left;">自回归语言建模</td> <td style="text-align:left;">序列到序列学习</td></tr> <tr><td style="text-align:left;"><strong>信息流</strong></td> <td style="text-align:left;">看到整个输入</td> <td style="text-align:left;">只能看到左侧上下文</td> <td style="text-align:left;">编码器看全源序列，解码器自回归生成目标序列</td></tr> <tr><td style="text-align:left;"><strong>核心能力</strong></td> <td style="text-align:left;"><strong>理解</strong></td> <td style="text-align:left;"><strong>生成</strong></td> <td style="text-align:left;"><strong>转换</strong></td></tr> <tr><td style="text-align:left;"><strong>代表模型</strong></td> <td style="text-align:left;">BERT, RoBERTa</td> <td style="text-align:left;">GPT系列, LLaMA</td> <td style="text-align:left;">T5, BART, 原始Transformer</td></tr> <tr><td style="text-align:left;"><strong>类比</strong></td> <td style="text-align:left;"><strong>阅读理解专家</strong></td> <td style="text-align:left;"><strong>故事讲述者</strong></td> <td style="text-align:left;"><strong>翻译官</strong></td></tr></tbody></table> <h3 id="_5-在语言模型中强化学习的概念-它如何应用于chatgpt"><a href="#_5-在语言模型中强化学习的概念-它如何应用于chatgpt" class="header-anchor">#</a> 5. 在语言模型中强化学习的概念?它如何应用于ChatGPT？</h3> <ul><li>强化学习里面包含，Action、State、Reward Model、Policy、Value Function等</li> <li>Action：模型可以执行的操作</li> <li>State：模型当前的状态</li> <li>Reward Model：用于评估模型执行 Action 后的奖励</li> <li>Policy：模型的策略，用于选择 Action</li> <li>Value Function：模型的价值函数，用于评估模型在当前状态下的价值</li> <li>强化学习，可以根据用户反馈或动作，根据得分及时更新策略，以循序渐进地形式，得到最大期望回报</li> <li>强化学习里面的奖励信号可以是用户的反馈，也可以是模型自己的反馈</li> <li>强化学习是一种通过奖励信号来训练模型的方法</li> <li>强化学习可以应用于ChatGPT，通过与用户的交互来训练模型，使模型能够生成更符合用户需求的文本</li></ul> <h3 id="_6-在gpt模型中-什么是温度系数"><a href="#_6-在gpt模型中-什么是温度系数" class="header-anchor">#</a> 6. 在GPT模型中，什么是温度系数？</h3> <blockquote><p>温度系数是一个控制生成文本随机性和创造性的超参数。</p></blockquote> <ul><li>温度系数是一个超参数，用于控制模型的输出分布的多样性</li> <li>温度系数越小，模型的输出分布越集中，模型的输出结果越确定</li> <li>温度系数越大，模型的输出分布越分散，模型的输出结果越随机</li></ul> <h3 id="_7-什么是旋转位置编码-rotary-position-encoding-简称rope"><a href="#_7-什么是旋转位置编码-rotary-position-encoding-简称rope" class="header-anchor">#</a> 7. 什么是旋转位置编码(Rotary Position Encoding)，简称ROPE？</h3> <ul><li>旋转位置编码是一种用于Transformer模型的位置编码方法</li> <li>旋转位置编码的核心思想是，将位置编码的维度分为两部分，分别对这两部分进行旋转</li> <li>旋转位置编码的优势是，它可以保持模型的平移不变性，同时也可以保持模型的旋转不变性</li> <li>旋转位置编码的劣势是，它的计算复杂度较高</li></ul> <h3 id="_8-为什么现在的大模型大多是decoder-only的架构"><a href="#_8-为什么现在的大模型大多是decoder-only的架构" class="header-anchor">#</a> 8. 为什么现在的大模型大多是decoder-only的架构？</h3> <ul><li><p><strong>Decoder 的天然属性</strong>：Decoder-only 架构本身就是为自回归生成而设计的。它的核心机制是<strong>因果掩码</strong>，确保在生成每个词时，只能看到它之前的词，而无法看到未来的词。这与文本生成的过程完全一致。</p></li> <li><p><strong>计算效率</strong>：Decoder-only 模型架构<strong>高度统一和简化</strong>。无论是预训练还是微调，它都在执行同一个核心任务：预测下一个词。这意味着整个计算图非常高效，可以更好地利用大规模并行计算（如 GPU/TPU）。</p></li> <li><p><strong>参数效率</strong>：在 Encoder-Decoder 架构中，参数被分摊到两个结构不同的组件中。而有研究表明，在计算预算固定的情况下，**将所有参数集中在一个庞大的、统一的 Decoder 中，比将其分割给 Encoder 和 Decoder 能带来更好的性能。**即“大力出奇迹”的策略在 Decoder-only 模型上更有效。</p></li> <li><p>scaling Laws（缩放定律）：OpenAI 等机构提出的缩放定律表明，模型的性能与模型大小、数据量和计算量之间存在可预测的幂律关系。<strong>Decoder-only 架构被证明是沿着这条 scaling law 进行扩展的最直接、最可靠的路径。</strong></p></li></ul> <p>总之，Decoder-only 架构成为主流，本质上<strong>是因为它在“生成能力”这个核心目标上，与任务本身最匹配</strong>，并且在当今“算力为王”的时代，<strong>提供了最优的扩展性、效率和性能平衡。</strong></p> <h3 id="_9-chatgpt的训练步骤有哪些"><a href="#_9-chatgpt的训练步骤有哪些" class="header-anchor">#</a> 9. ChatGPT的训练步骤有哪些？</h3> <p><img src="/hmblog/images/chat-gpt/chat-gpt-train.png" alt="ChatGPT的训练步骤"></p> <ol><li>第1步：预训练 - 构建知识基础</li> <li>第2步：监督微调 - 学会对话格式</li> <li>第3步：从人类反馈中强化学习 - 对齐人类偏好
<ul><li>3.1 训练奖励模型</li> <li>3.2 使用强化学习优化策略</li></ul></li></ol> <h3 id="_10-为什么transformer模型需要位置编码"><a href="#_10-为什么transformer模型需要位置编码" class="header-anchor">#</a> 10. 为什么Transformer模型需要位置编码？</h3> <ul><li>在 Transformer 模型中，由于不是循环（RNN）结构，模型本身无法捕捉输入序列中元素的位置信息。</li> <li>因为Transformer的核心自注意力机制本身是“位置无关”的，它无法感知词语的顺序信息，而顺序对于理解语言至关重要。</li></ul> <h3 id="_11-为什么对于chatgpt而言-提示工程很重要"><a href="#_11-为什么对于chatgpt而言-提示工程很重要" class="header-anchor">#</a> 11. 为什么对于ChatGPT而言，提示工程很重要？</h3> <ul><li>帮助模型理解用户的意图，从而生成符合用户需求的文本</li> <li>设定角色与人格：控制输出的风格和立场</li> <li>提示工程是约束模型、让其忠于事实的强大工具。</li> <li>实现复杂、多步骤的任务分解</li></ul> <p>对于复杂任务，一个简单的指令会让模型不知所措。提示工程教你如何将大任务拆解为清晰的、可执行的步骤。</p> <ul><li><strong>混乱的提示</strong>：<code>&quot;帮我写一份市场计划，要关于新产品，包括社交媒体、预算和竞争对手分析。&quot;</code></li> <li><strong>工程化的提示</strong>：
<ol><li><strong>目标</strong>：<code>&quot;为一款新的环保水瓶制定一份简要的市场计划。&quot;</code></li> <li><strong>步骤一</strong>：<code>&quot;首先，分析当前市场上3个主要竞争对手及其优劣势。&quot;</code></li> <li><strong>步骤二</strong>：<code>&quot;其次，提出一个针对千禧一代的社交媒体推广策略，包括建议使用的平台和内容类型。&quot;</code></li> <li><strong>步骤三</strong>：<code>&quot;最后，草拟一个初步的月度预算分配表。&quot;</code></li></ol></li></ul> <h3 id="_12-如何缓解llms复读机问题"><a href="#_12-如何缓解llms复读机问题" class="header-anchor">#</a> 12. 如何缓解LLMs复读机问题？</h3> <p>要缓解这个问题，我们需要一个多管齐下的策略，主要从<strong>解码策略</strong>、<strong>提示工程</strong>和<strong>模型层面</strong>入手。</p> <p><img src="/hmblog/images/chat-gpt/LLM-repeat.png" alt=""></p> <ul><li>因为LLMs模型的输出结果，是根据输入的提示词，生成的文本</li> <li>所以，为了避免LLMs模型重复输出相同的文本，我们可以在提示词中，加入一些随机的噪声，如添加随机的单词、句子、段落等</li> <li>这样，就可以避免LLMs模型重复输出相同的文本</li> <li>同时，也可以避免LLMs模型输出重复的句子</li></ul> <h2 id="阶段二"><a href="#阶段二" class="header-anchor">#</a> 阶段二</h2> <h3 id="_1-解释下langchain-agent的概念"><a href="#_1-解释下langchain-agent的概念" class="header-anchor">#</a> 1. 解释下langchain Agent的概念？</h3> <ul><li>它的核心能力是调用工具，拥有使用外部函数的能力，使智能Agent成为可能。</li> <li>LLM 作为代理的“大脑”，负责推理、规划和决策，而工具则是它完成任务所依赖的外部手段。</li> <li>具有 记忆能力 和  推理能力</li> <li>LangChain提供了执行器(Agent Executor)，用来运行代理并执行其决策的工具。</li></ul> <h3 id="_2-langchain的6大核心组件是什么-它们的作用分别是什么"><a href="#_2-langchain的6大核心组件是什么-它们的作用分别是什么" class="header-anchor">#</a> 2. langchain的6大核心组件是什么，它们的作用分别是什么？</h3> <ul><li>模型（Models）：包含各大语言模型的LangChain接口和调用细节，以及输出解析机制。</li> <li>提示模板（Prompts）：使提示工程流线化，进一步激发大语言模型的潜力。</li> <li>数据检索（Indexes）：构建并操作文档的方法，接受用户的查询并返回最相关的文档，轻松搭建本地知识库。</li> <li>记忆（Memory）：通过短时记忆和长时记忆，在对话过程中存储和检索数据，让ChatBot记住你。</li> <li>链（Chains），以特定方式封装各种功能，并通过一系列的组合，自动而灵活地完成任务。</li> <li>代理（Agents）：通过“代理”让大模型自主调用外部工具和内部工具，使智能Agent成为可能。</li></ul> <h3 id="_3-langchain有哪些优点和明显的缺点"><a href="#_3-langchain有哪些优点和明显的缺点" class="header-anchor">#</a> 3. langchain有哪些优点和明显的缺点？</h3> <blockquote><p>优点</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>1. 提供了大量的预置组件，让开发者可以快速构建应用
2. 组件生态丰富
3. LangChain提出的Chain、Agent、Tool、Memory等概念已经成为行业标准
4. 社区活跃，文档完善
</code></pre></div><ol><li>快速原型开发能力</li></ol> <ul><li>它提供了高度抽象的概念和大量预制组件，让开发者可以像搭乐高一样快速构建LLM应用。</li> <li>几行代码就能构建一个完整的RAG系统.</li></ul> <ol start="2"><li>组件生态丰富</li></ol> <ul><li>50+ 文档加载器（PDF、HTML、Word、Notion等）</li> <li>20+ 文本分割策略</li> <li>30+ 向量数据库集成</li> <li>10+ LLM提供商支持</li></ul> <ol start="3"><li>LangChain提出的Chain、Agent、Tool、Memory等概念已经成为行业标准，帮助开发者建立心智模型：</li></ol> <ul><li><code>Chain</code>：可复用的任务流水线</li> <li><code>Agent</code>：LLM驱动的决策系统</li> <li><code>Tool</code>：Agent可调用的函数</li> <li><code>Memory</code>：对话状态管理</li></ul> <ol start="4"><li><strong>社区活跃和文档完善</strong></li></ol> <ul><li>GitHub 80k+ stars，庞大的用户社区</li> <li>详细的文档和示例代码</li> <li>遇到问题容易找到解决方案</li></ul> <blockquote><p>缺点</p></blockquote> <ul><li><strong>调试困难, 很难追踪问题</strong></li> <li>性能开销</li> <li>API 不稳定与快速迭代
<ul><li>LangChain 的 API 变化非常频繁，导致代码很容易过时，维护成本高。</li></ul></li> <li>灵活性不够
<ul><li>当需要实现一个非常定制化、非标准的功能时，LangChain 的“框框”可能会限制你。</li></ul></li></ul> <h3 id="_4-langchain有哪些替代方案"><a href="#_4-langchain有哪些替代方案" class="header-anchor">#</a> 4. langchain有哪些替代方案？</h3> <p>轻量级替代品的出现：像 LlamaIndex（更专注于 RAG 场景）和 LangGraph（由 LangChain 官方推出，用于构建有状态、多参与者的 Agent 应用）等工具，提供了更专注、更清晰的范式。</p> <ol><li>LlamaIndex</li> <li>Haystack</li> <li>Semantic Kernel（微软出品）</li></ol> <h4 id="横向对比总结"><a href="#横向对比总结" class="header-anchor">#</a> 横向对比总结</h4> <table><thead><tr><th>维度</th> <th>LangChain</th> <th>LlamaIndex</th> <th>Haystack</th> <th>Semantic Kernel</th></tr></thead> <tbody><tr><td><strong>核心定位</strong></td> <td>通用LLM应用框架</td> <td><strong>专业RAG框架</strong></td> <td>NLP生产流水线</td> <td>AI规划与集成框架</td></tr> <tr><td><strong>设计哲学</strong></td> <td>模块化组合</td> <td>数据与检索优化</td> <td><strong>稳定流水线</strong></td> <td>传统代码+AI规划</td></tr> <tr><td><strong>学习曲线</strong></td> <td>陡峭</td> <td>中等</td> <td>中等</td> <td>较陡峭（概念独特）</td></tr> <tr><td><strong>调试难度</strong></td> <td>高（黑盒）</td> <td>中等</td> <td><strong>低（清晰流水线）</strong></td> <td>中等</td></tr> <tr><td><strong>生产就绪</strong></td> <td>中等</td> <td>中等</td> <td><strong>高</strong></td> <td>中等</td></tr> <tr><td><strong>RAG专业性</strong></td> <td>通用</td> <td><strong>最优</strong></td> <td>良好</td> <td>基础</td></tr> <tr><td><strong>Agent支持</strong></td> <td><strong>强大</strong></td> <td>有限</td> <td>有限</td> <td>独特规划方式</td></tr> <tr><td><strong>生态集成</strong></td> <td><strong>最丰富</strong></td> <td>专注数据源</td> <td>良好</td> <td>微软生态</td></tr></tbody></table> <h4 id="如何选择"><a href="#如何选择" class="header-anchor">#</a> 如何选择？</h4> <p><strong>选择 LlamaIndex 如果：</strong></p> <ul><li>你主要就是做 RAG 应用</li> <li>对检索质量有很高要求</li> <li>想要更简单直接的 API</li></ul> <p><strong>选择 Haystack 如果：</strong></p> <ul><li>需要稳定部署到生产环境</li> <li>重视可观测性和可维护性</li> <li>构建的是复杂、多步骤的 NLP 流水线</li></ul> <p><strong>选择 Semantic Kernel 如果：</strong></p> <ul><li>你是微软技术栈用户</li> <li>需要将 AI 深度集成到现有业务系统中</li> <li>需要 AI 进行复杂的任务规划和分解</li></ul> <h3 id="_5-什么是检索增强生成-rag"><a href="#_5-什么是检索增强生成-rag" class="header-anchor">#</a> 5. 什么是检索增强生成(RAG)？</h3> <blockquote><p>通过检索外部数据，增强⼤模型的⽣成效果。</p></blockquote> <ul><li>RAG即检索增强⽣成，为LLM提供了从某些数据源检索到的信息，并基于此修正⽣成的答案。</li> <li>RAG 基本上是Search + LLM 提示，可以通过⼤模型回答查询，并将搜索算法所找到的信息作为⼤模型的上下⽂。</li> <li>查询和检索到的上下⽂都会被注⼊到发送到 LLM 的提示语中</li></ul> <h3 id="_6-在做知识增强检索时-文本切分有哪些方法"><a href="#_6-在做知识增强检索时-文本切分有哪些方法" class="header-anchor">#</a> 6. 在做知识增强检索时，文本切分有哪些方法？</h3> <ul><li>按照句⼦来切分</li> <li>按照字符数来切分</li> <li>按固定字符数 结合overlap</li> <li>递归⽅法：RecursiveCharacterTextSplitter</li></ul> <h3 id="_7-目前主流的中文向量模型有哪些"><a href="#_7-目前主流的中文向量模型有哪些" class="header-anchor">#</a> 7. 目前主流的中文向量模型有哪些？</h3> <table><thead><tr><th style="text-align:left;">模型系列 / 来源</th> <th style="text-align:left;">代表性模型举例</th> <th style="text-align:left;">核心特点与适用场景</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>阿里通义千问 (Qwen)</strong></td> <td style="text-align:left;"><code>Qwen3-Embedding</code>系列 (如8B, 4B版本)</td> <td style="text-align:left;">专注于<strong>纯文本任务</strong>，在MTEB等权威榜单上表现优异，支持超100种语言，适合智能搜索、文本分类和RAG 。</td></tr> <tr><td style="text-align:left;"><strong>智源研究院 (BGE)</strong></td> <td style="text-align:left;"><code>BGE-Code-v1</code>, <code>BGE-VL-v1.5</code>, <code>BGE-VL-Screenshot</code></td> <td style="text-align:left;">生态丰富，覆盖<strong>代码、图文、截图</strong>等多模态检索，在特定领域基准测试中成绩领先，适合需要处理非纯文本数据的场景 。</td></tr> <tr><td style="text-align:left;"><strong>火山引擎 (Seed)</strong></td> <td style="text-align:left;"><code>Seed1.6-Embedding</code></td> <td style="text-align:left;">强调<strong>全模态</strong>能力，支持文本、图像、视频的混合检索，提供自定义指令功能，适合复杂的多模态和跨模态搜索需求 。</td></tr></tbody></table> <blockquote><p>如果您的主要目的是构建一个高效的语义检索系统（例如用于RAG），那么选择BGE或Qwen-Embedding这类专用模型是更直接、更有效的方案。</p></blockquote> <h3 id="_8-相比模型直接生成-rag的优势是什么"><a href="#_8-相比模型直接生成-rag的优势是什么" class="header-anchor">#</a> 8. 相比模型直接生成，RAG的优势是什么？</h3> <ul><li>因为RAG是基于检索外部知识来生成的的，可以提高模型回答问题的准确性</li> <li>直接生成是模型凭“记忆”和“直觉”回答问题，而 RAG 是让模型先“查阅资料”再回答问题。</li></ul> <blockquote><p>解决了直接生成的三大核心痛点</p></blockquote> <ol><li>知识滞后与事实性错误（幻觉）</li> <li>无法溯源，可信度低</li> <li>处理私有/领域知识时能力不足</li></ol> <h3 id="_9-self-rag-是什么-self-rag如何提升大型语言模型的质量和准确性"><a href="#_9-self-rag-是什么-self-rag如何提升大型语言模型的质量和准确性" class="header-anchor">#</a> 9. SELF-RAG 是什么，SELF-RAG如何提升大型语言模型的质量和准确性？</h3> <ul><li><strong>SELF-RAG</strong> 是一种让LLM在生成过程中<strong>自我评估</strong>、<strong>自我纠正</strong>的框架。</li> <li>与传统RAG不同，SELF-RAG让模型在生成每个段落时都主动决定是否需要检索、如何利用检索结果，以及评估生成内容的质量。</li></ul> <blockquote><p>SELF-RAG 流程</p></blockquote> <p>用户提问 → 检索相关文档 → LLM 对文档进行批判性评估（反思） → 根据评估结果，有选择地、智能地利用文档 → 生成最终答案。</p> <p>“反思”步骤是通过生成特殊的<strong>反思令牌（Special Tokens）</strong> 来实现的。这些令牌分为以下几类：</p> <ol><li><p><strong>检索令牌 - “要不要检索？”</strong></p> <ul><li>在生成答案的每一个阶段，模型会先自问：“我需要检索外部文档来帮助生成下一个段落吗？”</li></ul></li> <li><p><strong>相关性令牌 - “检索到的东西有用吗？”</strong></p> <ul><li>如果执行了检索，模型会评估每一篇检索到的文档。</li></ul></li> <li><p><strong>支持令牌 - “检索到的信息支持我的说法吗？”</strong></p> <ul><li>模型在生成具体陈述时，会判断该陈述是否得到了检索文档的支持。</li></ul></li> <li><p><strong>效用令牌 - “我生成的答案整体好吗？”</strong></p> <ul><li>最后，模型会对生成的完整段落进行整体评估。</li> <li>评判标准：<code>Good</code>， <code>Fair</code>， <code>Poor</code>。这确保了最终输出的综合质量。</li></ul></li></ol> <blockquote><p>SELF-RAG 如何提升LLM的质量和准确性？</p></blockquote> <ol><li><p><strong>SELF-RAG</strong>：通过 “支持令牌” ，模型只会生成有证据支持的陈述。如果证据不足或存在矛盾，模型会选择不生成或明确指出来，从而避免了传播错误信息。</p></li> <li><ul><li>模型不再是信息的被动接收者，而是主动的<strong>评估者</strong>。它只会筛选并利用那些被它判定为 <strong>“相关”</strong> 和 <strong>“准确”</strong> 的信息，使得最终答案的根基更加牢固。</li></ul></li> <li><ul><li>而SELF-RAG可以识别出哪几篇是高度相关的，并<strong>重点利用</strong>这些高质量信息，同时忽略或减少使用低质量信息。</li></ul></li> <li><ul><li>SELF-RAG 的输出可以附带其反思过程（例如，标注出哪些陈述有充分支持）。这为用户提供了<strong>可验证的线索</strong>，让用户知道答案的来源和可信度，增强了模型的透明度和可信度。</li></ul></li> <li><ul><li>当模型自身知识足够时，它选择 <code>No Retrieve</code>，自信地回答；当不确定时，它主动检索 <code>Retrieve</code>；当检索结果不好时，它承认知识的局限性。这使得模型表现得更像一位严谨的专家，而不是一个不懂装懂的学生。</li></ul></li></ol> <h3 id="_10-rag-和-微调的区别是什么"><a href="#_10-rag-和-微调的区别是什么" class="header-anchor">#</a> 10. RAG 和 微调的区别是什么？</h3> <ul><li><p>RAG（检索增强⽣成）是把内部的⽂档数据先进⾏embedding，借助检索先获得⼤致的知识范围答案，再结合prompt给到LLM，让LLM⽣成最终的答案</p></li> <li><p>Fine-tuning（微调）是⽤⼀定量的数据集对LLM进⾏局部参数的调整，以期望LLM更加理解我们的业务逻辑，有更好的zero-shot能⼒。</p></li></ul> <h3 id="_11-什么是graphrag"><a href="#_11-什么是graphrag" class="header-anchor">#</a> 11. 什么是GraphRAG？</h3> <p>GraphRAG 的核心思想是：将文档库中的信息提取并构建成<strong>一个结构化的知识图谱</strong>，然后利用这个图谱来增强大模型的检索和推理能力。</p> <ul><li><strong>GraphRAG</strong>：先从文档中提取实体（如人物、地点、概念、事件）和它们之间的关系，构建一个知识图谱。检索时进行图遍历和关系推理。</li></ul> <blockquote><p>GraphRAG VS RAG</p></blockquote> <table><thead><tr><th style="text-align:left;">特性</th> <th style="text-align:left;">传统 RAG</th> <th style="text-align:left;">GraphRAG</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>信息组织</strong></td> <td style="text-align:left;">扁平的文本片段</td> <td style="text-align:left;">结构化的关系网络</td></tr> <tr><td style="text-align:left;"><strong>检索方式</strong></td> <td style="text-align:left;">向量相似度</td> <td style="text-align:left;"><strong>关系与结构推理</strong></td></tr> <tr><td style="text-align:left;"><strong>推理能力</strong></td> <td style="text-align:left;">弱，基于局部上下文</td> <td style="text-align:left;"><strong>强，基于全局关系</strong></td></tr> <tr><td style="text-align:left;"><strong>可解释性</strong></td> <td style="text-align:left;">低（为什么检索这些片段？）</td> <td style="text-align:left;"><strong>高（答案路径清晰可见）</strong></td></tr></tbody></table> <blockquote><p>GraphRAG 的典型应用场景
GraphRAG 的优势决定了它更适合 “关联密集、需要推理” 的场景，而非简单的 “文本问答”：</p></blockquote> <ul><li><strong>知识图谱问答（KGQA）</strong>：如 “查询某明星的合作导演及其代表作”“某药物的适应症和禁忌症关联的疾病”；</li> <li><strong>商业情报分析</strong>：如 “梳理某行业的产业链关系（上游原材料→中游制造→下游品牌）”“分析某公司的投资版图和竞争对手网络”；</li> <li><strong>事件脉络梳理</strong>：如 “还原某历史事件的关键人物、时间线和因果关系（如‘二战主要战役的发起方和影响’）”；</li> <li><strong>合规与风控</strong>：如 “核查某企业的股权穿透关系（是否存在关联交易）”“识别金融诈骗中的人物 - 账户 - 资金流向关联”。</li></ul> <h2 id="阶段三"><a href="#阶段三" class="header-anchor">#</a> 阶段三</h2> <h3 id="_1-prompt-design-prompt-tuning-fine-tuning-有什么区别"><a href="#_1-prompt-design-prompt-tuning-fine-tuning-有什么区别" class="header-anchor">#</a> 1. Prompt design, Prompt Tuning， Fine-tuning 有什么区别？</h3> <blockquote><p>Prompt design：提示设计，或者叫<code>提示工程</code></p></blockquote> <ol><li>**学习如何向模型“提问”。**你不动模型本身，只优化你的指令。</li></ol> <blockquote><p>Prompt Tuning：提示微调，即微调模型的提示词</p></blockquote> <ol><li>**教模型学会一种新的“内部提问方式”。**你给模型一些可以学习的“软提示”，而不是修改它的核心知识。</li> <li>软提示有2种，1种是：soft prompt  1种是：hard prompt</li></ol> <blockquote><p>Fine-tuning：微调，即微调模型的参数</p></blockquote> <ul><li>全量微调的话: **给模型“回炉重造”，学习新专业。**你直接修改模型的核心参数，让它适应新任务。</li></ul> <ol><li>基于开源大模型进行微调，有全量参数微调，和 PEFT: 参数高效微调，也就是只有微调少量参数</li></ol> <h3 id="_2-参数高效的fine-tuning-peft-是什么"><a href="#_2-参数高效的fine-tuning-peft-是什么" class="header-anchor">#</a> 2. 参数高效的fine-tuning(PEFT)是什么？</h3> <p>Parameter-Efficient Fine-Tuning</p> <ul><li>用一些策略，仅训练少量参数，大部分模型参数冻结</li> <li>方法有：
<ul><li>适配器(Adapter)、软提示(Soft Prompts): Prompt Tuning、P-Tuning、Prefix-Tuning</li> <li>模型参数冻结</li> <li>LORA</li></ul></li></ul> <h3 id="_3-介绍一下prompt-tuning-技术"><a href="#_3-介绍一下prompt-tuning-技术" class="header-anchor">#</a> 3. 介绍一下Prompt-tuning 技术？</h3> <p>它的核心思想是：<strong>冻结主模型参数，在训练数据前，加入一小段Prompt（只通过优化一小段可学习的“软提示”（Soft Prompt）），来激发模型解决特定任务的潜能。</strong></p> <ul><li><p>只训练Prompt 的表示层，也就是一个Embedding 模块。</p></li> <li><p>其中， Prompt 有两种形式， 一种是: hard prompt ， 一种是: soft prompt</p></li></ul> <h4 id="代码演示"><a href="#代码演示" class="header-anchor">#</a> 代码演示</h4> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">from</span> peft <span class="token keyword">import</span> PromptTuningConfig<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> TaskType<span class="token punctuation">,</span> PromptTuningInit

<span class="token comment"># Soft Prompt 不需要显示指定 prompt</span>
<span class="token comment"># config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)</span>

<span class="token comment"># Hard Prompt</span>
config <span class="token operator">=</span> PromptTuningConfig<span class="token punctuation">(</span>
   task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span>
   prompt_tuning_init<span class="token operator">=</span>PromptTuningInit<span class="token punctuation">.</span>TEXT<span class="token punctuation">,</span>
   prompt_tuning_init_text<span class="token operator">=</span><span class="token string">&quot;下面是一段人与机器人的对话。&quot;</span><span class="token punctuation">,</span>
   num_virtual_tokens<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">(</span><span class="token string">&quot;下面是一段人与机器人的对话。&quot;</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">&quot;input_ids&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
   tokenizer_name_or_path<span class="token operator">=</span><span class="token string">&quot;Langboat/bloom-1b4-zh&quot;</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="_4-什么是prefix-tuning"><a href="#_4-什么是prefix-tuning" class="header-anchor">#</a> 4. 什么是Prefix Tuning？</h3> <ul><li>核心思想：在每一层之前添加可学习的“前缀”。</li> <li>相较于Prompt-Tuning和P-tuning,
<ul><li>Prefix-Tuning不再将Prompt加在输入的Embedding层,</li> <li>而是将其作为可学习的前缀, 放置在Transsformer模型中的每一层中, 具体表现形式为past_key_values。</li></ul></li></ul> <p><img src="/hmblog//images/fine-tuning/Prefix-Tuning.png" alt="Prefix Tuning"></p> <ul><li>图中的Prefix Encoder 不会跟 右边的 Embedding 拼起来，而是会放到Transformer Blocks 层里，参与计算，</li> <li>它是通过past_key_values 形式放进去的</li></ul> <p><img src="/hmblog//images/fine-tuning/past_key_values.png" alt="past_key_values"></p> <ul><li><strong>普通Prompt-Tuning</strong>：只在模型的<strong>输入嵌入层（Input Embedding Layer）</strong> 添加可训练的提示向量。这相当于在对话开始时给模型一个总体的指令。</li> <li><strong>Prefix-Tuning</strong>：不仅在输入层，而是在<strong>模型的每一层（或某几层）的激活（activation）之前</strong>，都添加一组可训练的前缀向量。这相当于在模型思考的每一个步骤、每一个阶段都不断地进行引导和提醒，确保它不偏离轨道。</li></ul> <hr> <p><strong>Prompt Tuning</strong> VS <strong>Prefix Tuning</strong></p> <ul><li><strong>思想</strong>：不修改模型本身，而是在输入序列前添加一些可训练的<strong>软提示向量</strong>。模型通过这些提示向量来适应下游任务。</li> <li><strong>区别</strong>:
<ul><li>Prompt Tuning 直接训练这些向量；</li> <li>Prefix Tuning 通过一个小型的前馈网络来生成这些向量，训练的是这个网络的参数。</li></ul></li></ul> <hr> <h3 id="_5-介绍下lora微调"><a href="#_5-介绍下lora微调" class="header-anchor">#</a> 5. 介绍下LORA微调？</h3> <ul><li>在每一个要计算的大的矩阵(权重)旁边，新起一条分支，</li> <li>这个分支的话，是由两个小矩阵组成, LoRA_A  和  LoRA_B</li> <li>那我更新的时候只更新这两个小矩阵</li> <li>训练完成之后，再把它合并回去，这就是LoRA</li></ul> <p><img src="/hmblog//images/fine-tuning/Lora1.png" alt="LoRA 的思想"></p> <ul><li>LoRA 原理</li></ul> <p><img src="/hmblog//images/fine-tuning/Lora2.png" alt="Lora"></p> <h3 id="_6-相比lora-adalora的改进点是什么"><a href="#_6-相比lora-adalora的改进点是什么" class="header-anchor">#</a> 6. 相比LORA，AdaLoRA的改进点是什么？</h3> <ul><li><strong>思想</strong>：LoRA 的自适应版本。它不是为所有选定的层分配固定的秩，而是根据重要性评分<code>动态地调整每个 LoRA 模块的秩（参数预算）</code>，将更多的参数分配给更重要的模块。</li> <li><strong>优势</strong>：在相同的参数预算下，通常能获得比标准 LoRA 更好的性能。</li></ul> <blockquote><p>引入了3种关键技术</p></blockquote> <ol><li><strong>参数重要性评分</strong></li> <li><strong>动态的预算分配和秩调整</strong></li> <li><strong>通过SVD参数化进行高效调整</strong></li></ol> <h4 id="总结与类比"><a href="#总结与类比" class="header-anchor">#</a> 总结与类比</h4> <ul><li><strong>LoRA</strong> 就像给公司每个部门分配<strong>固定且相同</strong>的预算，不管这个部门是核心研发还是后勤支持。</li> <li><strong>AdaLoRA</strong> 则像一位<strong>精明的CEO</strong>，他会根据每个部门的业绩（重要性分数）和公司总预算，定期进行审查：削减表现不佳部门的预算（修剪），并将资源重新分配给高绩效、高潜力的部门（生长）。</li></ul> <p>因此，AdaLoRA的主要优势在于其<strong>自适应性</strong>和<strong>效率</strong>。</p> <h3 id="_7-qlora模型有什么创新点"><a href="#_7-qlora模型有什么创新点" class="header-anchor">#</a> 7. QLORA模型有什么创新点？</h3> <h3 id="_8-稀疏微调是怎么工作的-有哪几个步骤"><a href="#_8-稀疏微调是怎么工作的-有哪几个步骤" class="header-anchor">#</a> 8. 稀疏微调是怎么工作的，有哪几个步骤？</h3> <p><strong>只更新模型庞大参数中的一小部分（稀疏），而冻结其余大部分参数。</strong></p> <p>目标： 极大地减少微调时需要更新的参数量。</p> <ul><li>核心思想是：<strong>大型预训练模型已经包含了丰富的通用知识，对于特定的下游任务，我们只需要激发或调整其中与之相关的一小部分神经元或参数子集就足够了，而不需要动整个网络。</strong></li></ul> <p><strong>常见示例与步骤：</strong></p> <ol><li><strong>只微调偏置项</strong> <ul><li><strong>步骤</strong>：
<ul><li>冻结模型中所有的权重矩阵（如Linear、LayerNorm的权重）。</li> <li>只将模型中所有的偏置项设置为可训练。</li> <li>进行训练。由于偏置项的数量远少于权重，实现了稀疏更新。</li></ul></li></ul></li> <li><strong>只微调某几层</strong> <ul><li><strong>步骤</strong>：
<ul><li>冻结模型的大部分层（例如，只更新最后4层Transformer块）。</li> <li>只解冻靠近输出端的少数几层进行微调。</li></ul></li></ul></li> <li><strong>只微调注意力模块中的特定部分</strong> <ul><li><strong>步骤</strong>：
<ul><li>冻结前馈神经网络。</li> <li>只更新自注意力机制中的参数（如Query, Key, Value投影矩阵）。</li></ul></li></ul></li></ol> <h3 id="方法三-基于附加稀疏适配器的微调-目前最流行"><a href="#方法三-基于附加稀疏适配器的微调-目前最流行" class="header-anchor">#</a> 方法三：基于附加稀疏适配器的微调（目前最流行）</h3> <p>这是目前最主流的“稀疏微调”方式，也是<strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> 的核心。它<strong>不直接更新原始模型参数</strong>，而是<strong>引入一小部分额外的、可训练的参数（适配器）</strong>，模型主体保持冻结。</p> <p><strong>工作原理：</strong></p> <ul><li>在预训练模型的架构中，插入一些小的、可训练的模块。</li> <li>在微调时，<strong>只训练这些新增的适配器</strong>，原始模型的<strong>所有参数都被冻结</strong>。从参数更新的角度看，这是“稀疏”的，因为只有新增的那一小部分参数被更新。</li></ul> <p><strong>常见示例与步骤：</strong></p> <ol><li><strong>LoRA及其变种</strong> <ul><li><strong>步骤</strong>：
<ul><li>冻结整个预训练模型。</li> <li>在原有的权重矩阵 ( W ) 旁，注入一个低秩适配器 ΔW = BA。</li> <li><strong>只训练 ( A ) 和 ( B ) 这两个小矩阵</strong>。前向传播变为：( h = Wx + BAx )。</li></ul></li></ul></li> <li><strong>Adapter模块</strong> <ul><li><strong>步骤</strong>：
<ul><li>冻结整个预训练模型。</li> <li>在Transformer块中的前馈网络或注意力模块之后，插入一个小的前馈神经网络（Adapter）。</li> <li><strong>只训练这些插入的Adapter模块</strong>。</li></ul></li></ul></li></ol> <p><strong>特点：</strong></p> <ul><li><strong>附加式</strong>：不改变原参数，而是增加新参数。</li> <li><strong>极高效</strong>：通常参数量极小（仅为原模型的0.01%~1%）。</li> <li><strong>模块化与安全</strong>：由于原模型不动，可以为一个模型创建多个适配器用于不同任务，且没有灾难性遗忘的风险。</li></ul> <h3 id="_9-监督微调sft后llm表现下降的原因"><a href="#_9-监督微调sft后llm表现下降的原因" class="header-anchor">#</a> 9. 监督微调SFT后LLM表现下降的原因？</h3> <ul><li>监督微调后模型表现下降，通常被称为 “灾难性遗忘” 或 “对齐税” 。</li></ul> <p><img src="/hmblog/images/question/SFT-LLM.png" alt="SFT 后LLM表现下降的原因分析"></p> <h4 id="如何缓解这些问题"><a href="#如何缓解这些问题" class="header-anchor">#</a> 如何缓解这些问题？</h4> <ol><li><strong>提升数据质量</strong>：精心清洗和构建SFT数据，确保正确性、多样性和高质量。<strong>“质量远胜于数量”</strong> 在SFT中尤其正确。</li> <li><strong>谨慎选择超参数</strong>：使用<strong>较低的学习率</strong>和<strong>较少的训练轮数</strong>（通常1-3个epoch）。始终使用验证集来监控训练，防止过拟合。</li> <li><strong>使用参数高效微调技术</strong>：如 <strong>LoRA</strong>。这种方法只更新一小部分参数，大部分预训练参数保持不变，从而极大地减轻了灾难性遗忘。</li></ol> <h3 id="_10-什么是p-tuning"><a href="#_10-什么是p-tuning" class="header-anchor">#</a> 10. 什么是P-Tuning？</h3> <p>P-Tuning的思想: 在Prompt-Tuning的基础上,对Prompt部分进行进一步的编码计算,加速收敛。</p> <ul><li>PEFT中支持两种编码方式,一种是LSTM,一种是MLP。</li> <li>与Prompt-Tuning不同的是, Prompt的形式只有Soft Prompt。
<ul><li>MLP: 全连接层，3层全连接层</li> <li>LSTM： 一层LSTM，2层全连接层</li></ul></li></ul> <p><img src="/hmblog//images/fine-tuning/P-Tuning.png" alt="P-Tuning"></p> <h3 id="_11-多轮对话任务如何微调模型"><a href="#_11-多轮对话任务如何微调模型" class="header-anchor">#</a> 11. 多轮对话任务如何微调模型？</h3> <p>一. 选择PEFT 参数高效微调</p> <ol><li>选择 LoRA / QLoRA  或者 <code>Adapter</code></li> <li>对话格式构建与损失计算</li> <li>step1: 构建带角色的对话模版</li> <li>step2: 计算仅针对助理回复的损失</li> <li>模型学会在上下文中生成回复</li></ol> <p><strong>LoRA</strong> 和 <strong>QLoRA</strong>。它在全参数微调的基础上，引入了可训练的旁路矩阵，极大地降低了计算成本。</p> <p><strong>工作原理：</strong></p> <ol><li>冻结预训练模型的所有参数。</li> <li>在模型的注意力层和全连接层旁，注入少量的可训练参数（LoRA 秩分解矩阵）。</li> <li>训练时，只更新这些新增的参数，而不触动原始模型的巨大参数库。</li></ol> <p><strong>为什么PEFT对多轮对话特别有益？</strong></p> <ul><li><strong>减轻灾难性遗忘</strong>：多轮对话数据通常远少于预训练数据。PEFT 能更好地保留模型在预训练阶段获得的世界知识和语言能力，防止其因过度专注于学习对话结构而遗忘根本。</li> <li><strong>高效且成本低</strong>：可以在消费级GPU上微调大模型。</li> <li><strong>模块化</strong>：可以为不同的对话风格或任务训练不同的 LoRA 适配器，灵活切换。</li></ul></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div></main></div> <!----></div> <ul class="sub-sidebar sub-sidebar-wrapper" style="width:12rem;" data-v-7115df4a data-v-1c636796><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_1-说一下chatgpt的优缺点" class="sidebar-link reco-side-_1-说一下chatgpt的优缺点" data-v-7115df4a>1. 说一下ChatGPT的优缺点</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_2-请简述下transformer基本流程" class="sidebar-link reco-side-_2-请简述下transformer基本流程" data-v-7115df4a>2. 请简述下Transformer基本流程</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_3-为什么基于transformer的架构需要多头注意力机制" class="sidebar-link reco-side-_3-为什么基于transformer的架构需要多头注意力机制" data-v-7115df4a>3. 为什么基于Transformer的架构需要多头注意力机制？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_4-编码器-解码器-编解码llm的区别" class="sidebar-link reco-side-_4-编码器-解码器-编解码llm的区别" data-v-7115df4a>4. 编码器，解码器，编解码LLM的区别？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_5-在语言模型中强化学习的概念-它如何应用于chatgpt" class="sidebar-link reco-side-_5-在语言模型中强化学习的概念-它如何应用于chatgpt" data-v-7115df4a>5. 在语言模型中强化学习的概念?它如何应用于ChatGPT？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_6-在gpt模型中-什么是温度系数" class="sidebar-link reco-side-_6-在gpt模型中-什么是温度系数" data-v-7115df4a>6. 在GPT模型中，什么是温度系数？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_7-什么是旋转位置编码-rotary-position-encoding-简称rope" class="sidebar-link reco-side-_7-什么是旋转位置编码-rotary-position-encoding-简称rope" data-v-7115df4a>7. 什么是旋转位置编码(Rotary Position Encoding)，简称ROPE？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_8-为什么现在的大模型大多是decoder-only的架构" class="sidebar-link reco-side-_8-为什么现在的大模型大多是decoder-only的架构" data-v-7115df4a>8. 为什么现在的大模型大多是decoder-only的架构？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_9-chatgpt的训练步骤有哪些" class="sidebar-link reco-side-_9-chatgpt的训练步骤有哪些" data-v-7115df4a>9. ChatGPT的训练步骤有哪些？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_10-为什么transformer模型需要位置编码" class="sidebar-link reco-side-_10-为什么transformer模型需要位置编码" data-v-7115df4a>10. 为什么Transformer模型需要位置编码？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_11-为什么对于chatgpt而言-提示工程很重要" class="sidebar-link reco-side-_11-为什么对于chatgpt而言-提示工程很重要" data-v-7115df4a>11. 为什么对于ChatGPT而言，提示工程很重要？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_12-如何缓解llms复读机问题" class="sidebar-link reco-side-_12-如何缓解llms复读机问题" data-v-7115df4a>12. 如何缓解LLMs复读机问题？</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#阶段二" class="sidebar-link reco-side-阶段二" data-v-7115df4a>阶段二</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_1-解释下langchain-agent的概念" class="sidebar-link reco-side-_1-解释下langchain-agent的概念" data-v-7115df4a>1. 解释下langchain Agent的概念？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_2-langchain的6大核心组件是什么-它们的作用分别是什么" class="sidebar-link reco-side-_2-langchain的6大核心组件是什么-它们的作用分别是什么" data-v-7115df4a>2. langchain的6大核心组件是什么，它们的作用分别是什么？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_3-langchain有哪些优点和明显的缺点" class="sidebar-link reco-side-_3-langchain有哪些优点和明显的缺点" data-v-7115df4a>3. langchain有哪些优点和明显的缺点？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_4-langchain有哪些替代方案" class="sidebar-link reco-side-_4-langchain有哪些替代方案" data-v-7115df4a>4. langchain有哪些替代方案？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_5-什么是检索增强生成-rag" class="sidebar-link reco-side-_5-什么是检索增强生成-rag" data-v-7115df4a>5. 什么是检索增强生成(RAG)？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_6-在做知识增强检索时-文本切分有哪些方法" class="sidebar-link reco-side-_6-在做知识增强检索时-文本切分有哪些方法" data-v-7115df4a>6. 在做知识增强检索时，文本切分有哪些方法？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_7-目前主流的中文向量模型有哪些" class="sidebar-link reco-side-_7-目前主流的中文向量模型有哪些" data-v-7115df4a>7. 目前主流的中文向量模型有哪些？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_8-相比模型直接生成-rag的优势是什么" class="sidebar-link reco-side-_8-相比模型直接生成-rag的优势是什么" data-v-7115df4a>8. 相比模型直接生成，RAG的优势是什么？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_9-self-rag-是什么-self-rag如何提升大型语言模型的质量和准确性" class="sidebar-link reco-side-_9-self-rag-是什么-self-rag如何提升大型语言模型的质量和准确性" data-v-7115df4a>9. SELF-RAG 是什么，SELF-RAG如何提升大型语言模型的质量和准确性？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_10-rag-和-微调的区别是什么" class="sidebar-link reco-side-_10-rag-和-微调的区别是什么" data-v-7115df4a>10. RAG 和 微调的区别是什么？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_11-什么是graphrag" class="sidebar-link reco-side-_11-什么是graphrag" data-v-7115df4a>11. 什么是GraphRAG？</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#阶段三" class="sidebar-link reco-side-阶段三" data-v-7115df4a>阶段三</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_1-prompt-design-prompt-tuning-fine-tuning-有什么区别" class="sidebar-link reco-side-_1-prompt-design-prompt-tuning-fine-tuning-有什么区别" data-v-7115df4a>1. Prompt design, Prompt Tuning， Fine-tuning 有什么区别？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_2-参数高效的fine-tuning-peft-是什么" class="sidebar-link reco-side-_2-参数高效的fine-tuning-peft-是什么" data-v-7115df4a>2. 参数高效的fine-tuning(PEFT)是什么？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_3-介绍一下prompt-tuning-技术" class="sidebar-link reco-side-_3-介绍一下prompt-tuning-技术" data-v-7115df4a>3. 介绍一下Prompt-tuning 技术？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_4-什么是prefix-tuning" class="sidebar-link reco-side-_4-什么是prefix-tuning" data-v-7115df4a>4. 什么是Prefix Tuning？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_5-介绍下lora微调" class="sidebar-link reco-side-_5-介绍下lora微调" data-v-7115df4a>5. 介绍下LORA微调？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_6-相比lora-adalora的改进点是什么" class="sidebar-link reco-side-_6-相比lora-adalora的改进点是什么" data-v-7115df4a>6. 相比LORA，AdaLoRA的改进点是什么？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_7-qlora模型有什么创新点" class="sidebar-link reco-side-_7-qlora模型有什么创新点" data-v-7115df4a>7. QLORA模型有什么创新点？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_8-稀疏微调是怎么工作的-有哪几个步骤" class="sidebar-link reco-side-_8-稀疏微调是怎么工作的-有哪几个步骤" data-v-7115df4a>8. 稀疏微调是怎么工作的，有哪几个步骤？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#方法三-基于附加稀疏适配器的微调-目前最流行" class="sidebar-link reco-side-方法三-基于附加稀疏适配器的微调-目前最流行" data-v-7115df4a>方法三：基于附加稀疏适配器的微调（目前最流行）</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_9-监督微调sft后llm表现下降的原因" class="sidebar-link reco-side-_9-监督微调sft后llm表现下降的原因" data-v-7115df4a>9. 监督微调SFT后LLM表现下降的原因？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_10-什么是p-tuning" class="sidebar-link reco-side-_10-什么是p-tuning" data-v-7115df4a>10. 什么是P-Tuning？</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/model-question.html#_11-多轮对话任务如何微调模型" class="sidebar-link reco-side-_11-多轮对话任务如何微调模型" data-v-7115df4a>11. 多轮对话任务如何微调模型？</a></li></ul></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-65133105 data-v-65133105><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-65133105><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-65133105></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-65133105></path></svg></div><!----></div></div>
    <script src="/hmblog/assets/js/app.ba48173d.js" defer></script><script src="/hmblog/assets/js/3.ac02bfbd.js" defer></script><script src="/hmblog/assets/js/1.ba4d6411.js" defer></script><script src="/hmblog/assets/js/59.ec40cafa.js" defer></script><script src="/hmblog/assets/js/9.d5a05a45.js" defer></script>
  </body>
</html>
