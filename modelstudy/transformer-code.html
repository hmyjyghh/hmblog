<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>手撕Transformer 代码 | 寒梦的博客</title>
    <meta name="generator" content="VuePress 1.9.4">
    <link rel="icon" href="/hmblog/logo.png">
    <meta name="description" content="宝剑锋从磨砺出，梅花香自苦寒来。">
    
    <link rel="preload" href="/hmblog/assets/css/0.styles.b92e6d01.css" as="style"><link rel="preload" href="/hmblog/assets/js/app.ba48173d.js" as="script"><link rel="preload" href="/hmblog/assets/js/3.ac02bfbd.js" as="script"><link rel="preload" href="/hmblog/assets/js/1.ba4d6411.js" as="script"><link rel="preload" href="/hmblog/assets/js/72.67728c96.js" as="script"><link rel="preload" href="/hmblog/assets/js/9.d5a05a45.js" as="script"><link rel="prefetch" href="/hmblog/assets/js/10.cdd9f1fd.js"><link rel="prefetch" href="/hmblog/assets/js/100.6bd782a8.js"><link rel="prefetch" href="/hmblog/assets/js/101.aa82f15d.js"><link rel="prefetch" href="/hmblog/assets/js/102.02a5e952.js"><link rel="prefetch" href="/hmblog/assets/js/103.f64270f5.js"><link rel="prefetch" href="/hmblog/assets/js/104.28c05f95.js"><link rel="prefetch" href="/hmblog/assets/js/105.4a3c3d99.js"><link rel="prefetch" href="/hmblog/assets/js/106.d7163a41.js"><link rel="prefetch" href="/hmblog/assets/js/107.03567d93.js"><link rel="prefetch" href="/hmblog/assets/js/108.9c57ec7a.js"><link rel="prefetch" href="/hmblog/assets/js/109.0b5cc849.js"><link rel="prefetch" href="/hmblog/assets/js/11.513bb120.js"><link rel="prefetch" href="/hmblog/assets/js/110.c000ed58.js"><link rel="prefetch" href="/hmblog/assets/js/111.288d96b3.js"><link rel="prefetch" href="/hmblog/assets/js/112.8a5358fb.js"><link rel="prefetch" href="/hmblog/assets/js/113.a1de3201.js"><link rel="prefetch" href="/hmblog/assets/js/114.d8618f6d.js"><link rel="prefetch" href="/hmblog/assets/js/115.17026eb3.js"><link rel="prefetch" href="/hmblog/assets/js/116.f0cb16ec.js"><link rel="prefetch" href="/hmblog/assets/js/117.66402938.js"><link rel="prefetch" href="/hmblog/assets/js/118.e16ec272.js"><link rel="prefetch" href="/hmblog/assets/js/119.53ee2325.js"><link rel="prefetch" href="/hmblog/assets/js/12.6eb5f340.js"><link rel="prefetch" href="/hmblog/assets/js/120.b2631c22.js"><link rel="prefetch" href="/hmblog/assets/js/121.9b598b7e.js"><link rel="prefetch" href="/hmblog/assets/js/122.05f086d4.js"><link rel="prefetch" href="/hmblog/assets/js/123.ee628a51.js"><link rel="prefetch" href="/hmblog/assets/js/124.3309b897.js"><link rel="prefetch" href="/hmblog/assets/js/125.8ba7f05f.js"><link rel="prefetch" href="/hmblog/assets/js/126.363ab065.js"><link rel="prefetch" href="/hmblog/assets/js/127.3dc43128.js"><link rel="prefetch" href="/hmblog/assets/js/128.ead549d4.js"><link rel="prefetch" href="/hmblog/assets/js/129.0219dd0d.js"><link rel="prefetch" href="/hmblog/assets/js/13.41ba4240.js"><link rel="prefetch" href="/hmblog/assets/js/130.50036dbd.js"><link rel="prefetch" href="/hmblog/assets/js/131.43d759e2.js"><link rel="prefetch" href="/hmblog/assets/js/132.73426bd0.js"><link rel="prefetch" href="/hmblog/assets/js/133.bf6d3d08.js"><link rel="prefetch" href="/hmblog/assets/js/134.2eb9f09e.js"><link rel="prefetch" href="/hmblog/assets/js/135.06734521.js"><link rel="prefetch" href="/hmblog/assets/js/136.82c90848.js"><link rel="prefetch" href="/hmblog/assets/js/137.8569b3cd.js"><link rel="prefetch" href="/hmblog/assets/js/138.8e5559db.js"><link rel="prefetch" href="/hmblog/assets/js/139.d7e6536d.js"><link rel="prefetch" href="/hmblog/assets/js/14.bd0b82cb.js"><link rel="prefetch" href="/hmblog/assets/js/140.8481dfb3.js"><link rel="prefetch" href="/hmblog/assets/js/15.491bfa3e.js"><link rel="prefetch" href="/hmblog/assets/js/16.00b7c2a7.js"><link rel="prefetch" href="/hmblog/assets/js/17.2edfa6dd.js"><link rel="prefetch" href="/hmblog/assets/js/18.c1ed5355.js"><link rel="prefetch" href="/hmblog/assets/js/19.127093b0.js"><link rel="prefetch" href="/hmblog/assets/js/20.13018e45.js"><link rel="prefetch" href="/hmblog/assets/js/21.af0d47de.js"><link rel="prefetch" href="/hmblog/assets/js/22.94430091.js"><link rel="prefetch" href="/hmblog/assets/js/23.7f4160d8.js"><link rel="prefetch" href="/hmblog/assets/js/24.53f66321.js"><link rel="prefetch" href="/hmblog/assets/js/25.70c7adf7.js"><link rel="prefetch" href="/hmblog/assets/js/26.ed970703.js"><link rel="prefetch" href="/hmblog/assets/js/27.61f08265.js"><link rel="prefetch" href="/hmblog/assets/js/28.47f448c8.js"><link rel="prefetch" href="/hmblog/assets/js/29.3a6d76f5.js"><link rel="prefetch" href="/hmblog/assets/js/30.fcf9b535.js"><link rel="prefetch" href="/hmblog/assets/js/31.8a27a1da.js"><link rel="prefetch" href="/hmblog/assets/js/32.e4efef75.js"><link rel="prefetch" href="/hmblog/assets/js/33.ea12175d.js"><link rel="prefetch" href="/hmblog/assets/js/34.bc951939.js"><link rel="prefetch" href="/hmblog/assets/js/35.adfc7113.js"><link rel="prefetch" href="/hmblog/assets/js/36.f4b05b10.js"><link rel="prefetch" href="/hmblog/assets/js/37.b160f43f.js"><link rel="prefetch" href="/hmblog/assets/js/38.182e0a40.js"><link rel="prefetch" href="/hmblog/assets/js/39.f6688462.js"><link rel="prefetch" href="/hmblog/assets/js/4.e51e222a.js"><link rel="prefetch" href="/hmblog/assets/js/40.9edf4cd4.js"><link rel="prefetch" href="/hmblog/assets/js/41.6ee87788.js"><link rel="prefetch" href="/hmblog/assets/js/42.e15f3316.js"><link rel="prefetch" href="/hmblog/assets/js/43.a703d986.js"><link rel="prefetch" href="/hmblog/assets/js/44.73bbf228.js"><link rel="prefetch" href="/hmblog/assets/js/45.c5ced175.js"><link rel="prefetch" href="/hmblog/assets/js/46.c039688e.js"><link rel="prefetch" href="/hmblog/assets/js/47.85473c5e.js"><link rel="prefetch" href="/hmblog/assets/js/48.011447cd.js"><link rel="prefetch" href="/hmblog/assets/js/49.64ffd976.js"><link rel="prefetch" href="/hmblog/assets/js/5.60398f15.js"><link rel="prefetch" href="/hmblog/assets/js/50.d4938794.js"><link rel="prefetch" href="/hmblog/assets/js/51.13d55290.js"><link rel="prefetch" href="/hmblog/assets/js/52.506ef0e3.js"><link rel="prefetch" href="/hmblog/assets/js/53.fbe62579.js"><link rel="prefetch" href="/hmblog/assets/js/54.72aa5c58.js"><link rel="prefetch" href="/hmblog/assets/js/55.8a3c2734.js"><link rel="prefetch" href="/hmblog/assets/js/56.4d420373.js"><link rel="prefetch" href="/hmblog/assets/js/57.57103446.js"><link rel="prefetch" href="/hmblog/assets/js/58.9aab8a53.js"><link rel="prefetch" href="/hmblog/assets/js/59.ec40cafa.js"><link rel="prefetch" href="/hmblog/assets/js/6.775d6c13.js"><link rel="prefetch" href="/hmblog/assets/js/60.05b1112b.js"><link rel="prefetch" href="/hmblog/assets/js/61.5dd88b4a.js"><link rel="prefetch" href="/hmblog/assets/js/62.4bf0f31f.js"><link rel="prefetch" href="/hmblog/assets/js/63.5fa24c27.js"><link rel="prefetch" href="/hmblog/assets/js/64.98804cd8.js"><link rel="prefetch" href="/hmblog/assets/js/65.9d6391ad.js"><link rel="prefetch" href="/hmblog/assets/js/66.afa16de6.js"><link rel="prefetch" href="/hmblog/assets/js/67.23a13c5e.js"><link rel="prefetch" href="/hmblog/assets/js/68.faffe00e.js"><link rel="prefetch" href="/hmblog/assets/js/69.8d2c347b.js"><link rel="prefetch" href="/hmblog/assets/js/7.e91fdee2.js"><link rel="prefetch" href="/hmblog/assets/js/70.bed5cdee.js"><link rel="prefetch" href="/hmblog/assets/js/71.712a04f2.js"><link rel="prefetch" href="/hmblog/assets/js/73.a49f5b5b.js"><link rel="prefetch" href="/hmblog/assets/js/74.fa293c83.js"><link rel="prefetch" href="/hmblog/assets/js/75.9452e054.js"><link rel="prefetch" href="/hmblog/assets/js/76.24ad0c90.js"><link rel="prefetch" href="/hmblog/assets/js/77.94b04afb.js"><link rel="prefetch" href="/hmblog/assets/js/78.97c9f7fb.js"><link rel="prefetch" href="/hmblog/assets/js/79.be8996a8.js"><link rel="prefetch" href="/hmblog/assets/js/8.968ba87b.js"><link rel="prefetch" href="/hmblog/assets/js/80.002c12e9.js"><link rel="prefetch" href="/hmblog/assets/js/81.724c88b9.js"><link rel="prefetch" href="/hmblog/assets/js/82.c17fc6b0.js"><link rel="prefetch" href="/hmblog/assets/js/83.b1e421f1.js"><link rel="prefetch" href="/hmblog/assets/js/84.94eacc74.js"><link rel="prefetch" href="/hmblog/assets/js/85.089b8ca8.js"><link rel="prefetch" href="/hmblog/assets/js/86.fb003f4f.js"><link rel="prefetch" href="/hmblog/assets/js/87.b6fd3eaa.js"><link rel="prefetch" href="/hmblog/assets/js/88.d8b79d91.js"><link rel="prefetch" href="/hmblog/assets/js/89.f6d32768.js"><link rel="prefetch" href="/hmblog/assets/js/90.89f31336.js"><link rel="prefetch" href="/hmblog/assets/js/91.47343b87.js"><link rel="prefetch" href="/hmblog/assets/js/92.2e8d45e3.js"><link rel="prefetch" href="/hmblog/assets/js/93.5e7328c6.js"><link rel="prefetch" href="/hmblog/assets/js/94.bde570da.js"><link rel="prefetch" href="/hmblog/assets/js/95.f0d4415e.js"><link rel="prefetch" href="/hmblog/assets/js/96.d48d924f.js"><link rel="prefetch" href="/hmblog/assets/js/97.3cea1091.js"><link rel="prefetch" href="/hmblog/assets/js/98.158fd54d.js"><link rel="prefetch" href="/hmblog/assets/js/99.ca68782b.js">
    <link rel="stylesheet" href="/hmblog/assets/css/0.styles.b92e6d01.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1c636796><div data-v-1c636796><div class="password-shadow password-wrapper-out" style="display:none;" data-v-2c3e9f55 data-v-1c636796 data-v-1c636796><h3 class="title" data-v-2c3e9f55>寒梦的博客</h3> <p class="description" data-v-2c3e9f55>宝剑锋从磨砺出，梅花香自苦寒来。</p> <label id="box" class="inputBox" data-v-2c3e9f55><input type="password" value="" data-v-2c3e9f55> <span data-v-2c3e9f55>Konck! Knock!</span> <button data-v-2c3e9f55>OK</button></label> <div class="footer" data-v-2c3e9f55><span data-v-2c3e9f55><i class="iconfont reco-theme" data-v-2c3e9f55></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-2c3e9f55>vuePress-theme-reco</a></span> <span data-v-2c3e9f55><i class="iconfont reco-copyright" data-v-2c3e9f55></i> <a data-v-2c3e9f55><span data-v-2c3e9f55>寒梦</span>
          
        <!---->
        2025
      </a></span></div></div> <div class="hide" data-v-1c636796><header class="navbar" data-v-1c636796><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hmblog/" class="home-link router-link-active"><!----> <span class="site-name">寒梦的博客</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      强大的MCP
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-knowledge.html" class="nav-link"><i class="undefined"></i>
  MCP 是什么
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/ide-mcp-server.html" class="nav-link"><i class="undefined"></i>
  IDE 使用MCP Server实操
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      Python
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/basic.html" class="nav-link"><i class="undefined"></i>
  Python 基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/data-structure.html" class="nav-link"><i class="undefined"></i>
  Python 数据结构
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/builtin-function.html" class="nav-link"><i class="undefined"></i>
  Python 内置函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/function.html" class="nav-link"><i class="undefined"></i>
  Python 函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/class.html" class="nav-link"><i class="undefined"></i>
  Python 类
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/module.html" class="nav-link"><i class="undefined"></i>
  Python 模块
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/package.html" class="nav-link"><i class="undefined"></i>
  Python 包
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/exception.html" class="nav-link"><i class="undefined"></i>
  Python 异常
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/file.html" class="nav-link"><i class="undefined"></i>
  Python 文件操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/regex.html" class="nav-link"><i class="undefined"></i>
  Python 正则表达式
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/thread.html" class="nav-link"><i class="undefined"></i>
  Python 多线程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/process.html" class="nav-link"><i class="undefined"></i>
  Python 多进程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/network.html" class="nav-link"><i class="undefined"></i>
  Python 网络编程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/database.html" class="nav-link"><i class="undefined"></i>
  Python 数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/numpy.html" class="nav-link"><i class="undefined"></i>
  Python numpy
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/array-operation.html" class="nav-link"><i class="undefined"></i>
  Python 数组操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-library.html" class="nav-link"><i class="undefined"></i>
  Python 常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/string-function.html" class="nav-link"><i class="undefined"></i>
  Python 字符串函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/practice.html" class="nav-link"><i class="undefined"></i>
  Python 日常练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-pip-install.html" class="nav-link"><i class="undefined"></i>
  pip 那些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/line-continue.html" class="nav-link"><i class="undefined"></i>
  Python 中的行续行符
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/technology-point.html" class="nav-link"><i class="undefined"></i>
  Python 中技术点练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/pandas-study.html" class="nav-link"><i class="undefined"></i>
  pandas 库的使用
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      大模型应用开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/whatIsModel.html" class="nav-link"><i class="undefined"></i>
  什么是大模型应用开发
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-basic.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发构建
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-deploy.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发部署
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vector-database.html" class="nav-link"><i class="undefined"></i>
  向量数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-pytorch.html" class="nav-link"><i class="undefined"></i>
  Fine-tuning框架PyTorch 和 Hugging face 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-code.html" aria-current="page" class="nav-link router-link-exact-active router-link-active"><i class="undefined"></i>
  手撕Transformer 代码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-knowledge.html" class="nav-link"><i class="undefined"></i>
  理解Transformer 代码必看
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-related.html" class="nav-link"><i class="undefined"></i>
  transformer 相关源码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-transformer.html" class="nav-link"><i class="undefined"></i>
  Transformers实战
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train.html" class="nav-link"><i class="undefined"></i>
  Transformers实战2
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-code.html" class="nav-link"><i class="undefined"></i>
  模型训练代码分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  Transformer 核心组件学习路线
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-question.html" class="nav-link"><i class="undefined"></i>
  Transformer 论文精读中的常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-english.html" class="nav-link"><i class="undefined"></i>
  大模型相关的英语词汇
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-case.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发案例
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-summary.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发总结
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-question.html" class="nav-link"><i class="undefined"></i>
  大模型应用常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/impl-transformer.html" class="nav-link"><i class="undefined"></i>
  手撕Transformer
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-RNN.html" class="nav-link"><i class="undefined"></i>
  RNN（循环神经网络）是什么？
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-evaluate.html" class="nav-link"><i class="undefined"></i>
  大模型评估指标
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain-rag.html" class="nav-link"><i class="undefined"></i>
  Langchain &amp; RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain.html" class="nav-link"><i class="undefined"></i>
  Langchain 核心知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag.html" class="nav-link"><i class="undefined"></i>
  RAG 知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-study.html" class="nav-link"><i class="undefined"></i>
  Langchain 学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag-pain.html" class="nav-link"><i class="undefined"></i>
  RAG 痛点分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-agent-practice.html" class="nav-link"><i class="undefined"></i>
  Langchain agent 实战作业二
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/extended-learning.html" class="nav-link"><i class="undefined"></i>
  扩展学习知识
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/self-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling 自己练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  模型微调需要的库或者方法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/lora-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  LoRA 微调
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/agent.html" class="nav-link"><i class="undefined"></i>
  Agent 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-prompt.html" class="nav-link"><i class="undefined"></i>
  练习langchain prompt的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/construct-dataset.html" class="nav-link"><i class="undefined"></i>
  如何构造数据集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/auto-model-desc.html" class="nav-link"><i class="undefined"></i>
  加载模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/compute-metrics.html" class="nav-link"><i class="undefined"></i>
  评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/ner-instruct-task.html" class="nav-link"><i class="undefined"></i>
  NER 任务指令数据构造
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-exercises-code.html" class="nav-link"><i class="undefined"></i>
  Fine-Tuning 实战作业三
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-error-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三错误收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-warn-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三警告收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-f1-scores.html" class="nav-link"><i class="undefined"></i>
  微调实战F1 分数打印
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-analyze.html" class="nav-link"><i class="undefined"></i>
  训练完结果分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step5.html" class="nav-link"><i class="undefined"></i>
  微调实战-step5--数据处理函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step9.html" class="nav-link"><i class="undefined"></i>
  微调实战-step9--评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vLLM-intro.html" class="nav-link"><i class="undefined"></i>
  vLLM
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      算法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/sort.html" class="nav-link"><i class="undefined"></i>
  排序算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/double-pointer.html" class="nav-link"><i class="undefined"></i>
  双指针算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-tree.html" class="nav-link"><i class="undefined"></i>
  二叉树
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/receive-rain.html" class="nav-link"><i class="undefined"></i>
  接雨水
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/dynamic-plan.html" class="nav-link"><i class="undefined"></i>
  动态规划
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/greedy.html" class="nav-link"><i class="undefined"></i>
  贪心算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/longasc-sequence.html" class="nav-link"><i class="undefined"></i>
  最长上升子序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-search.html" class="nav-link"><i class="undefined"></i>
  二分查找
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/reverse-list.html" class="nav-link"><i class="undefined"></i>
  反转链表
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/del-single-list.html" class="nav-link"><i class="undefined"></i>
  删除单链表-集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/other.html" class="nav-link"><i class="undefined"></i>
  其他
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/compare-al.html" class="nav-link"><i class="undefined"></i>
  m个数，最多用n次比较，找出第二大的数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/effect-bracket.html" class="nav-link"><i class="undefined"></i>
  有效的括号
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      其他
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/other/ai-agent.html" class="nav-link"><i class="undefined"></i>
  AI项目
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/python.html" class="nav-link"><i class="undefined"></i>
  python学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/conda.html" class="nav-link"><i class="undefined"></i>
  Conda 使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/using-packages.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/pytorch-know.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  transformer库中那些常用函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-collect.html" class="nav-link"><i class="undefined"></i>
  transformer库学习哪些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-1.html" class="nav-link"><i class="undefined"></i>
  transformer 使用T5模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-2.html" class="nav-link"><i class="undefined"></i>
  torch DataLoader
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-3.html" class="nav-link"><i class="undefined"></i>
  设置随机种子
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/model-refrence.html" class="nav-link"><i class="undefined"></i>
  优秀的参考文档
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/git-operate.html" class="nav-link"><i class="undefined"></i>
  git 操作命令
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/fine-tuning-adapters.html" class="nav-link"><i class="undefined"></i>
  PEFT 之Adapters
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/swanLab-info.html" class="nav-link"><i class="undefined"></i>
  深度学习之SwanLab
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/lora-0-1.html" class="nav-link"><i class="undefined"></i>
  从0到1手撕LoRA类
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      关于我
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/hmyjyghh" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/ghh_" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/cool-62-29/columns" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  知乎
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://hmyjyghh.github.io/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1c636796></div> <aside class="sidebar" data-v-1c636796><div class="personal-info-wrapper" data-v-6f92ba70 data-v-1c636796><!----> <h3 class="name" data-v-6f92ba70>
    寒梦
  </h3> <div class="num" data-v-6f92ba70><div data-v-6f92ba70><h3 data-v-6f92ba70>100</h3> <h6 data-v-6f92ba70>Articles</h6></div> <div data-v-6f92ba70><h3 data-v-6f92ba70>4</h3> <h6 data-v-6f92ba70>Tags</h6></div></div> <ul class="social-links" data-v-6f92ba70></ul> <hr data-v-6f92ba70></div> <nav class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      强大的MCP
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-knowledge.html" class="nav-link"><i class="undefined"></i>
  MCP 是什么
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/ide-mcp-server.html" class="nav-link"><i class="undefined"></i>
  IDE 使用MCP Server实操
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      Python
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/basic.html" class="nav-link"><i class="undefined"></i>
  Python 基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/data-structure.html" class="nav-link"><i class="undefined"></i>
  Python 数据结构
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/builtin-function.html" class="nav-link"><i class="undefined"></i>
  Python 内置函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/function.html" class="nav-link"><i class="undefined"></i>
  Python 函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/class.html" class="nav-link"><i class="undefined"></i>
  Python 类
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/module.html" class="nav-link"><i class="undefined"></i>
  Python 模块
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/package.html" class="nav-link"><i class="undefined"></i>
  Python 包
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/exception.html" class="nav-link"><i class="undefined"></i>
  Python 异常
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/file.html" class="nav-link"><i class="undefined"></i>
  Python 文件操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/regex.html" class="nav-link"><i class="undefined"></i>
  Python 正则表达式
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/thread.html" class="nav-link"><i class="undefined"></i>
  Python 多线程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/process.html" class="nav-link"><i class="undefined"></i>
  Python 多进程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/network.html" class="nav-link"><i class="undefined"></i>
  Python 网络编程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/database.html" class="nav-link"><i class="undefined"></i>
  Python 数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/numpy.html" class="nav-link"><i class="undefined"></i>
  Python numpy
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/array-operation.html" class="nav-link"><i class="undefined"></i>
  Python 数组操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-library.html" class="nav-link"><i class="undefined"></i>
  Python 常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/string-function.html" class="nav-link"><i class="undefined"></i>
  Python 字符串函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/practice.html" class="nav-link"><i class="undefined"></i>
  Python 日常练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-pip-install.html" class="nav-link"><i class="undefined"></i>
  pip 那些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/line-continue.html" class="nav-link"><i class="undefined"></i>
  Python 中的行续行符
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/technology-point.html" class="nav-link"><i class="undefined"></i>
  Python 中技术点练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/pandas-study.html" class="nav-link"><i class="undefined"></i>
  pandas 库的使用
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      大模型应用开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/whatIsModel.html" class="nav-link"><i class="undefined"></i>
  什么是大模型应用开发
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-basic.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发构建
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-deploy.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发部署
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vector-database.html" class="nav-link"><i class="undefined"></i>
  向量数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-pytorch.html" class="nav-link"><i class="undefined"></i>
  Fine-tuning框架PyTorch 和 Hugging face 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-code.html" aria-current="page" class="nav-link router-link-exact-active router-link-active"><i class="undefined"></i>
  手撕Transformer 代码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-knowledge.html" class="nav-link"><i class="undefined"></i>
  理解Transformer 代码必看
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-related.html" class="nav-link"><i class="undefined"></i>
  transformer 相关源码
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-transformer.html" class="nav-link"><i class="undefined"></i>
  Transformers实战
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train.html" class="nav-link"><i class="undefined"></i>
  Transformers实战2
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-code.html" class="nav-link"><i class="undefined"></i>
  模型训练代码分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  Transformer 核心组件学习路线
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-question.html" class="nav-link"><i class="undefined"></i>
  Transformer 论文精读中的常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-english.html" class="nav-link"><i class="undefined"></i>
  大模型相关的英语词汇
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-case.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发案例
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-summary.html" class="nav-link"><i class="undefined"></i>
  大模型应用开发总结
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-question.html" class="nav-link"><i class="undefined"></i>
  大模型应用常见问题
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/impl-transformer.html" class="nav-link"><i class="undefined"></i>
  手撕Transformer
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-RNN.html" class="nav-link"><i class="undefined"></i>
  RNN（循环神经网络）是什么？
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-evaluate.html" class="nav-link"><i class="undefined"></i>
  大模型评估指标
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain-rag.html" class="nav-link"><i class="undefined"></i>
  Langchain &amp; RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain.html" class="nav-link"><i class="undefined"></i>
  Langchain 核心知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag.html" class="nav-link"><i class="undefined"></i>
  RAG 知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-study.html" class="nav-link"><i class="undefined"></i>
  Langchain 学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag-pain.html" class="nav-link"><i class="undefined"></i>
  RAG 痛点分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-agent-practice.html" class="nav-link"><i class="undefined"></i>
  Langchain agent 实战作业二
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/extended-learning.html" class="nav-link"><i class="undefined"></i>
  扩展学习知识
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/self-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling 自己练习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  模型微调需要的库或者方法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/lora-fine-tuning.html" class="nav-link"><i class="undefined"></i>
  LoRA 微调
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/agent.html" class="nav-link"><i class="undefined"></i>
  Agent 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-prompt.html" class="nav-link"><i class="undefined"></i>
  练习langchain prompt的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/construct-dataset.html" class="nav-link"><i class="undefined"></i>
  如何构造数据集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/auto-model-desc.html" class="nav-link"><i class="undefined"></i>
  加载模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/compute-metrics.html" class="nav-link"><i class="undefined"></i>
  评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/ner-instruct-task.html" class="nav-link"><i class="undefined"></i>
  NER 任务指令数据构造
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-exercises-code.html" class="nav-link"><i class="undefined"></i>
  Fine-Tuning 实战作业三
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-error-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三错误收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-warn-collect.html" class="nav-link"><i class="undefined"></i>
  微调实战三警告收集
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuning-f1-scores.html" class="nav-link"><i class="undefined"></i>
  微调实战F1 分数打印
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-analyze.html" class="nav-link"><i class="undefined"></i>
  训练完结果分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step5.html" class="nav-link"><i class="undefined"></i>
  微调实战-step5--数据处理函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/finetuing-step9.html" class="nav-link"><i class="undefined"></i>
  微调实战-step9--评估函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vLLM-intro.html" class="nav-link"><i class="undefined"></i>
  vLLM
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      算法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/sort.html" class="nav-link"><i class="undefined"></i>
  排序算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/double-pointer.html" class="nav-link"><i class="undefined"></i>
  双指针算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-tree.html" class="nav-link"><i class="undefined"></i>
  二叉树
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/receive-rain.html" class="nav-link"><i class="undefined"></i>
  接雨水
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/dynamic-plan.html" class="nav-link"><i class="undefined"></i>
  动态规划
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/greedy.html" class="nav-link"><i class="undefined"></i>
  贪心算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/longasc-sequence.html" class="nav-link"><i class="undefined"></i>
  最长上升子序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-search.html" class="nav-link"><i class="undefined"></i>
  二分查找
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/reverse-list.html" class="nav-link"><i class="undefined"></i>
  反转链表
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/del-single-list.html" class="nav-link"><i class="undefined"></i>
  删除单链表-集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/other.html" class="nav-link"><i class="undefined"></i>
  其他
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/compare-al.html" class="nav-link"><i class="undefined"></i>
  m个数，最多用n次比较，找出第二大的数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/effect-bracket.html" class="nav-link"><i class="undefined"></i>
  有效的括号
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      其他
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/other/ai-agent.html" class="nav-link"><i class="undefined"></i>
  AI项目
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/python.html" class="nav-link"><i class="undefined"></i>
  python学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/conda.html" class="nav-link"><i class="undefined"></i>
  Conda 使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/using-packages.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/pytorch-know.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  transformer库中那些常用函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-collect.html" class="nav-link"><i class="undefined"></i>
  transformer库学习哪些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-1.html" class="nav-link"><i class="undefined"></i>
  transformer 使用T5模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-2.html" class="nav-link"><i class="undefined"></i>
  torch DataLoader
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-3.html" class="nav-link"><i class="undefined"></i>
  设置随机种子
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/model-refrence.html" class="nav-link"><i class="undefined"></i>
  优秀的参考文档
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/git-operate.html" class="nav-link"><i class="undefined"></i>
  git 操作命令
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/fine-tuning-adapters.html" class="nav-link"><i class="undefined"></i>
  PEFT 之Adapters
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/swanLab-info.html" class="nav-link"><i class="undefined"></i>
  深度学习之SwanLab
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/lora-0-1.html" class="nav-link"><i class="undefined"></i>
  从0到1手撕LoRA类
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      关于我
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/hmyjyghh" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/ghh_" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/cool-62-29/columns" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  知乎
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://hmyjyghh.github.io/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-2c3e9f55 data-v-1c636796><h3 class="title" data-v-2c3e9f55></h3> <!----> <label id="box" class="inputBox" data-v-2c3e9f55><input type="password" value="" data-v-2c3e9f55> <span data-v-2c3e9f55>Konck! Knock!</span> <button data-v-2c3e9f55>OK</button></label> <div class="footer" data-v-2c3e9f55><span data-v-2c3e9f55><i class="iconfont reco-theme" data-v-2c3e9f55></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-2c3e9f55>vuePress-theme-reco</a></span> <span data-v-2c3e9f55><i class="iconfont reco-copyright" data-v-2c3e9f55></i> <a data-v-2c3e9f55><span data-v-2c3e9f55>寒梦</span>
          
        <!---->
        2025
      </a></span></div></div> <div data-v-1c636796><div data-v-1c636796><main class="page"><section style="display:;"><div class="page-title"><h1 class="title">手撕Transformer 代码</h1> <div data-v-6acedb3b><i class="iconfont reco-account" data-v-6acedb3b><span data-v-6acedb3b>寒梦</span></i> <!----> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h1 id="手撕transformer-代码"><a href="#手撕transformer-代码" class="header-anchor">#</a> 手撕Transformer 代码</h1> <h2 id="参考文档"><a href="#参考文档" class="header-anchor">#</a> 参考文档：</h2> <ul><li><a href="https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/PaperNotes/Transformer%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.md" target="_blank" rel="noopener noreferrer">Transformer 论文精读<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="一-注意力机制"><a href="#一-注意力机制" class="header-anchor">#</a> 一. 注意力机制</h2> <h3 id="_1-自注意力机制-self-attention"><a href="#_1-自注意力机制-self-attention" class="header-anchor">#</a> 1. 自注意力机制(Self Attention)</h3> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># SelfAttention 类 (高层接口)</span>
<span class="token keyword">class</span> <span class="token class-name">SelfAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SelfAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 关键在这里：它创建了一个通用的Attention模块</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> Attention<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span> 

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 它直接将整个输入x作为Q, K, V传给内部的attention模块</span>
        <span class="token comment"># 在自注意力机制中，q, k, v 都来自同一输入序列</span>
        <span class="token comment"># q = k = v = x</span>
        out<span class="token punctuation">,</span> attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out<span class="token punctuation">,</span> attention_weights
</code></pre></div><h3 id="_2-交叉注意力机制"><a href="#_2-交叉注意力机制" class="header-anchor">#</a> 2. 交叉注意力机制</h3> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">CrossAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CrossAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> Attention<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>  <span class="token comment"># 使用通用 Attention 模块</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> kv<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 在交叉注意力机制中，q 和 k, v 不同</span>
        <span class="token comment"># q 来自解码器，k 和 v 来自编码器（观察模型架构图）</span>
        out<span class="token punctuation">,</span> attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>q<span class="token punctuation">,</span> kv<span class="token punctuation">,</span> kv<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out<span class="token punctuation">,</span> attention_weights
</code></pre></div><h3 id="_3-掩码注意力机制-mask-attention"><a href="#_3-掩码注意力机制-mask-attention" class="header-anchor">#</a> 3. 掩码注意力机制(Mask Attention)</h3> <div class="language-py extra-class"><pre class="language-py"><code>
</code></pre></div><h3 id="_4-多头注意力机制-multi-head-attention"><a href="#_4-多头注意力机制-multi-head-attention" class="header-anchor">#</a> 4. 多头注意力机制(Multi-Head Attention)</h3> <h4 id="_4-1-实现1"><a href="#_4-1-实现1" class="header-anchor">#</a> 4.1 实现1</h4> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># 先从符合直觉的角度构造多头。</span>

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        多头注意力机制。（暂时使用更复杂的变量名来减少理解难度，在最后将统一映射到论文的表达）
        参数:
            embed_size: 输入序列的嵌入维度。
            num_heads: 注意力头的数量，对应于数学公式中的 h。
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_size <span class="token operator">=</span> embed_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads

        <span class="token comment"># 为每个头单独定义 Q, K, V 的线性层，输出维度同为 embed_size</span>
        self<span class="token punctuation">.</span>w_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 输出线性层，用于将多头拼接后的输出映射回 embed_size</span>
        self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_heads <span class="token operator">*</span> embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        前向传播函数。
        
        参数:
            q: 查询矩阵 (batch_size, seq_len_q, embed_size)
            k: 键矩阵 (batch_size, seq_len_k, embed_size)
            v: 值矩阵 (batch_size, seq_len_v, embed_size)
            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_k)

        返回:
            out: 注意力加权后的输出
            attention_weights: 注意力权重矩阵
        &quot;&quot;&quot;</span>
        batch_size <span class="token operator">=</span> q<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        multi_head_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># 对每个头分别计算 Q, K, V，并执行缩放点积注意力</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
            Q <span class="token operator">=</span> self<span class="token punctuation">.</span>w_q<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>q<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, embed_size)</span>
            K <span class="token operator">=</span> self<span class="token punctuation">.</span>w_k<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_k, embed_size)</span>
            V <span class="token operator">=</span> self<span class="token punctuation">.</span>w_v<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_v, embed_size)</span>

            <span class="token comment"># 缩放点积注意力</span>
            scaled_attention<span class="token punctuation">,</span> _ <span class="token operator">=</span> scaled_dot_product_attention<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
            multi_head_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>scaled_attention<span class="token punctuation">)</span>

        <span class="token comment"># 将所有头的输出拼接起来</span>
        concat_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>multi_head_outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, num_heads * embed_size)</span>

        <span class="token comment"># 通过输出线性层</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>concat_out<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, embed_size)</span>

        <span class="token keyword">return</span> out
    

<span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment"># ...（使用之前的缩放点积注意力函数）</span>
    
    <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention_weights
</code></pre></div><h4 id="_4-2-实现2"><a href="#_4-2-实现2" class="header-anchor">#</a> 4.2 实现2</h4> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        多头注意力机制：每个头单独定义线性层。
        
        参数:
            embed_size: 输入序列的嵌入维度。
            num_heads: 注意力头的数量。
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> embed_size <span class="token operator">%</span> num_heads <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">&quot;embed_size 必须能被 num_heads 整除。&quot;</span>

        self<span class="token punctuation">.</span>embed_size <span class="token operator">=</span> embed_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> embed_size <span class="token operator">//</span> num_heads  <span class="token comment"># 每个头的维度</span>

        <span class="token comment"># 为每个头单独定义 Q, K, V 的线性层</span>
        self<span class="token punctuation">.</span>w_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 输出线性层，将多头拼接后的输出映射回 embed_size</span>
        self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> q<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        multi_head_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># 针对每个头独立计算 Q, K, V，并执行缩放点积注意力</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
            Q <span class="token operator">=</span> self<span class="token punctuation">.</span>w_q<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>q<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, head_dim)</span>
            K <span class="token operator">=</span> self<span class="token punctuation">.</span>w_k<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_k, head_dim)</span>
            V <span class="token operator">=</span> self<span class="token punctuation">.</span>w_v<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_v, head_dim)</span>

            <span class="token comment"># 执行缩放点积注意力</span>
            scaled_attention<span class="token punctuation">,</span> _ <span class="token operator">=</span> scaled_dot_product_attention<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
            multi_head_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>scaled_attention<span class="token punctuation">)</span>

        <span class="token comment"># 将所有头的输出拼接起来</span>
        concat_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>multi_head_outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, embed_size)</span>

        <span class="token comment"># 通过输出线性层</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>concat_out<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, embed_size)</span>

        <span class="token keyword">return</span> out

<span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>（使用之前的缩放点积注意力函数，区别在于修改了注释里面的 shape）

    <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention_weights
</code></pre></div><h4 id="_4-3-实现3"><a href="#_4-3-实现3" class="header-anchor">#</a> 4.3 实现3</h4> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> math

<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        多头注意力机制：每个头单独定义线性层。
        
        参数:
            d_model: 输入序列的嵌入维度。
            h: 注意力头的数量。
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> d_model <span class="token operator">%</span> h <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">&quot;d_model 必须能被 h 整除。&quot;</span>

        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model
        self<span class="token punctuation">.</span>h <span class="token operator">=</span> h

        <span class="token comment"># “共享”的 Q, K, V 线性层</span>
        self<span class="token punctuation">.</span>w_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>

        <span class="token comment"># 输出线性层，将多头拼接后的输出映射回 d_model</span>
        self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 获取查询和键值的序列长度</span>
        seq_len_q <span class="token operator">=</span> q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        seq_len_k <span class="token operator">=</span> k<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># 将线性变换后的“共享”矩阵拆分为多头，调整维度为 (batch_size, h, seq_len, d_k)</span>
        <span class="token comment"># d_k 就是每个注意力头的维度</span>
        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>w_q<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len_q<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        K <span class="token operator">=</span> self<span class="token punctuation">.</span>w_k<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len_k<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        V <span class="token operator">=</span> self<span class="token punctuation">.</span>w_v<span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len_k<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        <span class="token comment"># 执行缩放点积注意力</span>
        scaled_attention<span class="token punctuation">,</span> _ <span class="token operator">=</span> scaled_dot_product_attention<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

        <span class="token comment"># 合并多头并还原为 (batch_size, seq_len_q, d_model)</span>
        concat_out <span class="token operator">=</span> scaled_attention<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>

        <span class="token comment"># 通过输出线性层</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>concat_out<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, d_model)</span>

        <span class="token keyword">return</span> out

<span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    缩放点积注意力计算。

    返回:
        output: 注意力加权后的输出矩阵
        attention_weights: 注意力权重矩阵
    &quot;&quot;&quot;</span>
    d_k <span class="token operator">=</span> Q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># d_k  也就是embed_size</span>
    
    <span class="token comment"># 计算点积并进行缩放</span>
    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span>

    <span class="token comment"># 如果提供了掩码矩阵，则将掩码对应位置的分数设为 -inf</span>
    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'-inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 对缩放后的分数应用 Softmax 函数，得到注意力权重</span>
    attention_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment"># 加权求和，计算输出</span>
    output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_weights<span class="token punctuation">,</span> V<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention_weights
</code></pre></div><h2 id="二-位置编码"><a href="#二-位置编码" class="header-anchor">#</a> 二. 位置编码</h2> <h3 id="_1-positional-encoding"><a href="#_1-positional-encoding" class="header-anchor">#</a> 1. Positional Encoding</h3> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> math

<span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        位置编码，为输入序列中的每个位置添加唯一的位置表示，以引入位置信息。

        参数:
            d_model: 嵌入维度，即每个位置的编码向量的维度。
            dropout: 位置编码后应用的 Dropout 概率。
            max_len: 位置编码的最大长度，适应不同长度的输入序列。
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>  <span class="token comment"># 正如论文 5.4 节所提到的，需要将 Dropout 应用在 embedding 和 positional encoding 相加的时候</span>
        
        <span class="token comment"># 创建位置编码矩阵，形状为 (max_len, d_model)</span>
        pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 位置索引 (max_len, 1)</span>
        
        <span class="token comment"># 计算每个维度对应的频率</span>
        div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">-</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment"># 将位置和频率结合，计算 sin 和 cos</span>
        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>  <span class="token comment"># 偶数维度</span>
        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>  <span class="token comment"># 奇数维度</span>
        
        <span class="token comment"># 增加一个维度，方便后续与输入相加，形状变为 (1, max_len, d_model)</span>
        pe <span class="token operator">=</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 将位置编码注册为模型的缓冲区，不作为参数更新</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> pe<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        前向传播函数。

        参数:
            x: 输入序列的嵌入向量，形状为 (batch_size, seq_len, d_model)。

        返回:
            加入位置编码和 Dropout 后的嵌入向量，形状为 (batch_size, seq_len, d_model)。
        &quot;&quot;&quot;</span>
        <span class="token comment"># 取出与输入序列长度相同的部分位置编码，并与输入相加</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 应用 dropout</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre></div><h2 id="三-ffn"><a href="#三-ffn" class="header-anchor">#</a> 三.FFN</h2> <h3 id="_1-前馈神经网络-feed-forward-network"><a href="#_1-前馈神经网络-feed-forward-network" class="header-anchor">#</a> 1. 前馈神经网络(Feed Forward Network)</h3> <div class="language-py extra-class"><pre class="language-py"><code>
</code></pre></div><h2 id="add-norm"><a href="#add-norm" class="header-anchor">#</a> Add &amp; Norm</h2> <p><strong>Add &amp; Norm (残差和标准化)代码实现</strong></p> <h3 id="_1-add-norm"><a href="#_1-add-norm" class="header-anchor">#</a> 1. Add &amp; Norm</h3> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># Add &amp; Norm 是Transformer Block中的一个**标准操作步骤**。它明确地描述了**先执行残差连接，再执行层归一化**。</span>

<span class="token comment"># Add &amp; Norm = **残差连接 + 层归一化**</span>

<span class="token comment"># 代码实现</span>

<span class="token keyword">class</span> <span class="token class-name">SublayerConnection</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feature_size<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e-9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        子层连接，包括残差连接和层归一化，应用于 Transformer 的每个子层。

        参数:
            feature_size: 输入特征的维度大小，即归一化的特征维度。
            dropout: 残差连接中的 Dropout 概率。
            epsilon: 防止除零的小常数。
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>residual <span class="token operator">=</span> ResidualConnection<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>  <span class="token comment"># 使用 ResidualConnection 进行残差连接</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>feature_size<span class="token punctuation">,</span> epsilon<span class="token punctuation">)</span>  <span class="token comment"># 层归一化</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> sublayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 将子层输出应用 dropout 后经过残差连接后再进行归一化，可见本文「呈现」部分</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>residual<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sublayer<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre></div><blockquote><p>残差连接与层归一化：Pre-LN 与 Post-LN 的实现差异及对训练稳定性的影响。</p></blockquote> <p>这个点，在 Transformer 中也有写，暂时不做重点看</p> <h2 id="四-残差连接-residual-connection"><a href="#四-残差连接-residual-connection" class="header-anchor">#</a> 四. 残差连接(Residual Connection)</h2> <h2 id="五-层归一化-layer-normalization-ln"><a href="#五-层归一化-layer-normalization-ln" class="header-anchor">#</a> 五. 层归一化(Layer Normalization) LN</h2> <ul><li>Pre-LN: Pre-Layer Normalization   ---&gt;&gt;&gt; Pre-Norm</li> <li>Post-LN: Post-Layer Normalization   ---&gt;&gt;&gt; Post-Norm</li></ul> <p>这里的“Layer”指的是Transformer块（Transformer Block）中的子层（Sublayer），例如自注意力子层和前馈神经网络子层。</p> <blockquote><p>详细解释如下：
Add &amp; Norm  和  残差连接  和   层归一化的区别</p></blockquote> <p>它触及了Transformer架构中最精巧的设计之一。很多人会混淆这三者，但它们各司其职，共同协作。</p> <h3 id="_1-核心概念与区别-一句话总结"><a href="#_1-核心概念与区别-一句话总结" class="header-anchor">#</a> 1. 核心概念与区别（一句话总结）</h3> <ul><li><strong>残差连接</strong>：一条“高速公路”，用于<strong>解决梯度消失问题</strong>。</li> <li><strong>层归一化</strong>：一个“稳定器”，用于<strong>稳定数据分布，加速训练</strong>。</li> <li><strong>Add &amp; Norm</strong>：一个<strong>组合单元</strong>，它 = <strong>残差连接 + 层归一化</strong>。它是Transformer架构中一个具体的<strong>操作步骤</strong>。</li></ul> <hr> <h3 id="_2-逐一详解"><a href="#_2-逐一详解" class="header-anchor">#</a> 2. 逐一详解</h3> <h4 id="a-残差连接"><a href="#a-残差连接" class="header-anchor">#</a> a. 残差连接</h4> <ul><li><strong>是什么</strong>：一种将神经网络中某一层的输入<strong>直接跳过一层或多层</strong>，加到这些层的输出上的技术。公式为：<code>输出 = 输入 + 子层函数(输入)</code>。</li> <li><strong>为什么</strong>：
<ul><li><strong>核心目的</strong>：缓解深度神经网络中的<strong>梯度消失/爆炸</strong>问题。在反向传播时，梯度可以通过这条“高速公路”直接传回底层，而不必经过可能带来梯度衰减的复杂变换（如注意力计算、全连接层），使得训练非常深的网络成为可能。</li> <li><strong>额外好处</strong>：一定程度上保留了原始输入信息，防止网络退化（即层数加深后性能不升反降）。</li></ul></li></ul> <h4 id="b-层归一化"><a href="#b-层归一化" class="header-anchor">#</a> b. 层归一化</h4> <ul><li><strong>是什么</strong>：一种对<strong>单个样本</strong>在<strong>特征维度</strong>上进行标准化的技术。它会计算该样本所有特征的均值和方差，然后进行标准化（减去均值、除以方差），最后使用可学习的参数进行缩放和偏移。
<ul><li>公式：<code>LN(x) = γ * (x - μ) / √(σ² + ε) + β</code></li></ul></li> <li><strong>为什么</strong>：
<ul><li><strong>核心目的</strong>：<strong>稳定训练过程</strong>。深度神经网络中，每层的输入分布会随着前一层参数更新而发生变化（Internal Covariate Shift）。LN通过对每个样本进行归一化，将其激活值强制转换为均值为0、方差为1的标准分布，从而减小了这种分布变化，允许使用更大的学习率，加速模型收敛。</li></ul></li></ul> <h4 id="c-add-norm"><a href="#c-add-norm" class="header-anchor">#</a> c. Add &amp; Norm</h4> <ul><li><strong>是什么</strong>：这不是一个独立的技术，而是Transformer Block中的一个<strong>标准操作步骤</strong>。它明确地描述了<strong>先执行残差连接，再执行层归一化</strong>。
<ul><li>在<strong>原始Transformer (Post-LN)</strong> 中：<code>输出 = LayerNorm( 输入 + 子层(输入) )</code></li> <li>在现代<strong>Pre-LN</strong>架构中：<code>输出 = 输入 + 子层( LayerNorm(输入) )</code> （注意，这里Add和Norm的顺序变了，但“Add &amp; Norm”这个术语通常仍被用来指代这个组合单元）</li></ul></li> <li><strong>为什么</strong>：
<ul><li><strong>组合优势</strong>：它将<strong>残差连接</strong>的梯度畅通优势和<strong>层归一化</strong>的训练稳定优势<strong>结合</strong>在了一起。两者相辅相成，缺一不可。没有LN，深层Transformer的训练可能不稳定；没有残差，梯度无法有效回传，深层网络无法训练。</li></ul></li></ul> <hr> <h3 id="_3-关系与类比"><a href="#_3-关系与类比" class="header-anchor">#</a> 3. 关系与类比</h3> <p>为了更好地理解，我们可以用一个<strong>加工流水线</strong>的类比：</p> <ul><li><strong>原始输入</strong>：一块待加工的原材料。</li> <li><strong>子层函数</strong>：一个复杂的加工机器（比如“自注意力机器”或“前馈神经网络机器”）。</li> <li><strong>残差连接</strong>：一条<strong>传送带</strong>，将未加工的原材料<strong>直接运到</strong>加工机器的出口处。</li> <li><strong>层归一化</strong>：一个<strong>标准化工具</strong>，用来调整加工后产品的尺寸和形状，使其符合下一台机器要求的输入规格。</li> <li><strong>Add &amp; Norm</strong>：整个**“混合与标准化”工位**。在这个工位上，工人（或机器人）执行两个动作：
<ol><li><strong>Add</strong>：将传送带送来的原始原材料，与加工机器出来的半成品<strong>混合</strong>在一起。</li> <li><strong>Norm</strong>：立即使用标准化工具对这个<strong>混合后的结果</strong>进行标准化处理，然后送往下一个工位。</li></ol></li></ul> <p><strong>为什么先Add再Norm（Post-LN）？</strong>
因为标准化工具（Norm）需要处理的是<strong>混合后的新数据</strong>，确保它的分布是稳定的，再送给下一台机器。如果先Norm再Add，相当于先把原材料标准化，然后和机器加工的复杂输出混合，这个混合结果的分布可能依然不稳定，达不到Norm想要的效果。</p> <p><strong>为什么现代架构用Pre-LN？</strong>
因为实践发现，如果把标准化工具（Norm）放在加工机器<strong>之前</strong>，先对原材料进行预处理，再加工，然后再和原始原材料混合，这样整个流水线的运行（训练过程）会更加<strong>稳定高效</strong>，尤其对于超长的流水线（非常深的模型）。</p> <hr> <h3 id="_4-总结对比表"><a href="#_4-总结对比表" class="header-anchor">#</a> 4. 总结对比表</h3> <table><thead><tr><th style="text-align:left;">特性</th> <th style="text-align:left;">残差连接</th> <th style="text-align:left;">层归一化</th> <th style="text-align:left;">Add &amp; Norm</th></tr></thead> <tbody><tr><td style="text-align:left;"><strong>本质</strong></td> <td style="text-align:left;"><strong>一种连接方式/结构</strong></td> <td style="text-align:left;"><strong>一种标准化技术</strong></td> <td style="text-align:left;"><strong>一个具体的操作步骤</strong></td></tr> <tr><td style="text-align:left;"><strong>目的</strong></td> <td style="text-align:left;">解决梯度消失，便于训练深模型</td> <td style="text-align:left;">稳定激活值分布，加速收敛</td> <td style="text-align:left;"><strong>结合两者优势</strong>，构建Transformer核心组件</td></tr> <tr><td style="text-align:left;"><strong>操作</strong></td> <td style="text-align:left;"><code>输入 + 输出</code></td> <td style="text-align:left;"><code>(x - μ) / σ</code></td> <td style="text-align:left;"><code>LayerNorm(输入 + 子层(输入))</code></td></tr> <tr><td style="text-align:left;"><strong>关系</strong></td> <td style="text-align:left;">Add &amp; Norm 的 <strong>一部分</strong></td> <td style="text-align:left;">Add &amp; Norm 的 <strong>另一部分</strong></td> <td style="text-align:left;"><strong>= 残差连接 + 层归一化</strong></td></tr></tbody></table> <p><strong>结论：</strong>
“残差连接”和“层归一化”是两种可以独立存在的、基础性的<strong>技术思想</strong>。而“Add &amp; Norm”是Transformer架构对这两种技术思想的<strong>一种具体应用和实现方式</strong>，它们被紧密地耦合在一起，共同保证了模型的深度和训练稳定性。</p> <h2 id="六-重要点"><a href="#六-重要点" class="header-anchor">#</a> 六. 重要点</h2> <p>在 Transformer 模型中，<code>嵌入层</code>（Embedding Layer） 是处理输入和输出数据的关键步骤，因为模型实际操作的是<code>张量</code>（tensor），而非<code>字符串</code>（string）。在将输入文本传递给模型之前，首先需要进行<code>分词</code>（tokenization），即将文本拆解为多个 <code>token</code>，随后这些 token 会被映射为对应的 <code>token ID</code>，<strong>从而转换为模型可理解的数值形式</strong>。此时，数据的形状为 <code>(seq_len,)</code>，其中 <code>seq_len</code> 表示输入序列的长度。</p></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div></main></div> <!----></div> <ul class="sub-sidebar sub-sidebar-wrapper" style="width:12rem;" data-v-7115df4a data-v-1c636796><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#参考文档" class="sidebar-link reco-side-参考文档" data-v-7115df4a>参考文档：</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#一-注意力机制" class="sidebar-link reco-side-一-注意力机制" data-v-7115df4a>一. 注意力机制</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_1-自注意力机制-self-attention" class="sidebar-link reco-side-_1-自注意力机制-self-attention" data-v-7115df4a>1. 自注意力机制(Self Attention)</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_2-交叉注意力机制" class="sidebar-link reco-side-_2-交叉注意力机制" data-v-7115df4a>2. 交叉注意力机制</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_3-掩码注意力机制-mask-attention" class="sidebar-link reco-side-_3-掩码注意力机制-mask-attention" data-v-7115df4a>3. 掩码注意力机制(Mask Attention)</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_4-多头注意力机制-multi-head-attention" class="sidebar-link reco-side-_4-多头注意力机制-multi-head-attention" data-v-7115df4a>4. 多头注意力机制(Multi-Head Attention)</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#二-位置编码" class="sidebar-link reco-side-二-位置编码" data-v-7115df4a>二. 位置编码</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_1-positional-encoding" class="sidebar-link reco-side-_1-positional-encoding" data-v-7115df4a>1. Positional Encoding</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#三-ffn" class="sidebar-link reco-side-三-ffn" data-v-7115df4a>三.FFN</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_1-前馈神经网络-feed-forward-network" class="sidebar-link reco-side-_1-前馈神经网络-feed-forward-network" data-v-7115df4a>1. 前馈神经网络(Feed Forward Network)</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#add-norm" class="sidebar-link reco-side-add-norm" data-v-7115df4a>Add &amp; Norm</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_1-add-norm" class="sidebar-link reco-side-_1-add-norm" data-v-7115df4a>1. Add &amp; Norm</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#四-残差连接-residual-connection" class="sidebar-link reco-side-四-残差连接-residual-connection" data-v-7115df4a>四. 残差连接(Residual Connection)</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#五-层归一化-layer-normalization-ln" class="sidebar-link reco-side-五-层归一化-layer-normalization-ln" data-v-7115df4a>五. 层归一化(Layer Normalization) LN</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_1-核心概念与区别-一句话总结" class="sidebar-link reco-side-_1-核心概念与区别-一句话总结" data-v-7115df4a>1. 核心概念与区别（一句话总结）</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_2-逐一详解" class="sidebar-link reco-side-_2-逐一详解" data-v-7115df4a>2. 逐一详解</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_3-关系与类比" class="sidebar-link reco-side-_3-关系与类比" data-v-7115df4a>3. 关系与类比</a></li><li class="level-3" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#_4-总结对比表" class="sidebar-link reco-side-_4-总结对比表" data-v-7115df4a>4. 总结对比表</a></li><li class="level-2" data-v-7115df4a><a href="/hmblog/modelstudy/transformer-code.html#六-重要点" class="sidebar-link reco-side-六-重要点" data-v-7115df4a>六. 重要点</a></li></ul></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-65133105 data-v-65133105><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-65133105><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-65133105></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-65133105></path></svg></div><!----></div></div>
    <script src="/hmblog/assets/js/app.ba48173d.js" defer></script><script src="/hmblog/assets/js/3.ac02bfbd.js" defer></script><script src="/hmblog/assets/js/1.ba4d6411.js" defer></script><script src="/hmblog/assets/js/72.67728c96.js" defer></script><script src="/hmblog/assets/js/9.d5a05a45.js" defer></script>
  </body>
</html>
