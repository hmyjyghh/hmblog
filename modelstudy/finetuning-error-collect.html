<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OM 错误 | 寒梦的博客</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/hmblog/logo.png">
    <meta name="description" content="宝剑锋从磨砺出，梅花香自苦寒来。">
    
    <link rel="preload" href="/hmblog/assets/css/0.styles.e7d53aa5.css" as="style"><link rel="preload" href="/hmblog/assets/js/app.d50dda49.js" as="script"><link rel="preload" href="/hmblog/assets/js/7.5041dce4.js" as="script"><link rel="preload" href="/hmblog/assets/js/2.79670d2b.js" as="script"><link rel="preload" href="/hmblog/assets/js/1.1d6abb18.js" as="script"><link rel="preload" href="/hmblog/assets/js/78.90dbc4be.js" as="script"><link rel="preload" href="/hmblog/assets/js/34.b26cede8.js" as="script"><link rel="prefetch" href="/hmblog/assets/js/10.63d0ad8f.js"><link rel="prefetch" href="/hmblog/assets/js/100.b5370e15.js"><link rel="prefetch" href="/hmblog/assets/js/101.11d69293.js"><link rel="prefetch" href="/hmblog/assets/js/102.ec371acb.js"><link rel="prefetch" href="/hmblog/assets/js/103.b19e3302.js"><link rel="prefetch" href="/hmblog/assets/js/104.a5d1118b.js"><link rel="prefetch" href="/hmblog/assets/js/105.027e1e70.js"><link rel="prefetch" href="/hmblog/assets/js/106.29059eec.js"><link rel="prefetch" href="/hmblog/assets/js/107.df098f41.js"><link rel="prefetch" href="/hmblog/assets/js/108.b32395ba.js"><link rel="prefetch" href="/hmblog/assets/js/109.04680449.js"><link rel="prefetch" href="/hmblog/assets/js/11.08937b90.js"><link rel="prefetch" href="/hmblog/assets/js/110.3f0e265c.js"><link rel="prefetch" href="/hmblog/assets/js/111.267fca7d.js"><link rel="prefetch" href="/hmblog/assets/js/112.777e7460.js"><link rel="prefetch" href="/hmblog/assets/js/113.3fa5c9d1.js"><link rel="prefetch" href="/hmblog/assets/js/114.7942c611.js"><link rel="prefetch" href="/hmblog/assets/js/115.73cbc360.js"><link rel="prefetch" href="/hmblog/assets/js/116.13fbd424.js"><link rel="prefetch" href="/hmblog/assets/js/117.50aa2803.js"><link rel="prefetch" href="/hmblog/assets/js/118.66821f6e.js"><link rel="prefetch" href="/hmblog/assets/js/119.da118f68.js"><link rel="prefetch" href="/hmblog/assets/js/120.4472cbe6.js"><link rel="prefetch" href="/hmblog/assets/js/121.389268b2.js"><link rel="prefetch" href="/hmblog/assets/js/122.df5b0c2c.js"><link rel="prefetch" href="/hmblog/assets/js/123.d9c9fac7.js"><link rel="prefetch" href="/hmblog/assets/js/124.e61413bb.js"><link rel="prefetch" href="/hmblog/assets/js/125.01e4a7da.js"><link rel="prefetch" href="/hmblog/assets/js/126.c3eb0f4c.js"><link rel="prefetch" href="/hmblog/assets/js/127.ad38efca.js"><link rel="prefetch" href="/hmblog/assets/js/128.aa1a2c28.js"><link rel="prefetch" href="/hmblog/assets/js/129.8d20593d.js"><link rel="prefetch" href="/hmblog/assets/js/130.91e775b6.js"><link rel="prefetch" href="/hmblog/assets/js/131.eb9c25a2.js"><link rel="prefetch" href="/hmblog/assets/js/132.416369ca.js"><link rel="prefetch" href="/hmblog/assets/js/133.d39f05a1.js"><link rel="prefetch" href="/hmblog/assets/js/134.a8d198f5.js"><link rel="prefetch" href="/hmblog/assets/js/135.b1325deb.js"><link rel="prefetch" href="/hmblog/assets/js/136.20239e0e.js"><link rel="prefetch" href="/hmblog/assets/js/137.6b8f99e3.js"><link rel="prefetch" href="/hmblog/assets/js/138.1e5b21c9.js"><link rel="prefetch" href="/hmblog/assets/js/139.11c4327d.js"><link rel="prefetch" href="/hmblog/assets/js/14.0ac4aea5.js"><link rel="prefetch" href="/hmblog/assets/js/140.fee44758.js"><link rel="prefetch" href="/hmblog/assets/js/141.c3b89020.js"><link rel="prefetch" href="/hmblog/assets/js/142.df24c198.js"><link rel="prefetch" href="/hmblog/assets/js/143.9d2052df.js"><link rel="prefetch" href="/hmblog/assets/js/144.2f4e1d52.js"><link rel="prefetch" href="/hmblog/assets/js/145.51fc7ecd.js"><link rel="prefetch" href="/hmblog/assets/js/146.8a157335.js"><link rel="prefetch" href="/hmblog/assets/js/147.0cf4ccf1.js"><link rel="prefetch" href="/hmblog/assets/js/148.7fa222c9.js"><link rel="prefetch" href="/hmblog/assets/js/149.d2ea3e9f.js"><link rel="prefetch" href="/hmblog/assets/js/15.2cac15c3.js"><link rel="prefetch" href="/hmblog/assets/js/150.aece84af.js"><link rel="prefetch" href="/hmblog/assets/js/151.b2abf121.js"><link rel="prefetch" href="/hmblog/assets/js/152.f974f4d5.js"><link rel="prefetch" href="/hmblog/assets/js/153.576ae466.js"><link rel="prefetch" href="/hmblog/assets/js/154.feb7351a.js"><link rel="prefetch" href="/hmblog/assets/js/155.ceb7ee93.js"><link rel="prefetch" href="/hmblog/assets/js/156.59e5e60c.js"><link rel="prefetch" href="/hmblog/assets/js/157.b2d98899.js"><link rel="prefetch" href="/hmblog/assets/js/158.1d321223.js"><link rel="prefetch" href="/hmblog/assets/js/159.34fb3587.js"><link rel="prefetch" href="/hmblog/assets/js/16.41c97ec9.js"><link rel="prefetch" href="/hmblog/assets/js/160.327c24a5.js"><link rel="prefetch" href="/hmblog/assets/js/161.46989a6e.js"><link rel="prefetch" href="/hmblog/assets/js/162.aca9fc03.js"><link rel="prefetch" href="/hmblog/assets/js/163.7b750de2.js"><link rel="prefetch" href="/hmblog/assets/js/164.11817a34.js"><link rel="prefetch" href="/hmblog/assets/js/165.69f84a6d.js"><link rel="prefetch" href="/hmblog/assets/js/166.04a9455e.js"><link rel="prefetch" href="/hmblog/assets/js/167.de58f97c.js"><link rel="prefetch" href="/hmblog/assets/js/168.1fda293d.js"><link rel="prefetch" href="/hmblog/assets/js/169.658d866b.js"><link rel="prefetch" href="/hmblog/assets/js/17.29a60e10.js"><link rel="prefetch" href="/hmblog/assets/js/170.93cda1d5.js"><link rel="prefetch" href="/hmblog/assets/js/171.ec137401.js"><link rel="prefetch" href="/hmblog/assets/js/172.503542ca.js"><link rel="prefetch" href="/hmblog/assets/js/173.e01bb63d.js"><link rel="prefetch" href="/hmblog/assets/js/174.f62b3b69.js"><link rel="prefetch" href="/hmblog/assets/js/175.1e81affb.js"><link rel="prefetch" href="/hmblog/assets/js/176.c228e264.js"><link rel="prefetch" href="/hmblog/assets/js/177.8b701f5f.js"><link rel="prefetch" href="/hmblog/assets/js/178.b3040418.js"><link rel="prefetch" href="/hmblog/assets/js/179.95b66158.js"><link rel="prefetch" href="/hmblog/assets/js/18.27fd2b83.js"><link rel="prefetch" href="/hmblog/assets/js/180.28a002e6.js"><link rel="prefetch" href="/hmblog/assets/js/181.a5b2a85a.js"><link rel="prefetch" href="/hmblog/assets/js/182.b0d528f9.js"><link rel="prefetch" href="/hmblog/assets/js/183.064f0686.js"><link rel="prefetch" href="/hmblog/assets/js/184.a55849ef.js"><link rel="prefetch" href="/hmblog/assets/js/185.d30053a1.js"><link rel="prefetch" href="/hmblog/assets/js/186.43690601.js"><link rel="prefetch" href="/hmblog/assets/js/187.1885f261.js"><link rel="prefetch" href="/hmblog/assets/js/188.14bc02b8.js"><link rel="prefetch" href="/hmblog/assets/js/189.c39e7169.js"><link rel="prefetch" href="/hmblog/assets/js/19.e7351a57.js"><link rel="prefetch" href="/hmblog/assets/js/190.6df8a6cb.js"><link rel="prefetch" href="/hmblog/assets/js/191.7c034f61.js"><link rel="prefetch" href="/hmblog/assets/js/192.87493c0b.js"><link rel="prefetch" href="/hmblog/assets/js/193.614b7fd7.js"><link rel="prefetch" href="/hmblog/assets/js/194.55efe45a.js"><link rel="prefetch" href="/hmblog/assets/js/195.84ae7930.js"><link rel="prefetch" href="/hmblog/assets/js/196.9b92a9e9.js"><link rel="prefetch" href="/hmblog/assets/js/197.d5ce227e.js"><link rel="prefetch" href="/hmblog/assets/js/198.a889219a.js"><link rel="prefetch" href="/hmblog/assets/js/199.cb7acbba.js"><link rel="prefetch" href="/hmblog/assets/js/20.20706f57.js"><link rel="prefetch" href="/hmblog/assets/js/200.6b03dbe5.js"><link rel="prefetch" href="/hmblog/assets/js/201.04210c55.js"><link rel="prefetch" href="/hmblog/assets/js/202.841b96be.js"><link rel="prefetch" href="/hmblog/assets/js/203.e43080f2.js"><link rel="prefetch" href="/hmblog/assets/js/204.f9ac267d.js"><link rel="prefetch" href="/hmblog/assets/js/205.e7b6c5bc.js"><link rel="prefetch" href="/hmblog/assets/js/206.7856c113.js"><link rel="prefetch" href="/hmblog/assets/js/207.3fa7673f.js"><link rel="prefetch" href="/hmblog/assets/js/208.321f34da.js"><link rel="prefetch" href="/hmblog/assets/js/209.424fbd4a.js"><link rel="prefetch" href="/hmblog/assets/js/21.0feb36e8.js"><link rel="prefetch" href="/hmblog/assets/js/210.9674b994.js"><link rel="prefetch" href="/hmblog/assets/js/211.76ff10e6.js"><link rel="prefetch" href="/hmblog/assets/js/212.1f945f3c.js"><link rel="prefetch" href="/hmblog/assets/js/213.1b05ce01.js"><link rel="prefetch" href="/hmblog/assets/js/214.be92c192.js"><link rel="prefetch" href="/hmblog/assets/js/215.e9904e30.js"><link rel="prefetch" href="/hmblog/assets/js/216.82c8a94e.js"><link rel="prefetch" href="/hmblog/assets/js/217.42174e7c.js"><link rel="prefetch" href="/hmblog/assets/js/218.72f00225.js"><link rel="prefetch" href="/hmblog/assets/js/219.34766469.js"><link rel="prefetch" href="/hmblog/assets/js/22.40bc0c74.js"><link rel="prefetch" href="/hmblog/assets/js/220.7fa45ba3.js"><link rel="prefetch" href="/hmblog/assets/js/221.11000c3e.js"><link rel="prefetch" href="/hmblog/assets/js/222.c80e9b19.js"><link rel="prefetch" href="/hmblog/assets/js/223.55e2a3f3.js"><link rel="prefetch" href="/hmblog/assets/js/23.3f7042f4.js"><link rel="prefetch" href="/hmblog/assets/js/24.ed563c46.js"><link rel="prefetch" href="/hmblog/assets/js/25.ac1b0e72.js"><link rel="prefetch" href="/hmblog/assets/js/26.683143d5.js"><link rel="prefetch" href="/hmblog/assets/js/27.f0066995.js"><link rel="prefetch" href="/hmblog/assets/js/28.d8aebbf6.js"><link rel="prefetch" href="/hmblog/assets/js/29.411fc063.js"><link rel="prefetch" href="/hmblog/assets/js/3.1300dadf.js"><link rel="prefetch" href="/hmblog/assets/js/30.2f75779a.js"><link rel="prefetch" href="/hmblog/assets/js/31.a195dbd7.js"><link rel="prefetch" href="/hmblog/assets/js/32.a4da846d.js"><link rel="prefetch" href="/hmblog/assets/js/33.cbaf45e6.js"><link rel="prefetch" href="/hmblog/assets/js/35.b991843f.js"><link rel="prefetch" href="/hmblog/assets/js/36.ae8fa883.js"><link rel="prefetch" href="/hmblog/assets/js/37.dc5b3f34.js"><link rel="prefetch" href="/hmblog/assets/js/38.2acfc275.js"><link rel="prefetch" href="/hmblog/assets/js/39.c2783769.js"><link rel="prefetch" href="/hmblog/assets/js/4.a36b649a.js"><link rel="prefetch" href="/hmblog/assets/js/40.67bc9334.js"><link rel="prefetch" href="/hmblog/assets/js/41.0cffe87b.js"><link rel="prefetch" href="/hmblog/assets/js/42.7007a9e2.js"><link rel="prefetch" href="/hmblog/assets/js/43.4cdc1a74.js"><link rel="prefetch" href="/hmblog/assets/js/44.f4802bc7.js"><link rel="prefetch" href="/hmblog/assets/js/45.34debc14.js"><link rel="prefetch" href="/hmblog/assets/js/46.c2ce5dd7.js"><link rel="prefetch" href="/hmblog/assets/js/47.39c8ff51.js"><link rel="prefetch" href="/hmblog/assets/js/48.948d1838.js"><link rel="prefetch" href="/hmblog/assets/js/49.3a7623be.js"><link rel="prefetch" href="/hmblog/assets/js/5.ade88313.js"><link rel="prefetch" href="/hmblog/assets/js/50.ac02ad0e.js"><link rel="prefetch" href="/hmblog/assets/js/51.f16547e7.js"><link rel="prefetch" href="/hmblog/assets/js/52.7fe5922d.js"><link rel="prefetch" href="/hmblog/assets/js/53.c1c2c7fb.js"><link rel="prefetch" href="/hmblog/assets/js/54.0e306c09.js"><link rel="prefetch" href="/hmblog/assets/js/55.b421f487.js"><link rel="prefetch" href="/hmblog/assets/js/56.81af8d2b.js"><link rel="prefetch" href="/hmblog/assets/js/57.a75ace25.js"><link rel="prefetch" href="/hmblog/assets/js/58.da08dcac.js"><link rel="prefetch" href="/hmblog/assets/js/59.dab6d7fa.js"><link rel="prefetch" href="/hmblog/assets/js/6.3551780c.js"><link rel="prefetch" href="/hmblog/assets/js/60.119bb2a1.js"><link rel="prefetch" href="/hmblog/assets/js/61.edc5e570.js"><link rel="prefetch" href="/hmblog/assets/js/62.7716e6b6.js"><link rel="prefetch" href="/hmblog/assets/js/63.862022f3.js"><link rel="prefetch" href="/hmblog/assets/js/64.140c3499.js"><link rel="prefetch" href="/hmblog/assets/js/65.9f957148.js"><link rel="prefetch" href="/hmblog/assets/js/66.bfc80bb5.js"><link rel="prefetch" href="/hmblog/assets/js/67.3d565f98.js"><link rel="prefetch" href="/hmblog/assets/js/68.bf75b3e1.js"><link rel="prefetch" href="/hmblog/assets/js/69.702f7500.js"><link rel="prefetch" href="/hmblog/assets/js/70.4ff5100f.js"><link rel="prefetch" href="/hmblog/assets/js/71.8343a21d.js"><link rel="prefetch" href="/hmblog/assets/js/72.5f656cae.js"><link rel="prefetch" href="/hmblog/assets/js/73.8b747092.js"><link rel="prefetch" href="/hmblog/assets/js/74.ce37eef4.js"><link rel="prefetch" href="/hmblog/assets/js/75.146a5498.js"><link rel="prefetch" href="/hmblog/assets/js/76.4984e890.js"><link rel="prefetch" href="/hmblog/assets/js/77.2cd2e868.js"><link rel="prefetch" href="/hmblog/assets/js/79.1b20039d.js"><link rel="prefetch" href="/hmblog/assets/js/8.1407b990.js"><link rel="prefetch" href="/hmblog/assets/js/80.0494bb83.js"><link rel="prefetch" href="/hmblog/assets/js/81.ed79dacf.js"><link rel="prefetch" href="/hmblog/assets/js/82.1500a0ec.js"><link rel="prefetch" href="/hmblog/assets/js/83.6ea83899.js"><link rel="prefetch" href="/hmblog/assets/js/84.97c0c987.js"><link rel="prefetch" href="/hmblog/assets/js/85.5478ff3d.js"><link rel="prefetch" href="/hmblog/assets/js/86.eefe55cf.js"><link rel="prefetch" href="/hmblog/assets/js/87.36078833.js"><link rel="prefetch" href="/hmblog/assets/js/88.c154a1ac.js"><link rel="prefetch" href="/hmblog/assets/js/89.9d69c066.js"><link rel="prefetch" href="/hmblog/assets/js/9.7b6dd5b4.js"><link rel="prefetch" href="/hmblog/assets/js/90.52e79e0e.js"><link rel="prefetch" href="/hmblog/assets/js/91.ee7188ed.js"><link rel="prefetch" href="/hmblog/assets/js/92.e55d0541.js"><link rel="prefetch" href="/hmblog/assets/js/93.7a66a588.js"><link rel="prefetch" href="/hmblog/assets/js/94.862ef2ed.js"><link rel="prefetch" href="/hmblog/assets/js/95.3902dbfc.js"><link rel="prefetch" href="/hmblog/assets/js/96.58ae93f2.js"><link rel="prefetch" href="/hmblog/assets/js/97.b5822d64.js"><link rel="prefetch" href="/hmblog/assets/js/98.1873d69b.js"><link rel="prefetch" href="/hmblog/assets/js/99.e295e79d.js"><link rel="prefetch" href="/hmblog/assets/js/vendors~docsearch.e480d9b8.js">
    <link rel="stylesheet" href="/hmblog/assets/css/0.styles.e7d53aa5.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-7dd95ae2><div data-v-7dd95ae2><div class="password-shadow password-wrapper-out" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>寒梦的博客</h3> <p class="description" data-v-59e6cb88>宝剑锋从磨砺出，梅花香自苦寒来。</p> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><span data-v-59e6cb88>寒梦</span>
          
        <!---->
        2026
      </a></span></div></div> <div class="hide" data-v-7dd95ae2><header class="navbar" data-v-7dd95ae2><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/hmblog/" class="home-link router-link-active"><!----> <span class="site-name">寒梦的博客</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      Python
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/basic.html" class="nav-link"><i class="undefined"></i>
  Python基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/dict-knowledge.html" class="nav-link"><i class="undefined"></i>
  Python常用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/sequence.html" class="nav-link"><i class="undefined"></i>
  Python序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/list-comprehension.html" class="nav-link"><i class="undefined"></i>
  Python列表推导式
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/builtin-function.html" class="nav-link"><i class="undefined"></i>
  Python内置函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/function.html" class="nav-link"><i class="undefined"></i>
  Python函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/related-knowledge.html" class="nav-link"><i class="undefined"></i>
  Python相关知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/numpy.html" class="nav-link"><i class="undefined"></i>
  Python numpy
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/array-operation.html" class="nav-link"><i class="undefined"></i>
  Python数组操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-library.html" class="nav-link"><i class="undefined"></i>
  Python常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/string-function.html" class="nav-link"><i class="undefined"></i>
  Python字符串函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-pip-install.html" class="nav-link"><i class="undefined"></i>
  pip 那些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/line-continue.html" class="nav-link"><i class="undefined"></i>
  Python中的行续行符
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/pandas-study.html" class="nav-link"><i class="undefined"></i>
  pandas 库的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-important.html" class="nav-link"><i class="undefined"></i>
  Python几个常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-collect.html" class="nav-link"><i class="undefined"></i>
  Python汇总
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-web.html" class="nav-link"><i class="undefined"></i>
  PythonWeb框架
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-list.html" class="nav-link"><i class="undefined"></i>
  无切片，不python
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-set.html" class="nav-link"><i class="undefined"></i>
  Python中的集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-str.html" class="nav-link"><i class="undefined"></i>
  Python字符串及格式化
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-storage.html" class="nav-link"><i class="undefined"></i>
  Python永久存储
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-except.html" class="nav-link"><i class="undefined"></i>
  Python异常处理
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-class.html" class="nav-link"><i class="undefined"></i>
  Python类与对象
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-magic.html" class="nav-link"><i class="undefined"></i>
  Python里的魔法
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      大模型应用开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-basic.html" class="nav-link"><i class="undefined"></i>
  Transformer 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/basic-knowledge.html" class="nav-link"><i class="undefined"></i>
  大模型基础概念
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/prompts.html" class="nav-link"><i class="undefined"></i>
  提示词工程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/rag.html" class="nav-link"><i class="undefined"></i>
  检索增强生成RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/data-chunk.html" class="nav-link"><i class="undefined"></i>
  数据分块
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain-rag.html" class="nav-link"><i class="undefined"></i>
  Langchain &amp; RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag.html" class="nav-link"><i class="undefined"></i>
  RAG 知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag-pain.html" class="nav-link"><i class="undefined"></i>
  RAG 痛点分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-study.html" class="nav-link"><i class="undefined"></i>
  Langchain use
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vector-database.html" class="nav-link"><i class="undefined"></i>
  向量数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/first-model-project.html" class="nav-link"><i class="undefined"></i>
  RAG 项目实战
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/agent.html" class="nav-link"><i class="undefined"></i>
  Agent 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-prompt.html" class="nav-link"><i class="undefined"></i>
  LangChain Prompt的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-english.html" class="nav-link"><i class="undefined"></i>
  大模型相关的英语词汇
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/rl.html" class="nav-link"><i class="undefined"></i>
  强化学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/peft.html" class="nav-link"><i class="undefined"></i>
  大模型微调
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/quantization.html" class="nav-link"><i class="undefined"></i>
  模型量化
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vLLM-intro.html" class="nav-link"><i class="undefined"></i>
  vLLM
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-pytorch.html" class="nav-link"><i class="undefined"></i>
  PyTorch Dataset VS Huggingface Dataset
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-process.html" class="nav-link"><i class="undefined"></i>
  从零训练一个大模型的完整流程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/basic-aigc.html" class="nav-link"><i class="undefined"></i>
  生成式AI
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-related.html" class="nav-link"><i class="undefined"></i>
  模型训练相关
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      强大的MCP
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-knowledge.html" class="nav-link"><i class="undefined"></i>
  MCP 是什么
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/ide-mcp-server.html" class="nav-link"><i class="undefined"></i>
  IDE 使用MCP Server实操
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-tool.html" class="nav-link"><i class="undefined"></i>
  常见的MCP工具
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      算法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/sort.html" class="nav-link"><i class="undefined"></i>
  排序算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/double-pointer.html" class="nav-link"><i class="undefined"></i>
  双指针算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-tree.html" class="nav-link"><i class="undefined"></i>
  二叉树
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/receive-rain.html" class="nav-link"><i class="undefined"></i>
  接雨水
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/dynamic-plan.html" class="nav-link"><i class="undefined"></i>
  动态规划
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/greedy.html" class="nav-link"><i class="undefined"></i>
  贪心算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/longasc-sequence.html" class="nav-link"><i class="undefined"></i>
  最长上升子序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-search.html" class="nav-link"><i class="undefined"></i>
  二分查找
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/reverse-list.html" class="nav-link"><i class="undefined"></i>
  反转链表
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/del-single-list.html" class="nav-link"><i class="undefined"></i>
  删除单链表-集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/other.html" class="nav-link"><i class="undefined"></i>
  其他
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/compare-al.html" class="nav-link"><i class="undefined"></i>
  m个数，最多用n次比较，找出第二大的数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/effect-bracket.html" class="nav-link"><i class="undefined"></i>
  有效的括号
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      其他
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/other/ai-agent.html" class="nav-link"><i class="undefined"></i>
  AI项目
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/conda.html" class="nav-link"><i class="undefined"></i>
  Conda 使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/using-packages.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/pytorch-know.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  transformer库中那些常用函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-collect.html" class="nav-link"><i class="undefined"></i>
  transformer库学习哪些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-1.html" class="nav-link"><i class="undefined"></i>
  transformer 使用T5模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-2.html" class="nav-link"><i class="undefined"></i>
  torch DataLoader
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-3.html" class="nav-link"><i class="undefined"></i>
  设置随机种子
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/model-refrence.html" class="nav-link"><i class="undefined"></i>
  优秀的参考文档
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/git-operate.html" class="nav-link"><i class="undefined"></i>
  git 操作命令
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/fine-tuning-adapters.html" class="nav-link"><i class="undefined"></i>
  PEFT 之Adapters
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/swanLab-info.html" class="nav-link"><i class="undefined"></i>
  深度学习之SwanLab
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/lora-0-1.html" class="nav-link"><i class="undefined"></i>
  从0到1手撕LoRA类
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      关于我
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/hmyjyghh" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/ghh_" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/cool-62-29/columns" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  知乎
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://hmyjyghh.github.io/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-7dd95ae2></div> <aside class="sidebar" data-v-7dd95ae2><div class="personal-info-wrapper" data-v-1fad0c41 data-v-7dd95ae2><!----> <h3 class="name" data-v-1fad0c41>
    寒梦
  </h3> <div class="num" data-v-1fad0c41><div data-v-1fad0c41><h3 data-v-1fad0c41>146</h3> <h6 data-v-1fad0c41>Articles</h6></div> <div data-v-1fad0c41><h3 data-v-1fad0c41>4</h3> <h6 data-v-1fad0c41>Tags</h6></div></div> <ul class="social-links" data-v-1fad0c41></ul> <hr data-v-1fad0c41></div> <nav class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      Python
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/basic.html" class="nav-link"><i class="undefined"></i>
  Python基础
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/dict-knowledge.html" class="nav-link"><i class="undefined"></i>
  Python常用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/sequence.html" class="nav-link"><i class="undefined"></i>
  Python序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/list-comprehension.html" class="nav-link"><i class="undefined"></i>
  Python列表推导式
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/builtin-function.html" class="nav-link"><i class="undefined"></i>
  Python内置函数的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/function.html" class="nav-link"><i class="undefined"></i>
  Python函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/related-knowledge.html" class="nav-link"><i class="undefined"></i>
  Python相关知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/numpy.html" class="nav-link"><i class="undefined"></i>
  Python numpy
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/array-operation.html" class="nav-link"><i class="undefined"></i>
  Python数组操作
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-library.html" class="nav-link"><i class="undefined"></i>
  Python常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/string-function.html" class="nav-link"><i class="undefined"></i>
  Python字符串函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/use-pip-install.html" class="nav-link"><i class="undefined"></i>
  pip 那些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/line-continue.html" class="nav-link"><i class="undefined"></i>
  Python中的行续行符
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/pandas-study.html" class="nav-link"><i class="undefined"></i>
  pandas 库的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-important.html" class="nav-link"><i class="undefined"></i>
  Python几个常用库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-collect.html" class="nav-link"><i class="undefined"></i>
  Python汇总
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-web.html" class="nav-link"><i class="undefined"></i>
  PythonWeb框架
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-list.html" class="nav-link"><i class="undefined"></i>
  无切片，不python
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-set.html" class="nav-link"><i class="undefined"></i>
  Python中的集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-str.html" class="nav-link"><i class="undefined"></i>
  Python字符串及格式化
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-storage.html" class="nav-link"><i class="undefined"></i>
  Python永久存储
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-except.html" class="nav-link"><i class="undefined"></i>
  Python异常处理
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-class.html" class="nav-link"><i class="undefined"></i>
  Python类与对象
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/pystudy/python-magic.html" class="nav-link"><i class="undefined"></i>
  Python里的魔法
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      大模型应用开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/transformer-basic.html" class="nav-link"><i class="undefined"></i>
  Transformer 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/basic-knowledge.html" class="nav-link"><i class="undefined"></i>
  大模型基础概念
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/prompts.html" class="nav-link"><i class="undefined"></i>
  提示词工程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/rag.html" class="nav-link"><i class="undefined"></i>
  检索增强生成RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/data-chunk.html" class="nav-link"><i class="undefined"></i>
  数据分块
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-langchain-rag.html" class="nav-link"><i class="undefined"></i>
  Langchain &amp; RAG
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag.html" class="nav-link"><i class="undefined"></i>
  RAG 知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-rag-pain.html" class="nav-link"><i class="undefined"></i>
  RAG 痛点分析
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-study.html" class="nav-link"><i class="undefined"></i>
  Langchain use
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vector-database.html" class="nav-link"><i class="undefined"></i>
  向量数据库
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/first-model-project.html" class="nav-link"><i class="undefined"></i>
  RAG 项目实战
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-function-calling.html" class="nav-link"><i class="undefined"></i>
  Function Calling
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/agent.html" class="nav-link"><i class="undefined"></i>
  Agent 相关
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/langchain-prompt.html" class="nav-link"><i class="undefined"></i>
  LangChain Prompt的使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-english.html" class="nav-link"><i class="undefined"></i>
  大模型相关的英语词汇
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/rl.html" class="nav-link"><i class="undefined"></i>
  强化学习
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/peft.html" class="nav-link"><i class="undefined"></i>
  大模型微调
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/quantization.html" class="nav-link"><i class="undefined"></i>
  模型量化
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/vLLM-intro.html" class="nav-link"><i class="undefined"></i>
  vLLM
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-pytorch.html" class="nav-link"><i class="undefined"></i>
  PyTorch Dataset VS Huggingface Dataset
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-process.html" class="nav-link"><i class="undefined"></i>
  从零训练一个大模型的完整流程
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/basic-aigc.html" class="nav-link"><i class="undefined"></i>
  生成式AI
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/modelstudy/model-train-related.html" class="nav-link"><i class="undefined"></i>
  模型训练相关
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      强大的MCP
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-knowledge.html" class="nav-link"><i class="undefined"></i>
  MCP 是什么
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/ide-mcp-server.html" class="nav-link"><i class="undefined"></i>
  IDE 使用MCP Server实操
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/mcpstudy/mcp-tool.html" class="nav-link"><i class="undefined"></i>
  常见的MCP工具
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      算法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/sort.html" class="nav-link"><i class="undefined"></i>
  排序算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/double-pointer.html" class="nav-link"><i class="undefined"></i>
  双指针算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-tree.html" class="nav-link"><i class="undefined"></i>
  二叉树
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/receive-rain.html" class="nav-link"><i class="undefined"></i>
  接雨水
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/dynamic-plan.html" class="nav-link"><i class="undefined"></i>
  动态规划
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/greedy.html" class="nav-link"><i class="undefined"></i>
  贪心算法
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/longasc-sequence.html" class="nav-link"><i class="undefined"></i>
  最长上升子序列
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/binary-search.html" class="nav-link"><i class="undefined"></i>
  二分查找
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/reverse-list.html" class="nav-link"><i class="undefined"></i>
  反转链表
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/del-single-list.html" class="nav-link"><i class="undefined"></i>
  删除单链表-集合
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/other.html" class="nav-link"><i class="undefined"></i>
  其他
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/compare-al.html" class="nav-link"><i class="undefined"></i>
  m个数，最多用n次比较，找出第二大的数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/algorithm/effect-bracket.html" class="nav-link"><i class="undefined"></i>
  有效的括号
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      其他
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/hmblog/other/ai-agent.html" class="nav-link"><i class="undefined"></i>
  AI项目
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/conda.html" class="nav-link"><i class="undefined"></i>
  Conda 使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/using-packages.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架使用
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/pytorch-know.html" class="nav-link"><i class="undefined"></i>
  Pytorch 框架知识点
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-learn.html" class="nav-link"><i class="undefined"></i>
  transformer库中那些常用函数
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-collect.html" class="nav-link"><i class="undefined"></i>
  transformer库学习哪些事
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-1.html" class="nav-link"><i class="undefined"></i>
  transformer 使用T5模型
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-2.html" class="nav-link"><i class="undefined"></i>
  torch DataLoader
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/transformer-3.html" class="nav-link"><i class="undefined"></i>
  设置随机种子
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/model-refrence.html" class="nav-link"><i class="undefined"></i>
  优秀的参考文档
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/git-operate.html" class="nav-link"><i class="undefined"></i>
  git 操作命令
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/fine-tuning-adapters.html" class="nav-link"><i class="undefined"></i>
  PEFT 之Adapters
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/swanLab-info.html" class="nav-link"><i class="undefined"></i>
  深度学习之SwanLab
</a></li><li class="dropdown-item"><!----> <a href="/hmblog/other/lora-0-1.html" class="nav-link"><i class="undefined"></i>
  从0到1手撕LoRA类
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      关于我
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/hmyjyghh" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://gitee.com/ghh_" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Gitee
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://www.zhihu.com/people/cool-62-29/columns" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  知乎
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://hmyjyghh.github.io/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88></h3> <!----> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><span data-v-59e6cb88>寒梦</span>
          
        <!---->
        2026
      </a></span></div></div> <div data-v-7dd95ae2><div data-v-7dd95ae2><main class="page"><section style="display:;"><div class="page-title"><h1 class="title">OM 错误</h1> <div data-v-8a445198><i class="iconfont reco-account" data-v-8a445198><span data-v-8a445198>寒梦</span></i> <!----> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h2 id="om-错误"><a href="#om-错误" class="header-anchor">#</a> OM 错误</h2> <div class="language- extra-class"><pre class="language-text"><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 225.00 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 22.80 GiB is allocated by PyTorch, and 149.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre></div><p>这是一个典型的 <strong>GPU 内存不足</strong> 错误。你的 GPU 有 24GB 内存，但已经被占用了 23.32GB，只剩下 225MB 可用。</p> <h2 id="🔧-立即解决方案"><a href="#🔧-立即解决方案" class="header-anchor">#</a> 🔧 立即解决方案</h2> <h3 id="方案1-清理内存并减少批次大小"><a href="#方案1-清理内存并减少批次大小" class="header-anchor">#</a> 方案1：清理内存并减少批次大小</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> gc

<span class="token comment"># 清理GPU内存</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span>
gc<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 检查当前GPU内存使用情况</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;GPU内存使用: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;GPU内存保留: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_reserved<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>

<span class="token comment"># 减少批次大小</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
  output_dir<span class="token operator">=</span><span class="token string">&quot;./results&quot;</span><span class="token punctuation">,</span>
  per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token comment"># 大幅减少批次大小</span>
  per_device_eval_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
  dataloader_drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># 丢弃不完整的批次</span>
  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="方案2-使用梯度累积"><a href="#方案2-使用梯度累积" class="header-anchor">#</a> 方案2：使用梯度累积</h3> <div class="language-python extra-class"><pre class="language-python"><code>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;./results&quot;</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>      <span class="token comment"># 小批次</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>       <span class="token comment"># 梯度累积</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    dataloader_drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="方案3-启用内存优化"><a href="#方案3-启用内存优化" class="header-anchor">#</a> 方案3：启用内存优化</h3> <div class="language-python extra-class"><pre class="language-python"><code>training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;./results&quot;</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
    <span class="token comment"># 内存优化选项</span>
    dataloader_pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token comment"># 禁用内存锁定</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># 使用混合精度训练</span>
    <span class="token comment"># 或者使用bf16（如果GPU支持）</span>
    <span class="token comment"># bf16=True,</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">)</span>
</code></pre></div><h2 id="🎯-完整的优化配置"><a href="#🎯-完整的优化配置" class="header-anchor">#</a> 🎯 完整的优化配置</h2> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments<span class="token punctuation">,</span> Trainer

<span class="token comment"># 优化的训练参数</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;./results&quot;</span><span class="token punctuation">,</span>
    overwrite_output_dir<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    
    <span class="token comment"># 批次大小优化</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>      <span class="token comment"># 小批次</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>      <span class="token comment"># 等效批次大小 = 2 * 8 = 16</span>
    
    <span class="token comment"># 内存优化</span>
    dataloader_drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    dataloader_pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                          <span class="token comment"># 混合精度训练</span>
    
    <span class="token comment"># 训练参数</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">,</span>
    warmup_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
    
    <span class="token comment"># 保存和日志</span>
    logging_dir<span class="token operator">=</span><span class="token string">&quot;./logs&quot;</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>
    save_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
    eval_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
    
    <span class="token comment"># 防止内存泄漏</span>
    remove_unused_columns<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    label_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><h2 id="🔧-高级内存优化技术"><a href="#🔧-高级内存优化技术" class="header-anchor">#</a> 🔧 高级内存优化技术</h2> <h3 id="方案4-使用deepspeed-推荐用于大模型"><a href="#方案4-使用deepspeed-推荐用于大模型" class="header-anchor">#</a> 方案4：使用DeepSpeed（推荐用于大模型）</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 创建deepspeed配置</span>
deepspeed_config <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;fp16&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;enabled&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;optimizer&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;AdamW&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;params&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> <span class="token number">2e-5</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;zero_optimization&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;stage&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token comment"># 使用ZeRO第二阶段</span>
        <span class="token string">&quot;offload_optimizer&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;device&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;cpu&quot;</span>  <span class="token comment"># 将优化器卸载到CPU</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;train_batch_size&quot;</span><span class="token punctuation">:</span> <span class="token number">16</span><span class="token punctuation">,</span>
    <span class="token string">&quot;gradient_accumulation_steps&quot;</span><span class="token punctuation">:</span> <span class="token number">8</span>
<span class="token punctuation">}</span>

training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;./results&quot;</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    deepspeed<span class="token operator">=</span>deepspeed_config<span class="token punctuation">,</span>  <span class="token comment"># 使用DeepSpeed</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="方案5-模型内存优化"><a href="#方案5-模型内存优化" class="header-anchor">#</a> 方案5：模型内存优化</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForTokenClassification

<span class="token comment"># 加载模型时启用内存优化</span>
model <span class="token operator">=</span> AutoModelForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">&quot;bert-base-chinese&quot;</span><span class="token punctuation">,</span>
    num_labels<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>label2id<span class="token punctuation">)</span><span class="token punctuation">,</span>
    id2label<span class="token operator">=</span>id2label<span class="token punctuation">,</span>
    label2id<span class="token operator">=</span>label2id<span class="token punctuation">,</span>
    <span class="token comment"># 内存优化</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>  <span class="token comment"># 使用半精度</span>
<span class="token punctuation">)</span>

<span class="token comment"># 或者动态量化</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>half<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 转换为半精度</span>
</code></pre></div><h2 id="🚀-立即执行的紧急方案"><a href="#🚀-立即执行的紧急方案" class="header-anchor">#</a> 🚀 立即执行的紧急方案</h2> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 立即执行的完整方案</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> gc

<span class="token comment"># 1. 清理内存</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span>
gc<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 2. 设置环境变量（在代码开头）</span>
<span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;PYTORCH_CUDA_ALLOC_CONF&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">&quot;expandable_segments:True&quot;</span>

<span class="token comment"># 3. 最小化批次大小</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;./results&quot;</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>      <span class="token comment"># 最小批次</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>     <span class="token comment"># 累积到16</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    dataloader_drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    dataloader_pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    remove_unused_columns<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
    save_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 4. 创建Trainer</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>eval_dataset<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 5. 开始训练</span>
<span class="token keyword">try</span><span class="token punctuation">:</span>
    trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">except</span> RuntimeError <span class="token keyword">as</span> e<span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">&quot;out of memory&quot;</span> <span class="token keyword">in</span> <span class="token builtin">str</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;内存仍然不足，尝试进一步优化...&quot;</span><span class="token punctuation">)</span>
        <span class="token comment"># 进一步减少批次大小或使用CPU训练</span>
</code></pre></div><h2 id="💡-预防措施"><a href="#💡-预防措施" class="header-anchor">#</a> 💡 预防措施</h2> <h3 id="监控gpu内存"><a href="#监控gpu内存" class="header-anchor">#</a> 监控GPU内存</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">print_gpu_memory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;已分配: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;已保留: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_reserved<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;最大已分配: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>max_memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>

<span class="token comment"># 在训练循环中定期调用</span>
print_gpu_memory<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="使用更小的模型"><a href="#使用更小的模型" class="header-anchor">#</a> 使用更小的模型</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 如果仍然内存不足，使用更小的模型</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForTokenClassification

model <span class="token operator">=</span> AutoModelForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
  <span class="token string">&quot;hfl/chinese-bert-wwm-ext&quot;</span><span class="token punctuation">,</span>  <span class="token comment"># 更小的中文模型</span>
  <span class="token comment"># 或者 &quot;hfl/chinese-roberta-wwm-ext&quot;</span>
  num_labels<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>label2id<span class="token punctuation">)</span><span class="token punctuation">,</span>
  id2label<span class="token operator">=</span>id2label<span class="token punctuation">,</span>
  label2id<span class="token operator">=</span>label2id<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><h2 id="🎯-推荐执行顺序"><a href="#🎯-推荐执行顺序" class="header-anchor">#</a> 🎯 推荐执行顺序</h2> <ol><li><strong>立即执行</strong>：方案1 + 方案3（清理内存 + 小批次 + FP16）</li> <li><strong>如果仍然OOM</strong>：添加方案4（DeepSpeed）</li> <li><strong>最后手段</strong>：使用更小的模型或CPU训练</li></ol> <p>选择其中一个组合方案应该能解决你的内存问题！</p> <h2 id="om错误-清理内存后再开始训练"><a href="#om错误-清理内存后再开始训练" class="header-anchor">#</a> OM错误，清理内存后再开始训练</h2> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># 在开始训练前清理内存</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> gc

<span class="token comment"># 清理GPU内存</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span>
gc<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 检查可用内存</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;GPU内存使用情况:&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;已分配: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;已缓存: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_reserved<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
</code></pre></div><p>从您的内存使用情况来看，我发现了问题所在：</p> <h2 id="内存分析"><a href="#内存分析" class="header-anchor">#</a> 内存分析</h2> <div class="language- extra-class"><pre class="language-text"><code>已分配: 4.17 GB  (实际使用的GPU内存)
已缓存: 6.66 GB  (预分配的缓存内存)
剩余可用: 约 12.72 GB (24GB - 4.17GB - 6.66GB)
</code></pre></div><p><strong>关键问题</strong>：模型加载后已经占用了4.17GB，但训练开始时会需要更多内存用于前向传播、反向传播和优化器状态。</p> <h2 id="具体问题分析"><a href="#具体问题分析" class="header-anchor">#</a> 具体问题分析</h2> <ol><li><strong>模型参数</strong>：7B模型在float16下约需要14GB</li> <li><strong>优化器状态</strong>：Adam优化器需要2倍模型参数内存</li> <li><strong>激活值/梯度</strong>：需要额外内存</li> <li><strong>当前状态</strong>：模型只部分加载到GPU</li></ol> <h2 id="解决方案"><a href="#解决方案" class="header-anchor">#</a> 解决方案</h2> <h3 id="方案1-立即清理缓存并重新加载"><a href="#方案1-立即清理缓存并重新加载" class="header-anchor">#</a> 方案1：立即清理缓存并重新加载</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> gc

<span class="token comment"># 强制清理缓存</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span>
gc<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;清理后内存:&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;已分配: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;已缓存: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_reserved<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
</code></pre></div><h3 id="方案2-使用更小的模型-推荐"><a href="#方案2-使用更小的模型-推荐" class="header-anchor">#</a> 方案2：使用更小的模型（推荐）</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 切换到更小的模型</span>
small_model_name <span class="token operator">=</span> <span class="token string">&quot;Qwen/Qwen2.5-1.5B&quot;</span>  <span class="token comment"># 或 &quot;Qwen/Qwen2.5-0.5B&quot;</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;切换到更小的模型: </span><span class="token interpolation"><span class="token punctuation">{</span>small_model_name<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

<span class="token comment"># 重新加载tokenizer和模型</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>small_model_name<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    small_model_name<span class="token punctuation">,</span> 
    num_labels<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>label2id<span class="token punctuation">)</span><span class="token punctuation">,</span>
    id2label<span class="token operator">=</span>id2label<span class="token punctuation">,</span>
    label2id<span class="token operator">=</span>label2id<span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">&quot;auto&quot;</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="方案3-优化数据加载"><a href="#方案3-优化数据加载" class="header-anchor">#</a> 方案3：优化数据加载</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 进一步优化数据参数</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;./qwen-ner&quot;</span><span class="token punctuation">,</span>
    overwrite_output_dir<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token comment"># 必须为1</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>  <span class="token comment"># 增加梯度累积</span>
    save_strategy<span class="token operator">=</span><span class="token string">&quot;steps&quot;</span><span class="token punctuation">,</span>
    eval_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>  <span class="token comment"># 减少评估频率</span>
    save_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-4</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    dataloader_pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    remove_unused_columns<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token comment"># 添加内存优化参数</span>
    dataloader_num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token comment"># 避免多进程内存开销</span>
    no_cuda<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 优化collate_fn，减小序列长度</span>
<span class="token keyword">def</span> <span class="token function">collote_fn_memory_optimized</span><span class="token punctuation">(</span>batch_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'sentence'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch_samples<span class="token punctuation">]</span>  <span class="token comment"># 限制句子长度</span>
    batch_char_labels <span class="token operator">=</span> <span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'char_labels'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch_samples<span class="token punctuation">]</span>
    
    batch_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
        batch_sentence<span class="token punctuation">,</span>
        padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        max_length<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>  <span class="token comment"># 进一步减小序列长度</span>
        return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">,</span>
        is_split_into_words<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span>
    
    <span class="token comment"># 其余代码不变...</span>
    aligned_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> char_labels <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>batch_char_labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        word_ids <span class="token operator">=</span> batch_inputs<span class="token punctuation">.</span>word_ids<span class="token punctuation">(</span>batch_index<span class="token operator">=</span>i<span class="token punctuation">)</span>
        token_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        
        <span class="token keyword">for</span> word_idx <span class="token keyword">in</span> word_ids<span class="token punctuation">:</span>
            <span class="token keyword">if</span> word_idx <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>label2id<span class="token punctuation">[</span>char_labels<span class="token punctuation">[</span>word_idx<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        aligned_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token_labels<span class="token punctuation">)</span>
    
    batch_inputs<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>aligned_labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> batch_inputs
</code></pre></div><h3 id="方案4-分步加载和训练"><a href="#方案4-分步加载和训练" class="header-anchor">#</a> 方案4：分步加载和训练</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 分步验证内存使用</span>
<span class="token keyword">def</span> <span class="token function">step_by_step_memory_check</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;=== 分步内存检查 ===&quot;</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 1. 加载模型</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;1. 加载模型...&quot;</span><span class="token punctuation">)</span>
    model <span class="token operator">=</span> AutoModelForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
        model_name<span class="token punctuation">,</span> 
        num_labels<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>label2id<span class="token punctuation">)</span><span class="token punctuation">,</span>
        id2label<span class="token operator">=</span>id2label<span class="token punctuation">,</span>
        label2id<span class="token operator">=</span>label2id<span class="token punctuation">,</span>
        torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>
        device_map<span class="token operator">=</span><span class="token string">&quot;auto&quot;</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;   模型加载后内存: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
    
    <span class="token comment"># 2. 应用LoRA</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;2. 应用LoRA...&quot;</span><span class="token punctuation">)</span>
    peft_model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lora_config<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;   LoRA应用后内存: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> peft_model

peft_model <span class="token operator">=</span> step_by_step_memory_check<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="推荐执行顺序"><a href="#推荐执行顺序" class="header-anchor">#</a> 推荐执行顺序</h2> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 完整的优化方案</span>
<span class="token keyword">def</span> <span class="token function">optimized_training_pipeline</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 1. 清理内存</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>empty_cache<span class="token punctuation">(</span><span class="token punctuation">)</span>
    gc<span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 2. 使用小模型</span>
    small_model_name <span class="token operator">=</span> <span class="token string">&quot;Qwen/Qwen2.5-1.5B&quot;</span>  <span class="token comment"># 或 &quot;Qwen/Qwen2.5-0.5B&quot;</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;使用模型: </span><span class="token interpolation"><span class="token punctuation">{</span>small_model_name<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
    <span class="token comment"># 3. 重新加载</span>
    tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>small_model_name<span class="token punctuation">)</span>
    model <span class="token operator">=</span> AutoModelForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
        small_model_name<span class="token punctuation">,</span> 
        num_labels<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>label2id<span class="token punctuation">)</span><span class="token punctuation">,</span>
        id2label<span class="token operator">=</span>id2label<span class="token punctuation">,</span>
        label2id<span class="token operator">=</span>label2id<span class="token punctuation">,</span>
        torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>
        device_map<span class="token operator">=</span><span class="token string">&quot;auto&quot;</span>
    <span class="token punctuation">)</span>
    
    <span class="token comment"># 4. 检查内存</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;模型加载后内存: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
    
    <span class="token comment"># 5. 应用LoRA</span>
    peft_model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lora_config<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;LoRA应用后内存: </span><span class="token interpolation"><span class="token punctuation">{</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>memory_allocated<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1024</span><span class="token operator">**</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string"> GB&quot;</span></span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> peft_model<span class="token punctuation">,</span> tokenizer

<span class="token comment"># 执行优化流程</span>
peft_model<span class="token punctuation">,</span> tokenizer <span class="token operator">=</span> optimized_training_pipeline<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="关键建议"><a href="#关键建议" class="header-anchor">#</a> 关键建议</h2> <p><strong>强烈建议使用 1.5B 或 0.5B 模型</strong>，原因：</p> <ul><li>7B模型在您的环境下训练会很困难</li> <li>1.5B模型在NER任务上表现已经不错</li> <li>内存占用更可控，训练速度更快</li></ul> <p>您的24GB GPU更适合训练1.5B-3B规模的模型，7B模型需要更多的优化和可能的多GPU配置。</p> <hr> <h2 id="错误2-step5-数据处理函数上面"><a href="#错误2-step5-数据处理函数上面" class="header-anchor">#</a> 错误2，step5 数据处理函数上面</h2> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>batch_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre></div><p><strong>这个写法也报错</strong>, 待研究</p> <div class="language-py extra-class"><pre class="language-py"><code><span class="token comment"># step5 定义数据处理函数</span>
<span class="token keyword">def</span> <span class="token function">process_func_1</span><span class="token punctuation">(</span>batch_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
  batch_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'sentence'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch_samples<span class="token punctuation">]</span>
  batch_labels <span class="token operator">=</span> <span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch_samples<span class="token punctuation">]</span>
  
  batch_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
      batch_sentence<span class="token punctuation">,</span>
      padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
      truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
      max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
      return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">,</span>
      is_split_into_words<span class="token operator">=</span><span class="token boolean">True</span>
  <span class="token punctuation">)</span>
  
  aligned_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> i<span class="token punctuation">,</span> labels <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>batch_labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
      word_ids <span class="token operator">=</span> batch_inputs<span class="token punctuation">.</span>word_ids<span class="token punctuation">(</span>batch_index<span class="token operator">=</span>i<span class="token punctuation">)</span>
      token_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      
      <span class="token keyword">for</span> word_idx <span class="token keyword">in</span> word_ids<span class="token punctuation">:</span>
          <span class="token keyword">if</span> word_idx <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
              token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">)</span>
          <span class="token keyword">else</span><span class="token punctuation">:</span>
              token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>label2id<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>word_idx<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
      
      aligned_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token_labels<span class="token punctuation">)</span>
  
  <span class="token comment"># batch_inputs['labels'] = torch.tensor(aligned_labels, dtype=torch.long)</span>
  batch_inputs<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>aligned_labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
  <span class="token comment"># 注意看这一步，参考代码没有这一步操作，</span>
  
  <span class="token keyword">return</span> <span class="token punctuation">{</span>
    <span class="token string">'input_ids'</span><span class="token punctuation">:</span> batch_inputs<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> batch_inputs<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
    <span class="token string">'labels'</span><span class="token punctuation">:</span> batch_inputs<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span>
    <span class="token comment"># &quot;labels&quot;: all_labels # 参考代码里直接用的是:  aligned_labels 参考代码里面的变量是: all_labels</span>
  <span class="token punctuation">}</span>
</code></pre></div><hr> <p><strong>这个写法也是报错的，只能先用丁师兄给的</strong> <code>ner_qwen_lora.ipynb</code> 这个文件里面的写法</p> <blockquote><p>这个文件地址在: D:\ghh\Downloads\feishu\ner_qwen_lora.ipynb, 里面没用 <code>word_ids()</code></p></blockquote> <div class="language-py extra-class"><pre class="language-py"><code><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>batch_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
  batch_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'sentence'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch_samples<span class="token punctuation">]</span>
  batch_labels <span class="token operator">=</span> <span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch_samples<span class="token punctuation">]</span>
  
  batch_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>
    batch_sentence<span class="token punctuation">,</span>
    padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
    return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">,</span>
    is_split_into_words<span class="token operator">=</span><span class="token boolean">True</span>
  <span class="token punctuation">)</span>
  
  <span class="token comment"># 标签对齐逻辑</span>
  aligned_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>sentence<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>batch_sentence<span class="token punctuation">,</span> batch_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      word_ids <span class="token operator">=</span> batch_inputs<span class="token punctuation">.</span>word_ids<span class="token punctuation">(</span>batch_index<span class="token operator">=</span>i<span class="token punctuation">)</span>
      token_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      
      <span class="token comment"># 先创建字符级标签序列</span>
      char_level_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'O'</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>
      <span class="token keyword">for</span> entity <span class="token keyword">in</span> labels<span class="token punctuation">:</span>
          start<span class="token punctuation">,</span> end<span class="token punctuation">,</span> _<span class="token punctuation">,</span> entity_type <span class="token operator">=</span> entity
          <span class="token comment"># 设置BIO标签</span>
          char_level_labels<span class="token punctuation">[</span>start<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f'B-</span><span class="token interpolation"><span class="token punctuation">{</span>entity_type<span class="token punctuation">}</span></span><span class="token string">'</span></span>
          <span class="token keyword">for</span> pos <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>start <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
              <span class="token keyword">if</span> pos <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>char_level_labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
                  char_level_labels<span class="token punctuation">[</span>pos<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f'I-</span><span class="token interpolation"><span class="token punctuation">{</span>entity_type<span class="token punctuation">}</span></span><span class="token string">'</span></span>
      
      <span class="token comment"># 现在将字符级标签对齐到token级</span>
      previous_word_idx <span class="token operator">=</span> <span class="token boolean">None</span>
      <span class="token keyword">for</span> word_idx <span class="token keyword">in</span> word_ids<span class="token punctuation">:</span>
          <span class="token keyword">if</span> word_idx <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
              token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">)</span>
          <span class="token keyword">elif</span> word_idx <span class="token operator">!=</span> previous_word_idx<span class="token punctuation">:</span>
              <span class="token comment"># 新单词的第一个token</span>
              token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>label2id<span class="token punctuation">[</span>char_level_labels<span class="token punctuation">[</span>word_idx<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
          <span class="token keyword">else</span><span class="token punctuation">:</span>
              <span class="token comment"># 同一单词的后续subword tokens</span>
              token_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">)</span>  <span class="token comment"># 或者 label2id[char_level_labels[word_idx]]</span>
          previous_word_idx <span class="token operator">=</span> word_idx
      
      aligned_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token_labels<span class="token punctuation">)</span>
  
  batch_inputs<span class="token punctuation">[</span><span class="token string">'labels'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>aligned_labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
  
  <span class="token keyword">return</span> batch_inputs
</code></pre></div><h2 id="其他错误在-d-ghh-model-blog-model-blog-docs-vuepress-public-images-fine-tuning-error"><a href="#其他错误在-d-ghh-model-blog-model-blog-docs-vuepress-public-images-fine-tuning-error" class="header-anchor">#</a> 其他错误在：D:\ghh\model\blog\model-blog\docs.vuepress\public\images\fine-tuning\error</h2> <p>在这里，图片形式</p></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div></main></div> <!----></div> <ul class="sub-sidebar sub-sidebar-wrapper" style="width:12rem;" data-v-b57cc07c data-v-7dd95ae2><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#om-错误" class="sidebar-link reco-side-om-错误" data-v-b57cc07c>OM 错误</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#🔧-立即解决方案" class="sidebar-link reco-side-🔧-立即解决方案" data-v-b57cc07c>🔧 立即解决方案</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案1-清理内存并减少批次大小" class="sidebar-link reco-side-方案1-清理内存并减少批次大小" data-v-b57cc07c>方案1：清理内存并减少批次大小</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案2-使用梯度累积" class="sidebar-link reco-side-方案2-使用梯度累积" data-v-b57cc07c>方案2：使用梯度累积</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案3-启用内存优化" class="sidebar-link reco-side-方案3-启用内存优化" data-v-b57cc07c>方案3：启用内存优化</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#🎯-完整的优化配置" class="sidebar-link reco-side-🎯-完整的优化配置" data-v-b57cc07c>🎯 完整的优化配置</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#🔧-高级内存优化技术" class="sidebar-link reco-side-🔧-高级内存优化技术" data-v-b57cc07c>🔧 高级内存优化技术</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案4-使用deepspeed-推荐用于大模型" class="sidebar-link reco-side-方案4-使用deepspeed-推荐用于大模型" data-v-b57cc07c>方案4：使用DeepSpeed（推荐用于大模型）</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案5-模型内存优化" class="sidebar-link reco-side-方案5-模型内存优化" data-v-b57cc07c>方案5：模型内存优化</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#🚀-立即执行的紧急方案" class="sidebar-link reco-side-🚀-立即执行的紧急方案" data-v-b57cc07c>🚀 立即执行的紧急方案</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#💡-预防措施" class="sidebar-link reco-side-💡-预防措施" data-v-b57cc07c>💡 预防措施</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#监控gpu内存" class="sidebar-link reco-side-监控gpu内存" data-v-b57cc07c>监控GPU内存</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#使用更小的模型" class="sidebar-link reco-side-使用更小的模型" data-v-b57cc07c>使用更小的模型</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#🎯-推荐执行顺序" class="sidebar-link reco-side-🎯-推荐执行顺序" data-v-b57cc07c>🎯 推荐执行顺序</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#om错误-清理内存后再开始训练" class="sidebar-link reco-side-om错误-清理内存后再开始训练" data-v-b57cc07c>OM错误，清理内存后再开始训练</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#内存分析" class="sidebar-link reco-side-内存分析" data-v-b57cc07c>内存分析</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#具体问题分析" class="sidebar-link reco-side-具体问题分析" data-v-b57cc07c>具体问题分析</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#解决方案" class="sidebar-link reco-side-解决方案" data-v-b57cc07c>解决方案</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案1-立即清理缓存并重新加载" class="sidebar-link reco-side-方案1-立即清理缓存并重新加载" data-v-b57cc07c>方案1：立即清理缓存并重新加载</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案2-使用更小的模型-推荐" class="sidebar-link reco-side-方案2-使用更小的模型-推荐" data-v-b57cc07c>方案2：使用更小的模型（推荐）</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案3-优化数据加载" class="sidebar-link reco-side-方案3-优化数据加载" data-v-b57cc07c>方案3：优化数据加载</a></li><li class="level-3" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#方案4-分步加载和训练" class="sidebar-link reco-side-方案4-分步加载和训练" data-v-b57cc07c>方案4：分步加载和训练</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#推荐执行顺序" class="sidebar-link reco-side-推荐执行顺序" data-v-b57cc07c>推荐执行顺序</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#关键建议" class="sidebar-link reco-side-关键建议" data-v-b57cc07c>关键建议</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#错误2-step5-数据处理函数上面" class="sidebar-link reco-side-错误2-step5-数据处理函数上面" data-v-b57cc07c>错误2，step5 数据处理函数上面</a></li><li class="level-2" data-v-b57cc07c><a href="/hmblog/modelstudy/finetuning-error-collect.html#其他错误在-d-ghh-model-blog-model-blog-docs-vuepress-public-images-fine-tuning-error" class="sidebar-link reco-side-其他错误在-d-ghh-model-blog-model-blog-docs-vuepress-public-images-fine-tuning-error" data-v-b57cc07c>其他错误在：D:\ghh\model\blog\model-blog\docs\.vuepress\public\images\fine-tuning\error</a></li></ul></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><!----></div></div>
    <script src="/hmblog/assets/js/app.d50dda49.js" defer></script><script src="/hmblog/assets/js/7.5041dce4.js" defer></script><script src="/hmblog/assets/js/2.79670d2b.js" defer></script><script src="/hmblog/assets/js/1.1d6abb18.js" defer></script><script src="/hmblog/assets/js/78.90dbc4be.js" defer></script><script src="/hmblog/assets/js/34.b26cede8.js" defer></script>
  </body>
</html>
