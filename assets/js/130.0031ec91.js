(window.webpackJsonp=window.webpackJsonp||[]).push([[130],{572:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"手撕transformer-代码"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#手撕transformer-代码"}},[t._v("#")]),t._v(" 手撕Transformer 代码")]),t._v(" "),s("h2",{attrs:{id:"参考文档"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考文档"}},[t._v("#")]),t._v(" 参考文档：")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/PaperNotes/Transformer%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer 论文精读"),s("OutboundLink")],1)])]),t._v(" "),s("h2",{attrs:{id:"一-注意力机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一-注意力机制"}},[t._v("#")]),t._v(" 一. 注意力机制")]),t._v(" "),s("ul",[s("li",[t._v("关注的是：输入序列")])]),t._v(" "),s("h3",{attrs:{id:"_1-自注意力机制-self-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-自注意力机制-self-attention"}},[t._v("#")]),t._v(" 1. 自注意力机制(Self Attention)")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# SelfAttention 类 (高层接口)")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SelfAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SelfAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 关键在这里：它创建了一个通用的Attention模块")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 它直接将整个输入x作为Q, K, V传给内部的attention模块")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在自注意力机制中，q, k, v 都来自同一输入序列")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# q = k = v = x")]),t._v("\n        out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights\n")])])]),s("h3",{attrs:{id:"_2-交叉注意力机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-交叉注意力机制"}},[t._v("#")]),t._v(" 2. 交叉注意力机制")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CrossAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CrossAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用通用 Attention 模块")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在交叉注意力机制中，q 和 k, v 不同")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# q 来自解码器，k 和 v 来自编码器（观察模型架构图）")]),t._v("\n        out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights\n")])])]),s("h3",{attrs:{id:"_3-掩码注意力机制-mask-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-掩码注意力机制-mask-attention"}},[t._v("#")]),t._v(" 3. 掩码注意力机制(Mask Attention)")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[t._v("\n")])])]),s("h3",{attrs:{id:"_4-多头注意力机制-multi-head-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-多头注意力机制-multi-head-attention"}},[t._v("#")]),t._v(" 4. 多头注意力机制(Multi-Head Attention)")]),t._v(" "),s("h4",{attrs:{id:"_4-1-实现1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-实现1"}},[t._v("#")]),t._v(" 4.1 实现1")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先从符合直觉的角度构造多头。")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" F\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        多头注意力机制。（暂时使用更复杂的变量名来减少理解难度，在最后将统一映射到论文的表达）\n        参数:\n            embed_size: 输入序列的嵌入维度。\n            num_heads: 注意力头的数量，对应于数学公式中的 h。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" embed_size\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_heads "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_heads\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 为每个头单独定义 Q, K, V 的线性层，输出维度同为 embed_size")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModuleList"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_k "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModuleList"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_v "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModuleList"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出线性层，用于将多头拼接后的输出映射回 embed_size")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        前向传播函数。\n        \n        参数:\n            q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n            k: 键矩阵 (batch_size, seq_len_k, embed_size)\n            v: 值矩阵 (batch_size, seq_len_v, embed_size)\n            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_k)\n\n        返回:\n            out: 注意力加权后的输出\n            attention_weights: 注意力权重矩阵\n        """')]),t._v("\n        batch_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        multi_head_outputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对每个头分别计算 Q, K, V，并执行缩放点积注意力")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            Q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, embed_size)")]),t._v("\n            K "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_k, embed_size)")]),t._v("\n            V "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_v, embed_size)")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 缩放点积注意力")]),t._v("\n            scaled_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scaled_dot_product_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            multi_head_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scaled_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将所有头的输出拼接起来")]),t._v("\n        concat_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("multi_head_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, num_heads * embed_size)")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 通过输出线性层")]),t._v("\n        out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("concat_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, embed_size)")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n    \n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("scaled_dot_product_attention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ...（使用之前的缩放点积注意力函数）")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights\n")])])]),s("h4",{attrs:{id:"_4-2-实现2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-实现2"}},[t._v("#")]),t._v(" 4.2 实现2")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" F\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        多头注意力机制：每个头单独定义线性层。\n        \n        参数:\n            embed_size: 输入序列的嵌入维度。\n            num_heads: 注意力头的数量。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" embed_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" num_heads "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embed_size 必须能被 num_heads 整除。"')]),t._v("\n\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("embed_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" embed_size\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_heads "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_heads\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head_dim "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" embed_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v(" num_heads  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每个头的维度")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 为每个头单独定义 Q, K, V 的线性层")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModuleList"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head_dim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_k "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModuleList"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head_dim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_v "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ModuleList"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head_dim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出线性层，将多头拼接后的输出映射回 embed_size")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        batch_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        multi_head_outputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 针对每个头独立计算 Q, K, V，并执行缩放点积注意力")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            Q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, head_dim)")]),t._v("\n            K "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_k, head_dim)")]),t._v("\n            V "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_v, head_dim)")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行缩放点积注意力")]),t._v("\n            scaled_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scaled_dot_product_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            multi_head_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scaled_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将所有头的输出拼接起来")]),t._v("\n        concat_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("multi_head_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, embed_size)")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 通过输出线性层")]),t._v("\n        out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("concat_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, embed_size)")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("scaled_dot_product_attention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("（使用之前的缩放点积注意力函数，区别在于修改了注释里面的 shape）\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights\n")])])]),s("h4",{attrs:{id:"_4-3-实现3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-实现3"}},[t._v("#")]),t._v(" 4.3 实现3")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" F\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" math\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" h"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        多头注意力机制：每个头单独定义线性层。\n        \n        参数:\n            d_model: 输入序列的嵌入维度。\n            h: 注意力头的数量。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" d_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" h "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"d_model 必须能被 h 整除。"')]),t._v("\n\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("d_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" d_model\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("h "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" h\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# “共享”的 Q, K, V 线性层")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_k "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_v "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出线性层，将多头拼接后的输出映射回 d_model")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        batch_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取查询和键值的序列长度")]),t._v("\n        seq_len_q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        seq_len_k "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将线性变换后的“共享”矩阵拆分为多头，调整维度为 (batch_size, h, seq_len, d_k)")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# d_k 就是每个注意力头的维度")]),t._v("\n        Q "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" seq_len_q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("h"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        K "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" seq_len_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("h"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        V "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("w_v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" seq_len_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("h"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行缩放点积注意力")]),t._v("\n        scaled_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scaled_dot_product_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 合并多头并还原为 (batch_size, seq_len_q, d_model)")]),t._v("\n        concat_out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scaled_attention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contiguous"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 通过输出线性层")]),t._v("\n        out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("concat_out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch_size, seq_len_q, d_model)")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("scaled_dot_product_attention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    缩放点积注意力计算。\n\n    返回:\n        output: 注意力加权后的输出矩阵\n        attention_weights: 注意力权重矩阵\n    """')]),t._v("\n    d_k "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# d_k  也就是embed_size")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算点积并进行缩放")]),t._v("\n    scores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Q"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" K"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" math"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d_k"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果提供了掩码矩阵，则将掩码对应位置的分数设为 -inf")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" mask "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        scores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("masked_fill"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mask "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'-inf'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对缩放后的分数应用 Softmax 函数，得到注意力权重")]),t._v("\n    attention_weights "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加权求和，计算输出")]),t._v("\n    output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("attention_weights"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" V"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attention_weights\n")])])]),s("h2",{attrs:{id:"二-位置编码"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二-位置编码"}},[t._v("#")]),t._v(" 二. 位置编码")]),t._v(" "),s("h3",{attrs:{id:"_1-positional-encoding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-positional-encoding"}},[t._v("#")]),t._v(" 1. Positional Encoding")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" math\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("PositionalEncoding")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dropout"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_len"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        位置编码，为输入序列中的每个位置添加唯一的位置表示，以引入位置信息。\n\n        参数:\n            d_model: 嵌入维度，即每个位置的编码向量的维度。\n            dropout: 位置编码后应用的 Dropout 概率。\n            max_len: 位置编码的最大长度，适应不同长度的输入序列。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("PositionalEncoding"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 正如论文 5.4 节所提到的，需要将 Dropout 应用在 embedding 和 positional encoding 相加的时候")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建位置编码矩阵，形状为 (max_len, d_model)")]),t._v("\n        pe "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_len"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        position "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_len"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 位置索引 (max_len, 1)")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算每个维度对应的频率")]),t._v("\n        div_term "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("math"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将位置和频率结合，计算 sin 和 cos")]),t._v("\n        pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sin"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" div_term"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 偶数维度")]),t._v("\n        pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" div_term"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 奇数维度")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增加一个维度，方便后续与输入相加，形状变为 (1, max_len, d_model)")]),t._v("\n        pe "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将位置编码注册为模型的缓冲区，不作为参数更新")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register_buffer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pe'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        前向传播函数。\n\n        参数:\n            x: 输入序列的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n\n        返回:\n            加入位置编码和 Dropout 后的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 取出与输入序列长度相同的部分位置编码，并与输入相加")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 应用 dropout")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"三-ffn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三-ffn"}},[t._v("#")]),t._v(" 三.FFN")]),t._v(" "),s("h3",{attrs:{id:"_1-前馈神经网络-feed-forward-network"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-前馈神经网络-feed-forward-network"}},[t._v("#")]),t._v(" 1. 前馈神经网络(Feed Forward Network)")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[t._v("\n")])])]),s("h2",{attrs:{id:"add-norm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#add-norm"}},[t._v("#")]),t._v(" Add & Norm")]),t._v(" "),s("p",[s("strong",[t._v("Add & Norm (残差和标准化)代码实现")])]),t._v(" "),s("h3",{attrs:{id:"_1-add-norm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-add-norm"}},[t._v("#")]),t._v(" 1. Add & Norm")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Add & Norm 是Transformer Block中的一个**标准操作步骤**。它明确地描述了**先执行残差连接，再执行层归一化**。")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Add & Norm = **残差连接 + 层归一化**")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 代码实现")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SublayerConnection")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feature_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dropout"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epsilon"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        子层连接，包括残差连接和层归一化，应用于 Transformer 的每个子层。\n\n        参数:\n            feature_size: 输入特征的维度大小，即归一化的特征维度。\n            dropout: 残差连接中的 Dropout 概率。\n            epsilon: 防止除零的小常数。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SublayerConnection"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("residual "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ResidualConnection"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用 ResidualConnection 进行残差连接")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LayerNorm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feature_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epsilon"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 层归一化")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sublayer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将子层输出应用 dropout 后经过残差连接后再进行归一化，可见本文「呈现」部分")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("norm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("residual"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sublayer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("blockquote",[s("p",[t._v("残差连接与层归一化：Pre-LN 与 Post-LN 的实现差异及对训练稳定性的影响。")])]),t._v(" "),s("p",[t._v("这个点，在 Transformer 中也有写，暂时不做重点看")]),t._v(" "),s("h2",{attrs:{id:"四-残差连接-residual-connection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四-残差连接-residual-connection"}},[t._v("#")]),t._v(" 四. 残差连接(Residual Connection)")]),t._v(" "),s("h2",{attrs:{id:"五-层归一化-layer-normalization-ln"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#五-层归一化-layer-normalization-ln"}},[t._v("#")]),t._v(" 五. 层归一化(Layer Normalization) LN")]),t._v(" "),s("ul",[s("li",[t._v("Pre-LN: Pre-Layer Normalization   ---\x3e>> Pre-Norm")]),t._v(" "),s("li",[t._v("Post-LN: Post-Layer Normalization   ---\x3e>> Post-Norm")])]),t._v(" "),s("p",[t._v("这里的“Layer”指的是Transformer块（Transformer Block）中的子层（Sublayer），例如自注意力子层和前馈神经网络子层。")]),t._v(" "),s("blockquote",[s("p",[t._v("详细解释如下：\nAdd & Norm  和  残差连接  和   层归一化的区别")])]),t._v(" "),s("p",[t._v("它触及了Transformer架构中最精巧的设计之一。很多人会混淆这三者，但它们各司其职，共同协作。")]),t._v(" "),s("h3",{attrs:{id:"_1-核心概念与区别-一句话总结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-核心概念与区别-一句话总结"}},[t._v("#")]),t._v(" 1. 核心概念与区别（一句话总结）")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("残差连接")]),t._v("：一条“高速公路”，用于"),s("strong",[t._v("解决梯度消失问题")]),t._v("。")]),t._v(" "),s("li",[s("strong",[t._v("层归一化")]),t._v("：一个“稳定器”，用于"),s("strong",[t._v("稳定数据分布，加速训练")]),t._v("。")]),t._v(" "),s("li",[s("strong",[t._v("Add & Norm")]),t._v("：一个"),s("strong",[t._v("组合单元")]),t._v("，它 = "),s("strong",[t._v("残差连接 + 层归一化")]),t._v("。它是Transformer架构中一个具体的"),s("strong",[t._v("操作步骤")]),t._v("。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"_2-逐一详解"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-逐一详解"}},[t._v("#")]),t._v(" 2. 逐一详解")]),t._v(" "),s("h4",{attrs:{id:"a-残差连接"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#a-残差连接"}},[t._v("#")]),t._v(" a. 残差连接")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("是什么")]),t._v("：一种将神经网络中某一层的输入"),s("strong",[t._v("直接跳过一层或多层")]),t._v("，加到这些层的输出上的技术。公式为："),s("code",[t._v("输出 = 输入 + 子层函数(输入)")]),t._v("。")]),t._v(" "),s("li",[s("strong",[t._v("为什么")]),t._v("：\n"),s("ul",[s("li",[s("strong",[t._v("核心目的")]),t._v("：缓解深度神经网络中的"),s("strong",[t._v("梯度消失/爆炸")]),t._v("问题。在反向传播时，梯度可以通过这条“高速公路”直接传回底层，而不必经过可能带来梯度衰减的复杂变换（如注意力计算、全连接层），使得训练非常深的网络成为可能。")]),t._v(" "),s("li",[s("strong",[t._v("额外好处")]),t._v("：一定程度上保留了原始输入信息，防止网络退化（即层数加深后性能不升反降）。")])])])]),t._v(" "),s("h4",{attrs:{id:"b-层归一化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#b-层归一化"}},[t._v("#")]),t._v(" b. 层归一化")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("是什么")]),t._v("：一种对"),s("strong",[t._v("单个样本")]),t._v("在"),s("strong",[t._v("特征维度")]),t._v("上进行标准化的技术。它会计算该样本所有特征的均值和方差，然后进行标准化（减去均值、除以方差），最后使用可学习的参数进行缩放和偏移。\n"),s("ul",[s("li",[t._v("公式："),s("code",[t._v("LN(x) = γ * (x - μ) / √(σ² + ε) + β")])])])]),t._v(" "),s("li",[s("strong",[t._v("为什么")]),t._v("：\n"),s("ul",[s("li",[s("strong",[t._v("核心目的")]),t._v("："),s("strong",[t._v("稳定训练过程")]),t._v("。深度神经网络中，每层的输入分布会随着前一层参数更新而发生变化（Internal Covariate Shift）。LN通过对每个样本进行归一化，将其激活值强制转换为均值为0、方差为1的标准分布，从而减小了这种分布变化，允许使用更大的学习率，加速模型收敛。")])])])]),t._v(" "),s("h4",{attrs:{id:"c-add-norm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#c-add-norm"}},[t._v("#")]),t._v(" c. Add & Norm")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("是什么")]),t._v("：这不是一个独立的技术，而是Transformer Block中的一个"),s("strong",[t._v("标准操作步骤")]),t._v("。它明确地描述了"),s("strong",[t._v("先执行残差连接，再执行层归一化")]),t._v("。\n"),s("ul",[s("li",[t._v("在"),s("strong",[t._v("原始Transformer (Post-LN)")]),t._v(" 中："),s("code",[t._v("输出 = LayerNorm( 输入 + 子层(输入) )")])]),t._v(" "),s("li",[t._v("在现代"),s("strong",[t._v("Pre-LN")]),t._v("架构中："),s("code",[t._v("输出 = 输入 + 子层( LayerNorm(输入) )")]),t._v(" （注意，这里Add和Norm的顺序变了，但“Add & Norm”这个术语通常仍被用来指代这个组合单元）")])])]),t._v(" "),s("li",[s("strong",[t._v("为什么")]),t._v("：\n"),s("ul",[s("li",[s("strong",[t._v("组合优势")]),t._v("：它将"),s("strong",[t._v("残差连接")]),t._v("的梯度畅通优势和"),s("strong",[t._v("层归一化")]),t._v("的训练稳定优势"),s("strong",[t._v("结合")]),t._v("在了一起。两者相辅相成，缺一不可。没有LN，深层Transformer的训练可能不稳定；没有残差，梯度无法有效回传，深层网络无法训练。")])])])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"_3-关系与类比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-关系与类比"}},[t._v("#")]),t._v(" 3. 关系与类比")]),t._v(" "),s("p",[t._v("为了更好地理解，我们可以用一个"),s("strong",[t._v("加工流水线")]),t._v("的类比：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("原始输入")]),t._v("：一块待加工的原材料。")]),t._v(" "),s("li",[s("strong",[t._v("子层函数")]),t._v("：一个复杂的加工机器（比如“自注意力机器”或“前馈神经网络机器”）。")]),t._v(" "),s("li",[s("strong",[t._v("残差连接")]),t._v("：一条"),s("strong",[t._v("传送带")]),t._v("，将未加工的原材料"),s("strong",[t._v("直接运到")]),t._v("加工机器的出口处。")]),t._v(" "),s("li",[s("strong",[t._v("层归一化")]),t._v("：一个"),s("strong",[t._v("标准化工具")]),t._v("，用来调整加工后产品的尺寸和形状，使其符合下一台机器要求的输入规格。")]),t._v(" "),s("li",[s("strong",[t._v("Add & Norm")]),t._v("：整个**“混合与标准化”工位**。在这个工位上，工人（或机器人）执行两个动作：\n"),s("ol",[s("li",[s("strong",[t._v("Add")]),t._v("：将传送带送来的原始原材料，与加工机器出来的半成品"),s("strong",[t._v("混合")]),t._v("在一起。")]),t._v(" "),s("li",[s("strong",[t._v("Norm")]),t._v("：立即使用标准化工具对这个"),s("strong",[t._v("混合后的结果")]),t._v("进行标准化处理，然后送往下一个工位。")])])])]),t._v(" "),s("p",[s("strong",[t._v("为什么先Add再Norm（Post-LN）？")]),t._v("\n因为标准化工具（Norm）需要处理的是"),s("strong",[t._v("混合后的新数据")]),t._v("，确保它的分布是稳定的，再送给下一台机器。如果先Norm再Add，相当于先把原材料标准化，然后和机器加工的复杂输出混合，这个混合结果的分布可能依然不稳定，达不到Norm想要的效果。")]),t._v(" "),s("p",[s("strong",[t._v("为什么现代架构用Pre-LN？")]),t._v("\n因为实践发现，如果把标准化工具（Norm）放在加工机器"),s("strong",[t._v("之前")]),t._v("，先对原材料进行预处理，再加工，然后再和原始原材料混合，这样整个流水线的运行（训练过程）会更加"),s("strong",[t._v("稳定高效")]),t._v("，尤其对于超长的流水线（非常深的模型）。")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"_4-总结对比表"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-总结对比表"}},[t._v("#")]),t._v(" 4. 总结对比表")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("特性")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("残差连接")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("层归一化")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("Add & Norm")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("本质")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("一种连接方式/结构")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("一种标准化技术")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("一个具体的操作步骤")])])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("目的")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("解决梯度消失，便于训练深模型")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("稳定激活值分布，加速收敛")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("结合两者优势")]),t._v("，构建Transformer核心组件")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("操作")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("输入 + 输出")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("(x - μ) / σ")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("LayerNorm(输入 + 子层(输入))")])])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("关系")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("Add & Norm 的 "),s("strong",[t._v("一部分")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("Add & Norm 的 "),s("strong",[t._v("另一部分")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("= 残差连接 + 层归一化")])])])])]),t._v(" "),s("p",[s("strong",[t._v("结论：")]),t._v("\n“残差连接”和“层归一化”是两种可以独立存在的、基础性的"),s("strong",[t._v("技术思想")]),t._v("。而“Add & Norm”是Transformer架构对这两种技术思想的"),s("strong",[t._v("一种具体应用和实现方式")]),t._v("，它们被紧密地耦合在一起，共同保证了模型的深度和训练稳定性。")]),t._v(" "),s("h2",{attrs:{id:"六-重要点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#六-重要点"}},[t._v("#")]),t._v(" 六. 重要点")]),t._v(" "),s("p",[t._v("在 Transformer 模型中，"),s("code",[t._v("嵌入层")]),t._v("（Embedding Layer） 是处理输入和输出数据的关键步骤，因为模型实际操作的是"),s("code",[t._v("张量")]),t._v("（tensor），而非"),s("code",[t._v("字符串")]),t._v("（string）。在将输入文本传递给模型之前，首先需要进行"),s("code",[t._v("分词")]),t._v("（tokenization），即将文本拆解为多个 "),s("code",[t._v("token")]),t._v("，随后这些 token 会被映射为对应的 "),s("code",[t._v("token ID")]),t._v("，"),s("strong",[t._v("从而转换为模型可理解的数值形式")]),t._v("。此时，数据的形状为 "),s("code",[t._v("(seq_len,)")]),t._v("，其中 "),s("code",[t._v("seq_len")]),t._v(" 表示输入序列的长度。")])])}),[],!1,null,null,null);s.default=e.exports}}]);