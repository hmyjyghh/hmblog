(window.webpackJsonp=window.webpackJsonp||[]).push([[105],{587:function(s,t,a){"use strict";a.r(t);var n=a(3),e=Object(n.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" nn\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("replace_linear_with_lora")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  module"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" nn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Module"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  r"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("int")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  alpha"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("int")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("16")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  dropout_p"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("float")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.0")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  embed_requires_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("bool")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("      "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# embedding 层是否训练")]),s._v("\n  norm_requires_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("bool")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# norm 层是否训练")]),s._v("\n  head_requires_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("bool")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("       "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# lm_head 层是否训练（Causal LM才有）")]),s._v("\n  test_mode"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("bool")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("                "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 测试模式，用于控制 lora_B 是否为全零")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n  找到 module 中所有线性层并递归替换\n  """')]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" child "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" module"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("named_children"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 先处理额外的层，lm_head 也是 linear，所以先处理")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("any")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" s "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'embed'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'norm'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'lm_head'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n      requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" embed_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'embed'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name \\\n                      "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" norm_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'norm'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name \\\n                      "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" head_requires_grad\n      "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" param "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" child"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("parameters"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        param"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" requires_grad\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 替换所有线性层，QLoRA 做法")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("elif")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("isinstance")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("child"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" nn"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Linear"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n      lora_linear "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" LoraLinear"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("child"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" r"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("r"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" alpha"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("alpha"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" dropout_p"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("dropout_p"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" test_mode"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("test_mode"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n      "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("setattr")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("module"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" lora_linear"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 递归向下替换")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        replace_linear_with_lora"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n          child"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" r"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" alpha"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" dropout_p"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n          embed_requires_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" norm_requires_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" head_requires_grad"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n          test_mode"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("test_mode\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),t("ul",[t("li",[s._v("Python 中的行续行符")])]),s._v(" "),t("p",[s._v("这段代码中的反斜杠 "),t("code",[s._v("\\")]),s._v(" 是 "),t("strong",[s._v("Python 中的行续行符")]),s._v("。")]),s._v(" "),t("h2",{attrs:{id:"作用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#作用"}},[s._v("#")]),s._v(" 作用：")]),s._v(" "),t("p",[s._v("当一行代码太长时，可以使用 "),t("code",[s._v("\\")]),s._v(" 将其分成多行书写，让代码更易读。")]),s._v(" "),t("h2",{attrs:{id:"你的代码解析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#你的代码解析"}},[s._v("#")]),s._v(" 你的代码解析：")]),s._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" embed_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'embed'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name \\\n                "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" norm_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'norm'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name \\\n                "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" head_requires_grad\n")])])]),t("p",[s._v("相当于：")]),s._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" embed_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'embed'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" norm_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'norm'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" head_requires_grad\n")])])]),t("h2",{attrs:{id:"更清晰的写法-推荐"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#更清晰的写法-推荐"}},[s._v("#")]),s._v(" 更清晰的写法（推荐）：")]),s._v(" "),t("p",[s._v("实际上，Python 中更推荐使用括号来自动续行：")]),s._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  embed_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'embed'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name\n  "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" norm_requires_grad "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'norm'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name\n  "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" head_requires_grad\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),t("p",[s._v("或者进一步简化：")]),s._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'embed'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" embed_requires_grad\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("elif")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'norm'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" norm_requires_grad\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("  "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# lm_head")]),s._v("\n  requires_grad "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" head_requires_grad\n")])])]),t("h2",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[s._v("#")]),s._v(" 总结：")]),s._v(" "),t("p",[t("code",[s._v("\\")]),s._v(" 让长的单行代码可以分成多行书写，提高可读性。")]),s._v(" "),t("h3",{attrs:{id:"python-中的忽略变量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#python-中的忽略变量"}},[s._v("#")]),s._v(" python 中的"),t("code",[s._v("忽略变量")])]),s._v(" "),t("p",[t("code",[s._v("_")]),s._v(" 是 Python 约定俗成的 "),t("strong",[s._v('"忽略变量"')]),s._v("：")]),s._v(" "),t("ul",[t("li",[s._v("这个位置的数据 ("),t("code",[s._v("'口苦'")]),s._v(") 在后续代码中"),t("strong",[s._v("不会使用")])]),s._v(" "),t("li",[s._v("用 "),t("code",[s._v("_")]),s._v(' 明确告诉阅读者："我知道这里有数据，但故意不用"')]),s._v(" "),t("li",[s._v("避免创建无用的变量名污染命名空间")])]),s._v(" "),t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[s._v("labels "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'口苦'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'临床表现'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\nsentence "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'现头昏口苦'")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 生成字符级标签序列")]),s._v("\nchar_labels "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'O'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("len")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sentence"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" start"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" end"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" _"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" label_type "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" labels"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" pos "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("start"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" end "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" pos "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" start"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n      char_labels"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("pos"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string-interpolation"}},[t("span",{pre:!0,attrs:{class:"token string"}},[s._v("f'B-")]),t("span",{pre:!0,attrs:{class:"token interpolation"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("label_type"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")])]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'")])]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n      char_labels"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("pos"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string-interpolation"}},[t("span",{pre:!0,attrs:{class:"token string"}},[s._v("f'I-")]),t("span",{pre:!0,attrs:{class:"token interpolation"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("label_type"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")])]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'")])]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# for in 语法，中间可以有 这么多参数吗， 比如 start, end, _, label_type")]),s._v("\n\n这是一个非常典型的 Python 解包（Unpacking） 用法，完全合法且很常用。\n")])])])])}),[],!1,null,null,null);t.default=e.exports}}]);