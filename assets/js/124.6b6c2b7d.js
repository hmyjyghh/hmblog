(window.webpackJsonp=window.webpackJsonp||[]).push([[124],{568:function(_,v,t){"use strict";t.r(v);var r=t(3),s=Object(r.a)({},(function(){var _=this,v=_._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("h2",{attrs:{id:"强化学习相关"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#强化学习相关"}},[_._v("#")]),_._v(" 强化学习相关")]),_._v(" "),v("h3",{attrs:{id:"_1-有监督学习-vs-强化学习-rl"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-有监督学习-vs-强化学习-rl"}},[_._v("#")]),_._v(" 1. 有监督学习 vs 强化学习(RL)")]),_._v(" "),v("blockquote",[v("p",[_._v("从下面2个点进行分析")])]),_._v(" "),v("ol",[v("li",[v("code",[_._v("强化学习")]),_._v("比"),v("code",[_._v("有监督学习")]),_._v("更考虑整体影响:")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("有监督学习针对单个词元进行反馈, 其目标是"),v("strong",[_._v("要求模型针对给定的输入给出确切答案")]),_._v("。")])]),_._v(" "),v("li",[v("p",[_._v("而"),v("strong",[_._v("强化学习是针对整个输出文本进行反馈,")]),_._v(" 并不针对特定的词元。")])]),_._v(" "),v("li",[v("p",[_._v("这种反馈粒度的不同, 使得强化学习更适合大语言模型, 既可以兼顾表达多样性, 还可以增强对微小变化的敏感性。")])]),_._v(" "),v("li",[v("p",[_._v("自然语言十分灵活,可以用多种不同的方式表达相同的语义。而有监督学习很难支持上述学习方式。")])]),_._v(" "),v("li",[v("p",[_._v("强化学习可以"),v("strong",[_._v("通过奖励函数")]),_._v("达到同时兼顾多样性和微小变化敏感性两个方面。")])])]),_._v(" "),v("ol",[v("li",[_._v("强化学习更容易解决幻觉问题:")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("用户在大语言模型时主要有三类输输入:")]),_._v(" "),v("ul",[v("li",[_._v("(a)文本型(Text-Grounded): 用户输入相关文本和问题,让模型基于所提供的文本生成答案;")]),_._v(" "),v("li",[_._v("(b)求知型(Knowledge-Seeking): 用户仅提出问题,模型根据内在知识提供真实回答;")]),_._v(" "),v("li",[_._v('(c)创造型(Creative): 用户为提供问题或说明, 让模型进行创造性输出(例如,"写一个关于...的故事")。')])])]),_._v(" "),v("li",[v("p",[_._v("有监督学习算法非常容易"),v("strong",[_._v("使得求知型查询产生幻觉")]),_._v("。在模型并不包含或者知道答案的情况下, 有监督训练仍然会促使模型给出答案。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("而使用强化学习方法, 则可以通过定制奖励函数")]),_._v(", 将正确答案赋予非常高的分数, 放弃回答的答案赋予中低分数, 不正确的答案赋予非常高的负分, "),v("strong",[_._v("使得模型学会依赖内部知识选择放弃回答")]),_._v(", 从而在一定程度上缓解模型幻觉问题。")])])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("强化学习可以更好的解决多轮对话奖励累积问题:")])]),_._v(" "),v("ul",[v("li",[_._v("多轮对话能力是大语言模型重要的基础能力之一, "),v("strong",[_._v("多轮对话")]),_._v("是否达成最终目标, 需要考虑"),v("strong",[_._v("多次交互过程的整体情况,")])]),_._v(" "),v("li",[_._v("因此很难使用有监督学习方法构建。"),v("strong",[_._v("而使用强化学习方法")]),_._v(",可以通过"),v("strong",[_._v("构建奖励函数")]),_._v(", 将当前输出考虑整个对话的背景和连贯性。")])]),_._v(" "),v("h3",{attrs:{id:"_2-rlhf-基于人类反馈的强化学习流程"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-rlhf-基于人类反馈的强化学习流程"}},[_._v("#")]),_._v(" 2. RLHF，基于人类反馈的强化学习流程")]),_._v(" "),v("blockquote",[v("p",[_._v("基于人类反馈的强化学习, 主要分为"),v("strong",[_._v("奖励模型训练")]),_._v("和"),v("strong",[_._v("近端策略优化")]),_._v("两个步骤。")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("奖励模型")]),_._v("通过"),v("strong",[_._v("由人类反馈标注的偏好数据")]),_._v("来学习人类的偏好, 判断模型回复的有用性以及保证内容的无害性。")]),_._v(" "),v("li",[v("strong",[_._v("奖励模型")]),_._v("模拟了人类的偏好信息, 能够不断地为模型的训练提供奖励信号。")]),_._v(" "),v("li",[_._v("在"),v("strong",[_._v("获得奖励模型")]),_._v("后, 需要"),v("strong",[_._v("借助强化学习对语言模型继续进行微调")]),_._v("。")]),_._v(" "),v("li",[_._v("OpenAI在大多数任务中，使用的强化学习算法都是"),v("strong",[_._v("近端策略优化算法")]),_._v(" (PPO 算法)。")]),_._v(" "),v("li",[_._v("近端策略优化可以"),v("strong",[_._v("根据奖励模型获得的反馈优化模型")]),_._v(", 通过不断的迭代, "),v("strong",[_._v("让模型探索和发现更符合人类偏好的回复策略")]),_._v("。")])]),_._v(" "),v("blockquote",[v("p",[_._v("近端策略优化的流程如图所示。")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/rl/rlhf-1.jpg",alt:"RLHF"}})]),_._v(" "),v("h3",{attrs:{id:"_3-奖励模型是如何训练的-它的损失函数是什么"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-奖励模型是如何训练的-它的损失函数是什么"}},[_._v("#")]),_._v(" 3. 奖励模型是如何训练的, 它的损失函数是什么?")]),_._v(" "),v("ul",[v("li",[_._v("奖励模型通常也采用"),v("strong",[_._v("基于Transformer架构的预训练语言模型")]),_._v("，在奖励模型中,移除最后一个非嵌入层, "),v("strong",[_._v("并在最终的Transformer层上叠加了一个额外的线性层")]),_._v("。")]),_._v(" "),v("li",[_._v("无论输入的是何种文本, "),v("strong",[_._v("奖励模型都能为文本序列中的最后一个标记分配一个标量奖励值")]),_._v(", 样本质量越高, 奖励值越大。")])]),_._v(" "),v("blockquote",[v("p",[_._v("训练奖励模型通常需要使用由相同输入"),v("strong",[_._v("生成的两个不同输出")]),_._v("之间的配对比数据集。")])]),_._v(" "),v("div",{staticClass:"language-js extra-class"},[v("pre",{pre:!0,attrs:{class:"language-js"}},[v("code",[v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("{")]),_._v("\n    "),v("span",{pre:!0,attrs:{class:"token string-property property"}},[_._v('"question"')]),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v(":")]),_._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('"xxx"')]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v("\n    "),v("span",{pre:!0,attrs:{class:"token string-property property"}},[_._v('"chosen"')]),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v(":")]),_._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('""')]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v("\n    "),v("span",{pre:!0,attrs:{class:"token string-property property"}},[_._v('"reject"')]),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v(":")]),_._v(" "),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('""')]),_._v("\n"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("}")]),_._v("\n")])])]),v("p",[_._v("另外, 还引入了一种惩罚机制: "),v("code",[_._v("KL散度")]),_._v(", 这个"),v("code",[_._v("KL散度项")]),_._v("在这里发挥着两个重要的作用。")]),_._v(" "),v("ul",[v("li",[_._v("首先, 它作为一个熵奖励, 促进了在策略空间中的探索, "),v("strong",[_._v("避免了策略过早地收敛到单一模式")]),_._v("。")]),_._v(" "),v("li",[_._v("其次, 它确保强化学习策略的输出不会与奖励模型在训练阶段遇到的样本产生明显的偏差, 从而维持了学习过程的稳定性和一致性。("),v("strong",[_._v("避免模型训歪")]),_._v(")")])]),_._v(" "),v("blockquote",[v("p",[_._v("这种"),v("strong",[_._v("KL惩罚机制")]),_._v("在整个学习过程中"),v("strong",[_._v("起到了平衡和引导的作用")]),_._v(", 有助于取得更加稳健和可靠的训练效果。")])]),_._v(" "),v("p",[_._v("KL 散度，被用于惩罚RL策略在每个训练批次中生成大幅偏离初始模型, 以确保模型输出合理连贯的文本。\n如果去掉这一惩罚项可能导致模型在优化中生成乱码文本来愚弄奖励模型提供高奖励值。")]),_._v(" "),v("h3",{attrs:{id:"_4-介绍一下rlhf中ppo微调过程"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-介绍一下rlhf中ppo微调过程"}},[_._v("#")]),_._v(" 4. 介绍一下RLHF中PPO微调过程")]),_._v(" "),v("blockquote",[v("p",[_._v("PPO的目的是在不偏离原始模型太远的前提下，最大化从奖励模型获得的预期奖励。")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/rl/PPO.png",alt:"PPO 微调"}})]),_._v(" "),v("h4",{attrs:{id:"参与角色"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#参与角色"}},[_._v("#")]),_._v(" 参与角色")]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("策略模型")]),_._v("：我们需要优化的语言模型。"),v("strong",[_._v("它是PPO中的“演员”")]),_._v("，参数会持续更新。")]),_._v(" "),v("li",[v("strong",[_._v("参考模型")]),_._v("：通常是第一步得到的SFT模型的一个副本，"),v("strong",[_._v("其参数在整个PPO过程中被固定")]),_._v("。它代表“原始的、安全的”模型，用来约束策略模型不要走偏。")]),_._v(" "),v("li",[v("strong",[_._v("奖励模型")]),_._v("：第二步训练好的RM，为生成的完整文本序列给出一个总的奖励分数。"),v("strong",[_._v("它是PPO中的“裁判”")]),_._v("。")])]),_._v(" "),v("h4",{attrs:{id:"迭代步骤解释"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#迭代步骤解释"}},[_._v("#")]),_._v(" 迭代步骤解释：")]),_._v(" "),v("p",[v("strong",[_._v("步骤1：生成文本")])]),_._v(" "),v("p",[v("strong",[_._v("步骤2：计算奖励")])]),_._v(" "),v("p",[v("strong",[_._v("步骤3：计算优势与回报")])]),_._v(" "),v("ul",[v("li",[_._v("在强化学习中，我们不仅关心即时奖励，还关心长期回报。我们需要计算"),v("strong",[_._v("优势函数 A_t")]),_._v("，它衡量某个时间步的动作相对于平均情况有多好。")]),_._v(" "),v("li",[_._v("由于语言生成是序列化的，一个词的好坏会影响后续，因此需要使用"),v("strong",[_._v("广义优势估计")]),_._v(" 等技术，利用整个序列的奖励信号来估计每个生成token的优势值。")])]),_._v(" "),v("p",[v("strong",[_._v("步骤4：PPO优化更新")])]),_._v(" "),v("ul",[v("li",[_._v("现在我们有了四样东西：策略模型生成的"),v("code",[_._v("（状态， 动作， 奖励， 优势）")]),_._v("数据。")]),_._v(" "),v("li",[_._v("PPO的核心是构造一个特殊的目标函数，它在鼓励高奖励动作的同时，限制新旧策略的更新幅度。其核心是“裁剪”机制：\n"),v("ul",[v("li",[_._v("计算新旧策略（更新前和更新中的策略）选择某个动作的概率比值 "),v("code",[_._v("r_t(θ)")]),_._v("。")]),_._v(" "),v("li",[_._v("构建两个目标：一个是简单的优势加权概率比值 "),v("code",[_._v("r_t(θ) * A_t")]),_._v("；另一个是将"),v("code",[_._v("r_t(θ)")]),_._v("裁剪在 "),v("code",[_._v("[1-ε, 1+ε]")]),_._v(" 范围内的保守估计。")]),_._v(" "),v("li",[v("strong",[_._v("PPO目标函数取两者中的较小值")]),_._v("："),v("code",[_._v("L = min(r_t(θ) * A_t, clip(r_t(θ), 1-ε, 1+ε) * A_t)")]),_._v("。")]),_._v(" "),v("li",[_._v("这个“取最小”和“裁剪”操作确保了单次更新不会因为某个样本的"),v("code",[_._v("r_t(θ)")]),_._v("和"),v("code",[_._v("A_t")]),_._v("都极大而导致策略发生剧变，从而保证了训练的稳定性。")])])]),_._v(" "),v("li",[_._v("通过梯度上升最大化这个目标函数 "),v("code",[_._v("L")]),_._v("，来更新"),v("strong",[_._v("策略模型")]),_._v("的参数。")])]),_._v(" "),v("p",[v("strong",[_._v("步骤5：循环迭代")])]),_._v(" "),v("ul",[v("li",[_._v("更新后的策略模型成为新的“演员”，回到步骤1，用新的策略模型生成文本，开始新一轮迭代。")]),_._v(" "),v("li",[_._v("参考模型始终保持不变，作为稳定的锚点。")])]),_._v(" "),v("p",[v("strong",[_._v("RLHF中的PPO微调，本质上是让一个语言模型（策略）在一个模拟人类偏好的裁判（奖励模型）的指导下，通过“试错-反馈-调整”的方式进行学习，同时被一个保守的老师（参考模型）紧紧拉住，防止其学习步伐过大而失控。")])]),_._v(" "),v("h3",{attrs:{id:"_5-介绍一下llm的直接偏好优化-dpo"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_5-介绍一下llm的直接偏好优化-dpo"}},[_._v("#")]),_._v(" 5. 介绍一下LLM的直接偏好优化(DPO)")]),_._v(" "),v("p",[_._v("是一种更简单、更直接的培训方法。")]),_._v(" "),v("ul",[v("li",[_._v("DPO的工作原理是增加偏好样本的对数概率与减小非偏好样本响应的对数概率。")]),_._v(" "),v("li",[_._v("它结合了动态加权机制,以避免仅使用概率比目标时遇到的模型退化问题。")])]),_._v(" "),v("p",[_._v("DPO是一种隐式优化策略, 与现有的RLHF方法具有相同的目标, 但更容易实现且易于训练。")]),_._v(" "),v("p",[_._v("DPO针对人类偏好进行了优化, 同时避免了强化学习。")]),_._v(" "),v("ul",[v("li",[_._v("在大语言模型微调中,现有的基于人类反馈的方法都会首先将奖励模型拟合到一个包含提示和人类偏好的数据集上, 然后使用对比学习来找到一个策略最大化学习到的奖励。")]),_._v(" "),v("li",[_._v("相比之下, "),v("strong",[_._v("DPO 只通过简单的分类目标")]),_._v(", "),v("strong",[_._v("就能直接针对最满足人类偏好的策略进行优化")]),_._v(", 无需明确的奖励函数或者强化学习。")])]),_._v(" "),v("h3",{attrs:{id:"_6-什么是rlaif"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_6-什么是rlaif"}},[_._v("#")]),_._v(" 6. 什么是RLAIF?")]),_._v(" "),v("blockquote",[v("p",[_._v("背景： 当项目进入实际应用阶段, 我们常常面临一个问题: 缺乏足够的人人力进行数据标注。这增加了项目的复杂性。")])]),_._v(" "),v("p",[_._v("RLAIF的最大贡献是:")]),_._v(" "),v("ul",[v("li",[v("p",[v("strong",[_._v("提供一个潜在的替代方案: RLAIF使用现成的LLM来标记偏好, 而不是依赖人类。")]),_._v(" 研究发现, RLAIF和RLHF\n在改进方面产生了类似的结果。")]),_._v(" "),v("ul",[v("li",[_._v("具体来说,对于摘要任务, 人类评估者在大约70%的情况下更喜欢RLAIF和RLHF的输出, 而不是基线的有监督微调模型。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("证明AI反馈的有效性")]),_._v(": 当比较RLAIF和RLHF摘要时, 人类对两者都表示出相同的偏好。")])]),_._v(" "),v("li",[v("p",[_._v("这意味着使用AI反馈进行训练可以达到与使用人类反馈相似的性能, 从而为RLHF的可扩展性问题提供了一个潜在的解决方案。")])])]),_._v(" "),v("p",[_._v("RLAIF的引入为强化学习提供了一个新的、可扩展的方法, 该方法不依赖于昂贵和时间消耗的人类标签收集, 但仍然可以达到与人类反馈相似的性能。")]),_._v(" "),v("h3",{attrs:{id:"_7-rl为什么难以训练"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_7-rl为什么难以训练"}},[_._v("#")]),_._v(" 7. RL为什么难以训练?")]),_._v(" "),v("ul",[v("li",[_._v("奖励稀疏和延迟: 智能体难以确定哪些行为导致了奖励。")]),_._v(" "),v("li",[_._v("高维状态和动作空间: 增加计算复杂度和样本需求。")]),_._v(" "),v("li",[_._v("探索与利用的权衡: 需要平衡探索新策略和利用已知策略。")]),_._v(" "),v("li",[_._v("样本效率低: 大量数据需求导致训练时间和成本增加。")]),_._v(" "),v("li",[_._v("稳定性和收敛性问题: 策略剧烈波动影响训练效果。")]),_._v(" "),v("li",[_._v("部分可观测环境: 增加智能体的记忆和推理负担。")]),_._v(" "),v("li",[_._v("信号噪声和不确定性: 环境反馈信号中的噪声增加了学习难度。")]),_._v(" "),v("li",[_._v("算法复杂性: 算法实现和调优复杂,影响训练效果。")])]),_._v(" "),v("h3",{attrs:{id:"_8-请写出rlhf的优化目标公式"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_8-请写出rlhf的优化目标公式"}},[_._v("#")]),_._v(" 8. 请写出RLHF的优化目标公式")]),_._v(" "),v("p",[_._v("RLHF的目标是"),v("strong",[_._v("通过人类反馈来优化模型的行为")]),_._v("。")]),_._v(" "),v("blockquote",[v("p",[_._v("RLHF的优化目标公式可以表示为:")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/rl/rlhf-2.jpg",alt:"RLHF的优化目标公式"}})]),_._v(" "),v("p",[_._v("其中(0)是模型参数(TR)是参数为(0)的策略,(s)是状态,(a)是动作,(R(s,a))是基于人类反馈的奖励函数。\n我们通过人类反馈来定义奖励函数(R(s,a)),并优化模型参数(0)以最大化期望奖励。")]),_._v(" "),v("blockquote",[v("p",[_._v("这一过程通常包括以下几个步骤:")])]),_._v(" "),v("ol",[v("li",[_._v("收集人类反馈: 通过用户的交互和评价,收集关于模型行为的反馈数据。")]),_._v(" "),v("li",[_._v("定义奖励函数: 基于人类反馈数据构建奖励函数(R(s,a))。")]),_._v(" "),v("li",[_._v("策略优化: 使用强化学习算法(如策略梯度法)优化模型参数(\\theta),以最大化期望奖励。")])]),_._v(" "),v("ul",[v("li",[_._v("通过这个过程, "),v("strong",[_._v("模型能够逐渐学习和改进其行为")]),_._v(", "),v("strong",[_._v("更好地满足人类用户的需求和偏好。")])])]),_._v(" "),v("h3",{attrs:{id:"_9-rlhf模型为什么会表现比sft更好"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_9-rlhf模型为什么会表现比sft更好"}},[_._v("#")]),_._v(" 9. RLHF模型为什么会表现比SFT更好?")]),_._v(" "),v("p",[_._v("根据OpenAI的评估, "),v("strong",[_._v("与监督微调模型的输出相比, 人们更喜欢RLHF模型的输出。")])]),_._v(" "),v("blockquote",[v("p",[_._v("原因如下：")])]),_._v(" "),v("ul",[v("li",[_._v("与生成相比,做判断更容易。这就好比,如果让你写一首诗,你可能会觉得很困难;")]),_._v(" "),v("li",[_._v("但是如果让你对几首诗进行排序,就会相对容易很多。")])]),_._v(" "),v("p",[_._v("这可能有两个层面的含义:")]),_._v(" "),v("ul",[v("li",[_._v("人工标注数据的影响: 当人工标注数据时, 对几个答案做出选择要比写出几个备选答案要简单得多。")]),_._v(" "),v("li",[_._v("因此,由判断标签构成的数据库的质量更高, 这使得训练出的奖励模型型非常有效。")]),_._v(" "),v("li",[_._v("同样的原理, 强化学习训练时, 优化判别结果更为容易。这使更得最终优化的效果比较高,从而让模型更符合人的使用习惯。")])]),_._v(" "),v("h3",{attrs:{id:"_10-说说你知道的大模型训练or推理的常用优化手段"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_10-说说你知道的大模型训练or推理的常用优化手段"}},[_._v("#")]),_._v(" 10. 说说你知道的大模型训练or推理的常用优化手段")]),_._v(" "),v("blockquote",[v("p",[_._v("大模型训练和推理的优化通常涉及多个层面, 包括"),v("strong",[_._v("模型架构")]),_._v("、"),v("strong",[_._v("训练策略、数据处理、硬件利用和推理优化")]),_._v("。")])]),_._v(" "),v("ol",[v("li",[_._v("模型架构优化:")])]),_._v(" "),v("ul",[v("li",[_._v("模型量化: 将模型的权重和激活函数"),v("strong",[_._v("从浮点数(如32位或16位)量化到低精度(如8位或更低)")]),_._v(", 以减少内存占用和计算量。")]),_._v(" "),v("li",[_._v("模型蒸馏: "),v("strong",[_._v("将大模型的知识转移到更小的模型(学生模型)中")]),_._v(", 保持大部分性能, 但降低计算和存储需求。")]),_._v(" "),v("li",[_._v("模型剪枝: 移除模型中的冗余或不重要的权重, 降低模型大小和计算复杂性")]),_._v(" "),v("li",[_._v("模型分解: 将大模型分解为几个小型子模型, 可以并行处理, 减少计算时间")])]),_._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[_._v("训练策略优化:")])]),_._v(" "),v("ul",[v("li",[_._v("分阶段训练: 先在小数据集上预训练,然后在大数据集上微调。")]),_._v(" "),v("li",[_._v("动态学习率调度: 使用学习率衰减策略(如指数衰减、步进衰减、余弦退火等)来优化训练过程。")]),_._v(" "),v("li",[_._v("权重初始化: 使用合适的权重初始化方法(如Xavier、He初始化), 确保模型能更有效地收敛。")]),_._v(" "),v("li",[_._v("正则化: 如L1、L2正则化、Dropout、权重衰减等,防止过拟合")])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("数据处理优化:")])]),_._v(" "),v("ul",[v("li",[_._v("数据增强: 如旋转、裁剪、缩放等, 增加数据的多样性, 增强模型泛化能力。")]),_._v(" "),v("li",[_._v("批处理和预处理: 批量处理数据以提高计算效率, 预处理数据以减小模型的计算负担。")]),_._v(" "),v("li",[_._v("缓存和并行化: 缓存数据,减少1/0延迟; 并行化数据加载我,加快预处理速度。")])]),_._v(" "),v("ol",{attrs:{start:"4"}},[v("li",[_._v("硬件利用优化:")])]),_._v(" "),v("ul",[v("li",[_._v("GPU和TPU井行计算: 利用多GPU或TPU进行分布式训练,提高计算速度。")]),_._v(" "),v("li",[_._v("混合精度训练: 使用混合精度(如FP16和FP32的混合)进行训练, 降低内存需求, 提高训练速度。")]),_._v(" "),v("li",[_._v("内存管理和梯度累积: 管理内存分配,避免溢出,有时可以通过梯度累积策略在有限内存下进行大模型训练。")])]),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[_._v("推理优化:")])]),_._v(" "),v("ul",[v("li",[_._v("模型裁剪: 针对推理场景,对模型进行剪枝, 去除不重要的部分以减少推理时的计算需求。")]),_._v(" "),v("li",[_._v("量化推理: 使用量化模型进行推理, "),v("strong",[_._v("如从FP32转换到INT8, 降低计算和内存需求")]),_._v("。")]),_._v(" "),v("li",[v("strong",[_._v("模型蒸馏")]),_._v(": 用较小的模型执行推理, 尽管它们可能不包含所有大模型的复杂性, 但能保持大部分性能。")]),_._v(" "),v("li",[v("strong",[_._v("知识蒸馏")]),_._v(": 使用大模型的输出指导小模型的训练, 使得小模型能够在推理时提供类似大模型的性能。")]),_._v(" "),v("li",[_._v("动态推理: 根据输入数据的特性动态调整模型的计算资源, 减少不必要的计算。")])]),_._v(" "),v("ol",{attrs:{start:"6"}},[v("li",[_._v("模型服务器优化:")])]),_._v(" "),v("ul",[v("li",[_._v("负载均衡: 使用负载均衡策略分发推理请求, 避免单点压力过天。")]),_._v(" "),v("li",[_._v("异步请求处理: 通过异步1/0和多线程处理多个请求, 减少等待时间。")]),_._v(" "),v("li",[_._v("缓存策略: 对于常见请求, 可以缓存结果以提高响应速度。")])]),_._v(" "),v("h3",{attrs:{id:"_11-一般会对大模型里面的哪些算子做算子融合"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_11-一般会对大模型里面的哪些算子做算子融合"}},[_._v("#")]),_._v(" 11. 一般会对大模型里面的哪些算子做算子融合?")]),_._v(" "),v("blockquote",[v("p",[_._v("在大模型(如Transformer架构的模型,如BERT、GPT、T5等)中, 算子融合(Operator Fusion)是一种优化技术, 它将多个连续的运算融合成一个复合运算, "),v("strong",[_._v("以减少计算量和内在存访问, 从而提高模型的执行速度和效率")]),_._v("。")])]),_._v(" "),v("ol",[v("li",[v("p",[_._v("矩阵乘法和加法:")]),_._v(" "),v("ul",[v("li",[_._v("在Transformer的自注意力层和前馈神经网络中,经常需要进行大量的矩阵乘法和加法运算。通过融合这些操作,可以减少内存访问次数, 提高计算速度。")])])]),_._v(" "),v("li",[v("p",[_._v("激活函数:")]),_._v(" "),v("ul",[v("li",[_._v("将ReLU、Leaky ReLU、GELU等激活函数与前一个操作(如矩阵乘法)融合, 避免在计算激活时的数据复制和额外内存开销。")])])]),_._v(" "),v("li",[v("p",[_._v("softmax和加权和:")]),_._v(" "),v("ul",[v("li",[_._v("自注意力层中的softmax操作通常与加权和(加权后得到每个位置的加权和值)相结合, 通过融合这两个操作可以提高计算效率。")])])]),_._v(" "),v("li",[v("p",[_._v("归一化操作:")]),_._v(" "),v("ul",[v("li",[_._v("如"),v("code",[_._v("LayerNorm")]),_._v("和"),v("code",[_._v("BatchNorm")]),_._v("等, 它们通常与前后的操作融合, 减少内存访问并加速计算流程。")])])]),_._v(" "),v("li",[v("p",[_._v("残差连接:")]),_._v(" "),v("ul",[v("li",[_._v("融合残差连接(Residual Connections)可以减少数据在计算图中的跳转,降低延迟。")])])]),_._v(" "),v("li",[v("p",[_._v("嵌入查找:")]),_._v(" "),v("ul",[v("li",[_._v("对于输入序列的词汇表嵌入, 融合嵌入查找和添加位置编码, 可以减少数据移动, 尤其是在处理长序列时。")])])]),_._v(" "),v("li",[v("p",[_._v("张量融合:")]),_._v(" "),v("ul",[v("li",[_._v("通过融合张量的加法、乘法和其他操作, 减少数据在内存和GPU之间传输的次数, 降低带宽需求。")])])]),_._v(" "),v("li",[v("p",[_._v("注意力机制:")]),_._v(" "),v("ul",[v("li",[_._v("自注意力层中的多头注意力的运算可以融合, 比如将多个头部的加权和操作合并为一个操作。")])])])]),_._v(" "),v("ul",[v("li",[_._v("算子融合通常在模型部署时进行, 特别是对于大模型推理优化, 因为训练阶段的计算资源通常更为丰富。")]),_._v(" "),v("li",[_._v("通过融合操作, 可以减少数据的移动, 优化内存访问模式, 减少计算开开销, 最终提高模型在低资源环境下的执行效率。")])]),_._v(" "),v("h3",{attrs:{id:"_12-模型预训练超过了最大长度怎么做-模型输入很长怎么办"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_12-模型预训练超过了最大长度怎么做-模型输入很长怎么办"}},[_._v("#")]),_._v(" 12.模型预训练超过了最大长度怎么做? 模型输入很长怎么办?")]),_._v(" "),v("ul",[v("li",[v("p",[_._v("首先可以使用"),v("strong",[_._v("截断技术")]),_._v(", 将超过最大长度的输入直接截断, 只保留前面的部分。虽然这种方法简单, "),v("strong",[_._v("但可能会丢失一些关键的信息")]),_._v(",特别是当重要信息在输入的末尾时。")])]),_._v(" "),v("li",[v("p",[_._v("另一种方法是使用"),v("strong",[_._v("滑动窗口")]),_._v(", 将长输入切分成多个重叠的/小片段, 每个片段的长度等于模型的最大输入长度。然后对每个片段分别进行处理,最后合并这些片段的结果。这样可以确保输入的所有部分都被处理, 但计算成本较高。")])]),_._v(" "),v("li",[v("p",[_._v("分段处理是另一种常用的方法,将长输入分成多个不重叠的片段, 每个片段的长度等于模型的最大输入长度。"),v("strong",[_._v("对每个片段分别进行处理, 然后将这些片段的输出结果整合")]),_._v("。虽然这种方法计算成本较低, 但可能会丢失片段之间的上下文信息。")])]),_._v(" "),v("li",[v("p",[_._v("对于处理特别长的输入, 可以考虑使用"),v("strong",[_._v("层次化模型")]),_._v(", 这种模型结构"),v("strong",[_._v("先对输入进行分段处理, 然后用额外的层捕捉各段之间的上下文关系")]),_._v("。虽然这种方法能更好地保持长输入的全局信息, 但模型复杂度较高。")])]),_._v(" "),v("li",[v("p",[_._v("还有一些专门设计的长序列建模方法, 如Longformer、Reformer、BigBird等, 这些模型通过稀疏注意力机制或其他技术来处理更长的输入序列, 能有效降低计算量。")])]),_._v(" "),v("li",[v("p",[_._v("另外还可以"),v("strong",[_._v("在预处理阶段对长输入进行摘要提取")]),_._v(", 将其简化为一个更短的摘要,再输入模型。这种方法能有效减少输入长度, "),v("strong",[_._v("但依赖于摘要的质量。")])])])]),_._v(" "),v("blockquote",[v("p",[_._v("总的来说, 根据具体任务的需求和计算资源限制, 可以选择适合的方法来处理长输入。")])])])}),[],!1,null,null,null);v.default=s.exports}}]);