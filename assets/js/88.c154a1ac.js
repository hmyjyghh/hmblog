(window.webpackJsonp=window.webpackJsonp||[]).push([[88],{532:function(v,_,t){"use strict";t.r(_);var a=t(3),n=Object(a.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("h2",{attrs:{id:"llm-文本生成阶段"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#llm-文本生成阶段"}},[v._v("#")]),v._v(" LLM 文本生成阶段")]),v._v(" "),_("ul",[_("li",[v._v("如何一次为单个请求高效生成文本?")]),v._v(" "),_("li",[v._v("如何逐个标记地，为单个输入生成下一个标记")])]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_5.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("blockquote",[_("p",[v._v("预测下一个词阶段的优化")])]),v._v(" "),_("p",[v._v("KV缓存, 是LLM推断中最基本的优化之一, 导致了我们所说的标记生成的两个不同阶段之间的分离。")]),v._v(" "),_("ol",[_("li",[v._v("首先是我们所说的预填充阶段(Prefill), 即使用所有输入生成第一个标记时发生的情况。")]),v._v(" "),_("li",[v._v("然后是我们所说的解码阶段(Decode), "),_("strong",[v._v("即使用缓存的K和V值生成后续标记的地方")]),v._v("。")])]),v._v(" "),_("p",[v._v("过去的键值是为该特定输入计算然后存储的K和V值,这样我们就不必再次计算\n它们。")]),v._v(" "),_("p",[v._v("但是加入KV Cache 之后,因为现在我们每次只传递一个标记而不是整个\n输入,所以在解码步骤中,每个标记的时间显著下降,这就是这种大幅度下降的外观。")]),v._v(" "),_("p",[v._v("这实际上是优化LLMS文本生成的第一个重要里程碑, 这是这些推理系统真正核心的部分。")]),v._v(" "),_("p",[v._v("但在生成第一个标记之后,出现了一个非常有趣的优化,即生成后续标记时, "),_("strong",[v._v("我们实际上只需要将新标记作为输入提供")]),v._v(",\n然后我们唯一需要重新引入先前输入信息的时间是在"),_("strong",[v._v("进行这个注意力计算时")]),v._v(",具体来说是对这些矩阵K和V,我们需要这些矩阵的值,这些值是为先前的标记计算出来\n的,现在计算这个值将帮助我们生成下一个标记。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_2.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[v._v("因此,你会发现,实际上,"),_("strong",[v._v("先前标记的这些K和V的值可以被缓存,")]),v._v(" "),_("strong",[v._v("这样我们就不必在每次调用这个计算之间重新计算它们,")]),v._v("\n然后只需要进行一个非常轻量级的连接操作,\n在这个操作中"),_("strong",[v._v("我们获取缓存的值, 附加新标记的新K和V值,进行注意力计算,重新缓存这些值,然后生成下一个标记。")])]),v._v(" "),_("p",[v._v("这就是KV缓存的整体概念，是LLM推断中最基本的优化之一, 导致了我们所说的标记生成的两个不同阶段之间的分离。")]),v._v(" "),_("ol",[_("li",[v._v("首先是我们所说的预填充阶段(Prefill), 即使用所有输入生成第一个标记时发生的情况。")]),v._v(" "),_("li",[v._v("然后是我们所说的解码阶段(Decode), "),_("strong",[v._v("即使用缓存的K和V值生成后续标记的地方")]),v._v("。")])]),v._v(" "),_("p",[_("strong",[v._v("这实际上使我们能够避免为输入序列中的所有过去的标记一遍又一遍地执行所有那些冗余的计算。")])]),v._v(" "),_("p",[v._v("在这之上,还有很多复杂技术, 比如页面侦测是一个非常流行的技术, "),_("strong",[v._v("它真的试图优化KV缓存的利用")]),v._v(", 无论是在内存和计算上还是在CUDA层发生了什么,所以我们可以在此基础上添加更多的优化层,像Lora这样的系统实现了这些优化。")]),v._v(" "),_("h3",{attrs:{id:"使用kv-cache-vs-不使用kv-cache"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#使用kv-cache-vs-不使用kv-cache"}},[v._v("#")]),v._v(" 使用KV Cache VS 不使用KV Cache")]),v._v(" "),_("ul",[_("li",[v._v("是哪里产生的大量耗时，注意力计算")]),v._v(" "),_("li",[v._v("通过 duration_s  和  duration_cached_s")]),v._v(" "),_("li",[v._v("用 matplotlib.pyplot 绘制曲线图，看效果对比")])]),v._v(" "),_("div",{staticClass:"language-python extra-class"},[_("pre",{pre:!0,attrs:{class:"language-python"}},[_("code",[v._v("duration_s "),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("[")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("]")]),v._v("\nduration_cached_s "),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("[")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("]")]),v._v("\nplt"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(".")]),v._v("plot"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("duration_s"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\nplt"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(".")]),v._v("plot"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("duration_cached_s"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\nplt"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(".")]),v._v("show"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n")])])]),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_1.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("橙线代表的是在有缓存的情况下生成标记所需的时间。")]),v._v(" "),_("li",[v._v("但是加入KV Cache 之后,因为现在我们每次只传递一个标记而不是整个\n输入,所以在解码步骤中,每个标记的时间显著下降,这就是这种大幅度下降的外观。")]),v._v(" "),_("li",[v._v("时间节省的主要来源")])]),v._v(" "),_("h3",{attrs:{id:"多头注意力机制-矩阵乘法-qk、v-相乘"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#多头注意力机制-矩阵乘法-qk、v-相乘"}},[v._v("#")]),v._v(" 多头注意力机制， 矩阵乘法，QK、V 相乘")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("矩阵的大小，最终与输入序列的大小成正比。")]),v._v(" "),_("li",[v._v("然后我们计算总体注意力计算。\n或者我们进行一些矩阵乘\n法，我们应用掩码，我们应用softmex操作并生成输出。")])]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("对于任何的Transformer 模型来说，最大的计算瓶颈，无疑就是 注意力计算。")]),v._v(" "),_("li")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_4.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("h2",{attrs:{id:"批处理阶段优化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#批处理阶段优化"}},[v._v("#")]),v._v(" 批处理阶段优化")]),v._v(" "),_("blockquote",[_("p",[v._v("可以了解到，批处理是如何通过增加吞吐量来降低延迟的？")])]),v._v(" "),_("ul",[_("li",[v._v("将多个请求一起批处理， 如何在处理更多请求的同时（称为吞吐量） 和快速响应 任何一个请求(称为延迟)之间进行权衡。")]),v._v(" "),_("li",[v._v("批处理是一种我们可以使用的技术，专们用于提高吞吐量，这在我们想要构建一个能够处理多个并发请求的服务器系统时非常重要，而不是像我们在这里处理的单个请求一样。")])]),v._v(" "),_("h3",{attrs:{id:"引入填充标记、-注意力蒙版"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#引入填充标记、-注意力蒙版"}},[v._v("#")]),v._v(" 引入填充标记、 注意力蒙版")]),v._v(" "),_("p",[v._v("在左侧引入填充标记，目的是确保：整体上矩阵的形状是一致的。")]),v._v(" "),_("ul",[_("li",[v._v('tokenizer 可以处理提示词列表: ["query1", "query2"]')]),v._v(" "),_("li")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_5.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[v._v("在左侧，填充标记，确保整体上矩阵上的形状是一样的")]),v._v(" "),_("p",[v._v("huggingface tokenizer 支持处理 prompts 列表")]),v._v(" "),_("div",{staticClass:"language-python extra-class"},[_("pre",{pre:!0,attrs:{class:"language-python"}},[_("code",[v._v("tokenizer"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("prompts"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" padding"),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),_("span",{pre:!0,attrs:{class:"token boolean"}},[v._v("True")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" return_tensors"),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),_("span",{pre:!0,attrs:{class:"token string"}},[v._v('"pt"')]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n")])])]),_("p",[v._v("shape: torch.Size([3, 7]), 形状是： 批量大小是3, 所有提供的输入中的最大序列长度为7 的  张量")]),v._v(" "),_("p",[v._v("填充后，注意力蒙版，有0，有1， 0：对应着我们引入的相同的填充标记。 后续不参与计算（告诉模型不应该关注，不要考虑填充标记）")]),v._v(" "),_("p",[v._v("应该忽略填充标记，不要影响想要生成的整体输出")]),v._v(" "),_("p",[v._v("在处理批量输入时，我们要介绍的一项新功能是位置ID的概念： position_ids")]),v._v(" "),_("p",[v._v("这些位置 ID 是与 HuggingFace Transformer实现中的某些特定内容相关的，但本质上只是告诉模型输入序列中每个标记的序数位置。")]),v._v(" "),_("p",[v._v("这只是个从Q到N的列表，对于N个标退，但是对于批量推断。我们确实需要将其填充为零，以使镇充标己在序列开始处被归零。从而官们不会对递增序列产生影响。")]),v._v(" "),_("p",[v._v("因此，我们将直先执行此操作，您可以以看到这里有一些额外的填充标退，但是在我们越过填充标记局。序列开始正式启动")]),v._v(" "),_("p",[v._v("现在,在选择下一个标记ID之前,我们所做的是取第一个批\n次,然后取最后一个序列元素,然后取所有的词汇可能性,\n然后计算argmax,但是这一次,我们要做的是改变这个最后\n一次的逻辑计算,而是要选择所有的批次,\n然后不再取全局\nargmax,而是要返回一个下一个标记ID的向量,每个批次一\n个,为了做到这一点,我们将对这个维度取argmax。")]),v._v(" "),_("p",[v._v("所以这一次让我们改变一下，让我们处理批次中的所有元素而不仅仅是第一个，并且我们将计算下—个标记ID而不是单个的下一个标记ID，并且我们将沿着维度试算下一个标這ID，然后像以前一样返回下一个标记旧以及我们的passkey值。")]),v._v(" "),_("blockquote",[_("p",[v._v("代码讲解，可参考代码")])]),v._v(" "),_("ul",[_("li",[v._v("例如，这将是我们想要为批次中的每一行生成的10个标记")]),v._v(" "),_("li",[v._v("我们将使用它来生成位置ID,排除注意力掩码中对应于填充标记的零的元素。")])]),v._v(" "),_("p",[v._v("然后,我们将扩展我们的批次,包括那些位置ID,以及我们\n最初想要传递给模型的所有其他关键字参数。")]),v._v(" "),_("p",[v._v("现在让我们继续做有趣的部分,根据先前的输入和我们刚\n刚生成的新标记集构建下一个输入批次。")]),v._v(" "),_("p",[v._v("因此,这里我们使用了KV缓存版本的这个过程,在我们称\n之为预填充步骤期间,我们会在一开始丢弃原始批次的输入。")]),v._v(" "),_("p",[v._v("对于位置ID,我们想要做一些类似的事情,即取位置ID\n的最后一个元素并将其增加一,你在这里看到了,然后我们\n将有效地继续丢弃序列中的所有前面的元素。")]),v._v(" "),_("p",[v._v("因此, 这将告诉我们我们正在提供的下一个标记集的位置ID是多少。")]),v._v(" "),_("p",[v._v("对于注意力蒙版,我们将做和之前完全一样的事情。")]),v._v(" "),_("p",[v._v("但是这里有一个稍微不同的额外注意事项,不同之处在于\n我们需要添加一个形状与批处理维度相等的全为1的向量\n到注意力掩码中。")]),v._v(" "),_("p",[v._v("现在让我们继续取出我们在这个特定迭代中生成的下一个\n标记ID向量,并将其转换为一个字符串列表,每个批次一个。")]),v._v(" "),_("p",[v._v("此时,我们有一个名为generateBatch的辅助函数,它接\n受一个来自标记器的输入字典和我们要生成的最大新标记\n数。")]),v._v(" "),_("p",[v._v("为此,我们将做一些有趣的事情,即我们将以红色实际呈现\n生成的标记,以便我们可以在视觉上将它们与原始输入区\n分开来。")]),v._v(" "),_("p",[v._v("13:30")]),v._v(" "),_("p",[v._v("因此,在这一点上,我们可以相当自信地认为该模型正在有\n效地正确地进行批处理,特别是因为我们的第一个序列与\n我们在为单个批次执行此操作时生成的完全相同,我们知\n道通过引入这种批处理,我们仍然在最终获得相同的输出。")]),v._v(" "),_("p",[v._v("14:07")]),v._v(" "),_("p",[v._v("批处理的一个明确目标是增加系统的吞吐量, 即在一段时间内我们可以生成的标记数量,在多个请求同时到达的情\n况下。")]),v._v(" "),_("p",[v._v("我们将观察到存在吞吐量和延迟之间的基本权衡。")]),v._v(" "),_("p",[v._v("14:34，")]),v._v(" "),_("p",[v._v("为了说明这一点,让我们从一个例子开始,我们只处理延迟\n优化系统,在这个系统中,每次收到请求时,我们将贪婪地处理它,而不进行任何批处理。")]),v._v(" "),_("p",[v._v("我们用不同的颜色标注时间轴上处理这个特定输入的部分\n,从它空闲时开始。")]),v._v(" "),_("p",[v._v("下一个请求进来了,它需要在原始输入仍在处理时空闲一\n段时间。")]),v._v(" "),_("p",[v._v("然后再次,它贪婪地立即处理,第三个请求也是如此。")]),v._v(" "),_("p",[v._v("这旨在优化我们的系统以降低延迟,我们试图尽量缩短任\n何单个请求的等待时间。")]),v._v(" "),_("p",[v._v("所以在这里我们看到,我们的平均请求延迟为1.2秒,但我\n们的吞吐量只有每秒一个请求,因为我们没有进行任何批\n处理。")]),v._v(" "),_("p",[v._v("15:25\n但在批处理的情况下,我们可以考虑如何在延迟和吞吐量\n之间权衡,优先考虑吞吐量而不是延迟。")]),v._v(" "),_("p",[v._v("一个我们可以做的事情是选择等待处理请求,直到达到一\n定数量的请求或者达到一定的时间限制。")]),v._v(" "),_("p",[v._v("15:44")]),v._v(" "),_("p",[v._v("在这种情况下,我们实际上会稍微降低我们的延迟,因为最\n初请求的用户必须稍等一会儿,但我们处理的请求总数在\n特定时间间隔内实际上是增加的。")]),v._v(" "),_("p",[v._v("在这种情况下,我们现在能够每秒处理。1.2个请求,\n而之前我们只能每秒处理一个请求。")]),v._v(" "),_("p",[v._v("所以这是一个很好的例子,尝试优先批处理可以帮助我们\n获得更好的吞吐量,但会牺牲我们可以提供给用户的延迟")]),v._v(" "),_("p",[v._v("好的,现在我们想要买际地研究延迟与吞吐量之间的这种\n影响,并试图了解等待不同批次大小对总吞吐量的影响,我\n们能够每秒生成多少个标记,以及平均延迟,或者每个标记\n平均生成需要多少秒。")]),v._v(" "),_("p",[v._v("从16:42 开始看")]),v._v(" "),_("p",[v._v("代码讲解, 结合代码去看")]),v._v(" "),_("p",[v._v("在这种情况下,我们将只设置一个常量,用于我们想要生成\n的标记数,这个数量将是10。")]),v._v(" "),_("p",[v._v("16：51\n然后,我们将定义一些数据结构来衡量我们的观察结果,比\n如持续时间,即每个样本处理所花费的时间,吞吐量,即每\n个实验样本的吞吐量,以及延迟,即每个样本的平均延迟。")]),v._v(" "),_("p",[v._v("在这种情况下,我们将尝试不同的二的幂次方,基本上从1\n到128变化,并看看这对吞吐量和延迟的影响。")]),v._v(" "),_("p",[v._v("17:28\n为了运行我们的实验,我们将首先迭代我们列表中的每个\n批处理大小,并在我们执行这些操作时进行一些简单的调\n试,我们将在每个步骤打印出批处理大小。")]),v._v(" "),_("p",[v._v("好的, 接下来我们将为每个批次生成标记并记录持续时间")]),v._v(" "),_("p",[v._v("第二步,我们将从一组大小为批处理大小的提示中形成一个批次。")]),v._v(" "),_("p",[v._v("所以我们要说的是,对于范围内的i值,我们将从我们最初\n的三个提示的原始列表中获取一个提示,并且我们将按顺\n序进行,所以这里的模运算确保我们可以为任何给定的i值\n获取一个提示。")]),v._v(" "),_("p",[v._v("这只是为了确保我们在发送给批处理的提示中有一些变化\n,而不是一遍又一遍地说同样的话。")]),v._v(" "),_("p",[v._v("生成批次,所以我们将生成最终输出作为字符串,然后最终\n记录整个过程花费的时间,单位为秒。")]),v._v(" "),_("p",[v._v("18:34， 接下来, 我们将继续记录这个特定批次大小的吞吐量和平均延迟的观察结果。")]),v._v(" "),_("p",[v._v("18:42\n所以首先,我们要计算总共生成了多少个标记,这是批次大小乘以最大标记数。")]),v._v(" "),_("p",[v._v("18：51\n然后通过将标记数除以持续时间(以秒为单位)来计算吞吐量。")]),v._v(" "),_("p",[v._v("18:57\n然后这里的平均延迟可以通过将持续时间(以秒为单位)除以最大标记数来计算。")]),v._v(" "),_("p",[v._v("19:24, 运行看结果")]),v._v(" "),_("p",[v._v("所以这里唯一的区别实际上是, 这里的结束标记值不影响平均延迟的计算。")]),v._v(" "),_("p",[v._v("19:28, 重要")]),v._v(" "),_("p",[v._v("如果你只是凭眼力观察,你会发现随着批处理大小的增加,\n延迟从低点开始,随着时间的推移而增加,而吞吐量也开始增加。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/though-%E5%90%9E%E5%90%90%E9%87%8F.jpg",alt:"批处理"}})]),v._v(" "),_("p",[v._v("因此,你可能还记得我们想要更高的吞吐量,更高的吞吐量是好的,我们想要更低的延迟。")]),v._v(" "),_("p",[v._v("19:49\n事实上,我们观察到了我们预期的权衡,随着时间的推移, 变得越来越好的吞吐量开始降低我们的延迟。")]),v._v(" "),_("p",[v._v("好的, 现在让我们定义一个函数来为我们绘制这种关系。")]),v._v(" "),_("p",[v._v("20:06,\n我们将向我们的渲染图函数传递一些东西,批处理大小,它将是x轴,吞吐量和延迟,它们将是我们两个重叠的y轴")]),v._v(" "),_("p",[v._v("然后我们只需为这些不同的轴传递一些标签。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/batching-1.jpg",alt:"批处理"}})]),v._v(" "),_("ul",[_("li",[v._v("从图中你可以看到,吞吐量在这里以红色开始相当低, 然后随着我们继续"),_("strong",[v._v("提供越来越大的批处理大小而开始线性增加")])])]),v._v(" "),_("p",[v._v("20：41\n而延迟也开始上升,虽然相对于吞吐量而言要慢一些,直到\n我们开始达到延迟变得非常高而吞吐量无法跟上的程度,\n你可以合理地说继续使用更大的批处理大小没有任何好处\n,因为延迟的权衡非常严重。")]),v._v(" "),_("p",[v._v("但对于这个范围内的几乎任何事情,你可以说你正在探索\n一个非常合理的权衡,在这里,吞吐量正在增加,延迟仍然\n很低,尽管在增加,真正取决于你个人的用例和判断,即什\n么是你要为其优化的正确批处理大小。")]),v._v(" "),_("p",[v._v("21:25, 所以批处理就是要获得更好的吞吐量,并愿意牺牲一定量\n的潜在延迟,以此来提高系统中多个用户或同时进入系统的多个请求的整体服务质量。")]),v._v(" "),_("p",[v._v("21:43, 在下一课中学习连续批处理,这是一种针对批处理\n的优化,试图解决延迟增加而吞吐量也增加的问题,\n试图保持高吞吐量的同时, 最小化生成下一个标记的延迟。")]),v._v(" "),_("h2",{attrs:{id:"动态批处理阶段优化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#动态批处理阶段优化"}},[v._v("#")]),v._v(" 动态批处理阶段优化")]),v._v(" "),_("h3",{attrs:{id:"continuous-batching"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#continuous-batching"}},[v._v("#")]),v._v(" Continuous Batching")]),v._v(" "),_("ul",[_("li",[v._v("在上一课中,您看到了批处理如何通过增加吞吐量来降低延迟。")]),v._v(" "),_("li",[v._v("在本课中,您将实现一种称为连续批处理的技术,利用迭代的逐词文本生成过程来获得高吞吐量和低延迟推理的最佳效果。")])]),v._v(" "),_("p",[v._v("00：32\n首先,让我们重新审视之前的同步批处理情况,即我们有多\n个不同时间到达的请求,然后我们希望将它们一起批处理到一个单独的批次中,以提高吞吐量。")]),v._v(" "),_("p",[v._v("正如我们所看到的,当我们这样做时,我们的吞吐量会增加\n,但我们的延迟会受到重大打击。")]),v._v(" "),_("p",[v._v("00：43\n然而,我们可以观察到的一点是,在同步批处理情况下,即\n使我们将所有这些请求放在一个单独的批次中并一起处理\n到最后,实际上,我们生成的每个标记都可以被视为一个单\n独的、独立的操作,可以从其他步骤中分解和分离出来。")]),v._v(" "),_("p",[v._v("这是因为,正如您所记得的,这些自回归语言模型实际上是\n逐标记生成的。")]),v._v(" "),_("p",[v._v("01:13,")]),v._v(" "),_("p",[v._v("这就引出了连续批处理的概念,即如果我们实际上按照请\n求的到达顺序贪婪地逐标记处理这些请求,但是在看到一\n个新请求时,当我们完成一个特定标记后,我们决定是否要\n继续将该请求合并到我们现有的批处理中,这样它们就可\n以继续一起生成标记,从而获得吞吐量的优势。")]),v._v(" "),_("p",[v._v("01:41, continuous-batching-1.jpg")]),v._v(" "),_("p",[v._v("进一步说,如果您的请求在不同时间完成,因为它们要么在\n较早时间开始,要么要生成的标记数较少,或者它们遇到停\n止标记,那么您可以有效地从批处理中删除其中一个序列,\n并将其替换为另一个正在等待轮到的请求。")]),v._v(" "),_("p",[v._v("02:07\n因此,不断地在批处理中移动元素的想法,即使有些元素可\n能会在某段时间内保留,也是我们所称的连续批处理的核心。")]),v._v(" "),_("p",[v._v("还有一件需要注意的事情是,再次强调这里的预填充与解\n码的概念是很重要的。")]),v._v(" "),_("p",[v._v("02:25, continuous-batching-2.jpg\n例如,您会注意到在图中,第一个标记的估计时间比后续标\n记长一些。")]),v._v(" "),_("p",[v._v("02:34, 因此,在连续批处理系统中,我们通常会将这些预加载和解\n码步骤概念上作为单独的批处理来处理。")]),v._v(" "),_("p",[v._v("02:40\n因此,如果我们同时收到多个需要进行预加载的请求,我们\n将一起处理它们,但否则,我们将尝试将所有解码请求保持\n在单个批处理中,以便我们可以限制我们必须做的过量填\n充,并且可以提高系统的吞吐量和延迟。")]),v._v(" "),_("p",[v._v("因此,在这个假设的例子中,您可以看到我们有点像两全其美。")]),v._v(" "),_("p",[v._v("03:04, 我们的延迟非常低：continuous-batching-3.jpg")]),v._v(" "),_("p",[v._v("03:06, 事实上,它甚至比我们一次处理一个请求的情况下还要低,\n并且在这种情况下,我们的吞吐量与同步批处理的情况下一样好。")]),v._v(" "),_("p",[v._v("但是,如果我们进一步扩展到生成不同数量标记的请求的\n情况,我们甚至可能会通过连续批处理来提高吞吐量。")]),v._v(" "),_("p",[v._v("03:36， 开始讲解代码, 参考代码")]),v._v(" "),_("p",[v._v("13:10, 当我们将它们连接起来时,实质上我们想要一个新的张量,\n其中包含两者的元素,并在必要时引入一些额外的填充,以\n确保形状对齐。 continuous-batching-4-merge-batch.jpg")]),v._v(" "),_("p",[v._v("14:11,")]),v._v(" "),_("p",[v._v("它将首先从批处理中删除已达到终端状态的元素,要么是\n因为终止标记,要么是因为已经生成了最大数量的标记,然\n后它将执行与合并步骤相反的操作,从开始处删除多余的\n填充标记,因为这会增加计算和内存开销。")]),v._v(" "),_("p",[v._v("14:20, continuous-batching-5-filtering-batches.jpg")]),v._v(" "),_("p",[v._v("14:34\n所以,当我们调用过滤批处理函数时,我们得到两个结果,\n一个是可能只剩下需要生成的内容的较小的缓存批次,另\n一个是在此步骤中被移除的批次中的索引列表。")]),v._v(" "),_("p",[v._v("14:57, 有一个initBatch函数,我们有generateNextToken函数\n,mergeBatches函数和filterBatch函数。")]),v._v(" "),_("p",[v._v("filter_batch 函数，所以,如果你想了解更多关于细节的信息,这些信息更多地\n与PyTorch有关,我邀请你看一看那个文件,它将包含有关\n每个步骤发生了什么的注释。")]),v._v(" "),_("p",[v._v("15:26， 重要，对比，做对比\n最后,让我们继续使用之前用于同步分批处理的相同输入\n来运行我们的连续分批处理过程,并查看是否有任何运行时的改进。")]),v._v(" "),_("p",[v._v("15:40， continuous-batching-6.jpg\n很好,完成时间约为21秒,您可能还记得,之前的实现大约\n需要71秒,因此整体端到端运行时显著减少了。")]),v._v(" "),_("p",[v._v("15:55, 您可能也注意到,在最后几个元素时需要更长一点的时间,\n因此可以想象这对应于我们想要生成100个标记的那些较\n长的序列。")]),v._v(" "),_("p",[v._v("很重要，很重要， Continuous Batching 重要原理，实现\n16:11, 因此,我们能够在最开始将它们添加到批处理中,然后在整\n个过程中继续添加和删除那些10个标记的较小序列。")]),v._v(" "),_("p",[v._v("16:19, 然后那些落后的、那些100个标记序列最终在未尾完成而\n不会在整个过程中造成瓶颈。")]),v._v(" "),_("p",[v._v("16:27， 这基本上就是连续批处理能够为我们做到的事情。")]),v._v(" "),_("p",[v._v("16:34，\n而这最终不仅是提高LLM推断的吞吐量和延迟的关键驱动\n因素之一,还是您常见的LLM推断系统中看到的标记流式处\n理能力的关键,因为我们能够及时处理它们,并从系统中逐\n个获取标记,我们能够非常迅速地将这些结果返回给用户,\n而不必等待这些大的批处理步骤。")]),v._v(" "),_("p",[v._v("17:03， 在接下来的课程中,我们将超越批处理,开始讨论其他可以\n提高系统效率的方法,包括量化,它帮助我们减少运行更大\n模型所需的内存压力,以及我们可以使用的技术来高效地\n提供经过精细调整的模型,例如低秩适应。")]),v._v(" "),_("p",[v._v("量化的目的，减少显存的使用，")]),v._v(" "),_("h2",{attrs:{id:"暂未整理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#暂未整理"}},[v._v("#")]),v._v(" 暂未整理")]),v._v(" "),_("p",[v._v("这就引出了连续批处理的概念,即如果我们实际上按照请求的到达顺序贪婪地逐标记处理这些请求,但是在看到一个新请求时,当我们完成一个特定标记后,我们决定是否要继续将该请求合并到我们现有的批处理中,这样它们就可以继续一起生成标记,从而获得吞吐量的优势。")]),v._v(" "),_("p",[v._v("因此,如果我们同时收到多个需要进行预加载的请求,我们将一起处理它们,但否则,我们将尝试将所有解码请求保持在单个批处理中,以便我们可以限制我们必须做的过量填充,并且可以提高系统的吞吐量和延迟。")]),v._v(" "),_("p",[v._v("进一步扩展到: 生成不同数量标记的请求的情况,我们甚至可能会"),_("strong",[v._v("通过连续批处理来提高吞吐量。")])]),v._v(" "),_("p",[v._v("该课程能学到:")]),v._v(" "),_("ul",[_("li",[_("strong",[v._v("了解自回归大型语言模型如何逐个标记生成文本")]),v._v("。")]),v._v(" "),_("li",[v._v("使用代码实现现代LLM推理堆栈的基本组成部分，"),_("strong",[v._v("包括KV缓存、连续批处理和模型量化，并对其对推理吞吐量和延迟的影响进行基准测试。")])]),v._v(" "),_("li",[v._v("探索LoRA适配器的工作细节，并学习批处理技术如何使不同的LoRA适配器能够同时为多个客户提供服务。")]),v._v(" "),_("li",[v._v("通过实际使用Predibase的LoRAX框架推理服务器，亲身实践这些优化技术在真实世界的LLM推理服务器中的应用。")])]),v._v(" "),_("blockquote",[_("p",[_("strong",[v._v("了解LLM服务器在内部是如何运作的将极大地增强您提高LLM应用程序性能和效率的选择。")])])]),v._v(" "),_("h3",{attrs:{id:"其他-暂未整理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#其他-暂未整理"}},[v._v("#")]),v._v(" 其他，暂未整理")]),v._v(" "),_("p",[_("strong",[v._v("将多个请求批处理")]),v._v("在一起"),_("strong",[v._v("是提高 LLM 推理系统吞吐量的关键技术之一。")])]),v._v(" "),_("h2",{attrs:{id:"其他-暂未整理-2"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#其他-暂未整理-2"}},[v._v("#")]),v._v(" 其他，暂未整理")]),v._v(" "),_("p",[v._v("KV Cache\n还有像KV Cache 这样的技术,通过在每个标记生成后将变压器网络的注意力计算结果存储在内存中来加快推理速度,\n这样在生成后续标记时，就不必重新执行这些计算")]),v._v(" "),_("p",[v._v("KV Cache 是如何大幅减少每个后续标记的延迟。")]),v._v(" "),_("p",[v._v("通过同时实施这些技术中的几种,您可以提高延迟(用户接收到响应所需的时间)以及吞吐量(服务器处理请求的速率）。")]),v._v(" "),_("ol",[_("li",[v._v("首先了解 自回归大型语言模型 如何一次生成一个标记的文本。")]),v._v(" "),_("li",[v._v("还将学习如何将提示批处理为单个张量，以便LLM 可以同时处理多个输入。")]),v._v(" "),_("li",[v._v("然后扩展这个想法，到一种：连续批处理的技术（Continuous Batching），它允许你在新请求到达时，和 旧请求完成时，动态更新批处理。")]),v._v(" "),_("li",[v._v("像 Credabase 这样的LLM 托管服务，能够同时为许多客户提供良好的延迟和吞吐量。")]),v._v(" "),_("li",[v._v("量化函数")]),v._v(" "),_("li",[v._v("LoRa, 还将多个 Lora 和 连续批处理技术相结合，以同时为数百个微调模型提供服务，同时保持吞吐量和低延迟。")]),v._v(" "),_("li",[v._v("Predabase")])]),v._v(" "),_("p",[v._v("了解这些基础知识很重要， 为你的项目或者应用 做决策")]),v._v(" "),_("h2",{attrs:{id:"模型架构"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#模型架构"}},[v._v("#")]),v._v(" 模型架构")]),v._v(" "),_("h3",{attrs:{id:"bert-模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#bert-模型"}},[v._v("#")]),v._v(" BERT 模型")]),v._v(" "),_("ul",[_("li",[v._v("基于编码器的模型")])]),v._v(" "),_("h3",{attrs:{id:"gpt-2"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#gpt-2"}},[v._v("#")]),v._v(" GPT-2")]),v._v(" "),_("ul",[_("li",[v._v("仅解码器模型，Decoder Only， 是没有编码器的。")])])])}),[],!1,null,null,null);_.default=n.exports}}]);