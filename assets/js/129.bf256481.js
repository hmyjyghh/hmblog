(window.webpackJsonp=window.webpackJsonp||[]).push([[129],{571:function(_,v,t){"use strict";t.r(v);var r=t(3),e=Object(r.a)({},(function(){var _=this,v=_._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("h2",{attrs:{id:"transformer"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[_._v("#")]),_._v(" Transformer")]),_._v(" "),v("h3",{attrs:{id:"_1-transformer-架构图分析"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-transformer-架构图分析"}},[_._v("#")]),_._v(" 1. Transformer 架构图分析")]),_._v(" "),v("blockquote",[v("p",[_._v("Transformer 模型包含两个主要部分: "),v("code",[_._v("Encoder(编码器)")]),_._v(" 和 "),v("code",[_._v("Decoder (解码器）")])])]),_._v(" "),v("p",[_._v("是一种用于"),v("strong",[_._v("理解数据序列的神经网络架构")]),_._v(", 广泛应用于NLP任务。")]),_._v(" "),v("h4",{attrs:{id:"encoder-编码器"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#encoder-编码器"}},[_._v("#")]),_._v(" Encoder(编码器)")]),_._v(" "),v("ol",[v("li",[_._v("Encoder接收输入序列, 并将其转换为连续表示序列。"),v("strong",[_._v("该序列将作为Decoder生成输出的基础")]),_._v("。")]),_._v(" "),v("li",[_._v("输入文本首先被转换为"),v("code",[_._v("word embeddings")]),_._v(", 这些向量表示能够捕捉词汇的语义信息。")]),_._v(" "),v("li",[_._v("向embeddings注入"),v("code",[_._v("positional encoding")]),_._v(", 以标识"),v("code",[_._v("词汇在序列中的位置信息")]),_._v("及词问相对距离。")]),_._v(" "),v("li",[_._v("每个编码块包含两个核心子模块: "),v("code",[_._v("Multi-Head Attention")]),_._v("机制与"),v("code",[_._v("FFN")]),_._v(" (前馈网络层)。")]),_._v(" "),v("li",[_._v("每个子模块均采用"),v("strong",[_._v("残差连接")]),_._v("并接入"),v("code",[_._v("normalization")]),_._v("层, 该设计能有效促进深层模型训练并稳定学习过程。")])]),_._v(" "),v("p",[_._v("上述流程描述的是单个Encoder的处理过程。Encoder最终输出的向量维度与输入维度保持一致, 实际应用中可将多个Encoder进行堆叠使用。")]),_._v(" "),v("h3",{attrs:{id:"decoder-解码器"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#decoder-解码器"}},[_._v("#")]),_._v(" Decoder(解码器)")]),_._v(" "),v("ol",[v("li",[_._v("Decoder生成输出序列(如另一种语言的句子), "),v("strong",[_._v("一次生成一个词")]),_._v("。它基于encoder输出的连续表示序列和先前生成的符号来生成下一个符号。")]),_._v(" "),v("li",[_._v("Decoder也由多个相同的层组成, 但额外添加了一个"),v("strong",[_._v("多头注意力机制来关注编码器的输出")]),_._v("。")]),_._v(" "),v("li",[_._v("首个子模块采用"),v("code",[_._v("Masked Multi-Head Self-Attention")]),_._v("机制,通i过masking确保对特定位置的预测不会依赖后续位置信息。")]),_._v(" "),v("li",[_._v("架构图中，可以看到接收"),v("code",[_._v("Encoder")]),_._v("输出的"),v("code",[_._v("Multi-Head Attention")]),_._v("模块, 该模块被称作"),v("code",[_._v("Encoder-Decoder Attention")]),_._v(", 其"),v("code",[_._v("keys")]),_._v("与"),v("code",[_._v("values")]),_._v("向量源于Encoder。")]),_._v(" "),v("li",[_._v("第二个子模块对Encoder输出执行"),v("code",[_._v("Multi-Head Attention")]),_._v("计算, "),v("code",[_._v("keys与values")]),_._v("取自Encoder输出, "),v("strong",[_._v("而queries来自前一层Decoder的输出")]),_._v("。\n"),v("ul",[v("li",[_._v('例如预测"am"时,"I" 将作为query与key, '),v("strong",[_._v("values则来自Encoder")])])])]),_._v(" "),v("li",[_._v('以生成翻译结果"I am..."为例,Decoder首先生成"I", 随后将该结果回传至'),v("code",[_._v("Decoder")]),_._v('网络以生成后续词汇"am"(对应架构图中Decoder模块的"Outputs"流向)。')]),_._v(" "),v("li",[v("code",[_._v("Decoder")]),_._v("中的"),v("code",[_._v("Multi-Head Attention")]),_._v("机制存在特殊约束: 由于输出序列不完整, 注意力"),v("strong",[_._v("仅允许关注输出序列中已生成的位置")]),_._v("(通过将未来位置得分设置为负无穷实现)。")]),_._v(" "),v("li",[v("code",[_._v("Decoder")]),_._v("内的"),v("code",[_._v("Encoder-Decoder Attention")]),_._v("运作方式与常规"),v("code",[_._v("Attention")]),_._v("类似, 但其query矩阵来自下层"),v("code",[_._v("Decoder")]),_._v("输出, keys与values向量则取自Encoder堆栈的最终输出。")])]),_._v(" "),v("h3",{attrs:{id:"最终层和softmax层"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#最终层和softmax层"}},[_._v("#")]),_._v(" 最终层和Softmax层")]),_._v(" "),v("blockquote",[v("p",[_._v("涉及：logits、概率分布")])]),_._v(" "),v("ul",[v("li",[_._v("Decoder堆栈的最终输出为一个浮点数向量。")]),_._v(" "),v("li",[_._v("通过未端Linear层与Softmax层将该向量转换为具体单词。")]),_._v(" "),v("li",[_._v("Linear层作为基础神经网络层,将Decoder输出向量转换为维度更大的logits向量。")]),_._v(" "),v("li",[_._v('假设模型从训练数据中学得10,000个唯一英语单词, 这些词汇构成模型的"输出词汇表"。')]),_._v(" "),v("li",[_._v("此时logits向量将包含10,000个数值位, 每个位置对应输出词汇表中一个特定词的得分。")]),_._v(" "),v("li",[_._v("Softmax层将这些得分转换为概率分布, 所有概率值为正数且总和为1.0。")]),_._v(" "),v("li",[_._v("选择"),v("strong",[_._v("概率最高的位置")]),_._v(", 该位置对应的单词即作为当前步骤的最终输出。")]),_._v(" "),v("li",[_._v("当模型识别到"),v("code",[_._v("<end of sentence>")]),_._v("标记时, "),v("strong",[_._v("此生成过程终止")]),_._v("。")])]),_._v(" "),v("h3",{attrs:{id:"架构图"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#架构图"}},[_._v("#")]),_._v(" 架构图")]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/llm/transformer/transformer.png",alt:"transformer"}})]),_._v(" "),v("blockquote",[v("p",[_._v("相关")])]),_._v(" "),v("ul",[v("li",[_._v("Add & Norm, 残差连接 和 层归一化的 作用是什么?")])]),_._v(" "),v("h3",{attrs:{id:"_2-transformer-基本流程"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-transformer-基本流程"}},[_._v("#")]),_._v(" 2. Transformer 基本流程")]),_._v(" "),v("ul",[v("li",[_._v("同上")])]),_._v(" "),v("h2",{attrs:{id:"注意力机制"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#注意力机制"}},[_._v("#")]),_._v(" 注意力机制")]),_._v(" "),v("p",[_._v("transformer 中的"),v("code",[_._v("Attention")]),_._v("机制, 其核心在于"),v("strong",[_._v("关注输入序列中的不同部分")]),_._v(", 计算每个词与其他词的相关性。")]),_._v(" "),v("blockquote",[v("p",[_._v("Attention机制主要包括以下步骤:")])]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("输入表示")]),_._v(": "),v("strong",[_._v("将输入序列映射为查询(Query)、键(Key)和值(Value)三个矩阵")]),_._v("。")]),_._v(" "),v("li",[v("strong",[_._v("计算相似度")]),_._v(": 通过"),v("code",[_._v("点积计算")]),_._v("查询与键的相似度, 并进行缩放。")]),_._v(" "),v("li",[v("strong",[_._v("应用softmax")]),_._v(": 将相似度通过softmax转换为权重。")]),_._v(" "),v("li",[v("strong",[_._v("加权求和")]),_._v(": 使用这些权重对值进行加权求和,得到"),v("code",[_._v("Attention")]),_._v("输出。")])]),_._v(" "),v("h3",{attrs:{id:"公式如下"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#公式如下"}},[_._v("#")]),_._v(" 公式如下：")]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/llm/transformer/Attention.png",alt:"transformer Attention"}})]),_._v(" "),v("blockquote",[v("p",[v("code",[_._v("Self-Attention")]),_._v("和"),v("code",[_._v("Cross-Attention")]),_._v("是 Transformer中两种不同的注意力机制")])]),_._v(" "),v("h3",{attrs:{id:"_1-self-attention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-self-attention"}},[_._v("#")]),_._v(" 1. Self-Attention")]),_._v(" "),v("ul",[v("li",[_._v("QKV 必须同源")]),_._v(" "),v("li",[v("code",[_._v("Self-Attention")]),_._v("的作用是: 在同一个序列内, "),v("strong",[_._v("计算每个词与其他词的相关性")]),_._v(", 它位于编码器和解码器的各层中,")]),_._v(" "),v("li",[_._v("其优点是能够捕捉"),v("strong",[_._v("序列中长距离依赖关系, 处理整个输入序列, 关注输入的不同部分")]),_._v("。")])]),_._v(" "),v("h3",{attrs:{id:"_2-cross-attention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-cross-attention"}},[_._v("#")]),_._v(" 2. Cross Attention")]),_._v(" "),v("ul",[v("li",[v("p",[v("code",[_._v("QK")]),_._v(" 不同源， 但是"),v("code",[_._v("KV")]),_._v("同源")])]),_._v(" "),v("li",[v("p",[v("code",[_._v("Self-Attention")]),_._v(" 在编码器和解码器中都使用, "),v("strong",[_._v("用于处理单个序列内部的依赖关系")]),_._v("。")])]),_._v(" "),v("li",[v("p",[v("code",[_._v("Cross-Attention")]),_._v(" "),v("strong",[_._v("仅在解码器中使用")]),_._v(", 用于解码器与编码器之间的交互, "),v("strong",[_._v("使得生成的词与输入序列对齐")]),_._v("。")])])]),_._v(" "),v("p",[_._v("总的来说:")]),_._v(" "),v("ul",[v("li",[v("p",[v("code",[_._v("Self-Attention")]),_._v(" 是在序列内部计算相关性, 适用于编码器和解码器的每一层。")])]),_._v(" "),v("li",[v("p",[_._v("Cross-Attention是在解码器中, 用于解码器和编码器之间的交互, "),v("strong",[_._v("帮助生成输出时参考输入序列")]),_._v("。")])])]),_._v(" "),v("h3",{attrs:{id:"_3-multi-head-attention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-multi-head-attention"}},[_._v("#")]),_._v(" 3. Multi-Head Attention")]),_._v(" "),v("ul",[v("li",[v("p",[_._v("Transformer中的多头注意力机制，借鉴了"),v("code",[_._v("CNN")]),_._v("中同一卷积层内"),v("strong",[_._v("使用多个卷积核的思想")]),_._v(",")])]),_._v(" "),v("li",[v("p",[_._v("Transformer的多头注意力机制与之类似，"),v("strong",[_._v("它采用多个并行的注意力头")]),_._v("，每个头都有独立的参数:"),v("code",[_._v("QKV")]),_._v("，基于"),v("strong",[_._v("缩放点积注意力")]),_._v("进行计算。")])]),_._v(" "),v("li",[v("p",[_._v("不同的注意力头可以关注输入序列的不同方面，将输入向量映射到不同的表示子空间中，捕捉不同的特征信息，最后**再将各头的输出拼接在一起，**形成整体的输出结果。")])]),_._v(" "),v("li",[v("p",[_._v("这种设计让模型能够从多个角度分析输入序列，增强了模型的表达能力，就像多个卷积核使CNN能提取更丰富的图像特征一样。")])])]),_._v(" "),v("ol",[v("li",[_._v("原文中使用了8个"),v("code",[_._v("scaled dot-product attention")]),_._v(", 在同一个"),v("code",[_._v("Multi-head Attention")]),_._v("层中, 输入均为"),v("code",[_._v("QKV")]),_._v(", 同时进行注意力的计算,")]),_._v(" "),v("li",[_._v("彼此之前参数不共享, "),v("strong",[_._v("最终将结果拼接起来")]),_._v(", "),v("strong",[_._v("这样可以允许模型在不同的表示子空间里学习到相关的信息")]),_._v("。")])]),_._v(" "),v("ul",[v("li",[_._v("缩放点积注意力机制：QK 相乘，做一次scale, 然后softmax, 再和V 进行加权求和。")])]),_._v(" "),v("h2",{attrs:{id:"位置编码"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#位置编码"}},[_._v("#")]),_._v(" 位置编码")]),_._v(" "),v("h3",{attrs:{id:"_1-什么是位置编码"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-什么是位置编码"}},[_._v("#")]),_._v(" 1. 什么是位置编码?")]),_._v(" "),v("blockquote",[v("p",[_._v("位置编码是一种用于: "),v("strong",[_._v("在序列数据中为每个位置添加位置信息的技术")]),_._v("。")])]),_._v(" "),v("ul",[v("li",[_._v("在"),v("code",[_._v("NLP")]),_._v(" 中，"),v("strong",[_._v("位置编码通常用于处理文本序列")]),_._v("。")]),_._v(" "),v("li",[_._v("引入位置编码，可以帮助模型更好地理解和处理序列数据。")])]),_._v(" "),v("p",[v("strong",[_._v("在"),v("code",[_._v("Transformer")]),_._v("模型中，")])]),_._v(" "),v("ul",[v("li",[_._v("通过为输入序列中的"),v("strong",[_._v("每一个位置分配一个独一无二的向量")]),_._v("，来实现添加位置信息。")]),_._v(" "),v("li",[_._v("这些向量包含了位置的信息, 通常会与词嵌入(word embeddirgs)相加, 形成最终的输入表示。")]),_._v(" "),v("li",[v("strong",[_._v("使模型能够区分不同位置上的词汇")]),_._v(", 并理解它们在句子结构中的相对位置。")])]),_._v(" "),v("h4",{attrs:{id:"两个关键点分析"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#两个关键点分析"}},[_._v("#")]),_._v(" "),v("strong",[_._v("两个关键点分析")])]),_._v(" "),v("ol",[v("li",[_._v("使模型能够区分不同位置上的词汇，并理解它们在句子结构中的相对位置。")]),_._v(" "),v("li",[_._v("帮助模型更好地理解和处理序列数据。")])]),_._v(" "),v("h4",{attrs:{id:"实际使用"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#实际使用"}},[_._v("#")]),_._v(" "),v("strong",[_._v("实际使用")])]),_._v(" "),v("ul",[v("li",[v("p",[v("code",[_._v("Positional Encoding")]),_._v(" 是模型在特定任务上训练时学习到的, 然后会与"),v("code",[_._v("词向量embedding")]),_._v("相加, 再送入"),v("code",[_._v("Multi-Head Attention")]),_._v("。")])]),_._v(" "),v("li",[v("p",[_._v("这可以确保embeddings"),v("strong",[_._v("能理解每个词的位置")]),_._v("或序列中不同词之间的距离。")])])]),_._v(" "),v("h3",{attrs:{id:"_2-为什么transformer需要位置编码"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-为什么transformer需要位置编码"}},[_._v("#")]),_._v(" 2. 为什么Transformer需要位置编码?")]),_._v(" "),v("ul",[v("li",[_._v("Transformer 抛弃了RNN循环神经网络，顺序处理数据的方式")]),_._v(" "),v("li",[_._v("改用注意力机制，这种方式不知道词汇之间的位置信息 和 语义关系")]),_._v(" "),v("li",[_._v("所以需要引入"),v("strong",[_._v("位置编码")]),_._v("，让模型知道输入序列中各个单词的顺序，以便更好地理解序列数据。")])]),_._v(" "),v("h3",{attrs:{id:"_3-相对位置编码"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-相对位置编码"}},[_._v("#")]),_._v(" 3. 相对位置编码")]),_._v(" "),v("blockquote",[v("p",[_._v("相对位置编码是一种: 在"),v("code",[_._v("Transformer")]),_._v("用于"),v("strong",[_._v("捕捉序列中元素之间的相对位置关系")]),_._v(", 而不是像绝对位置编码那样为每个位置分配一个固定的、绝对的编码。")])]),_._v(" "),v("ul",[v("li",[_._v("在绝对位置编码中, 序列中的每个位置都有一个独一无二的向量表示, 这个表示是基于位置的绝对数值(例如, 在序列中的第几个位置)。")]),_._v(" "),v("li",[_._v("而"),v("strong",[_._v("相对位置编码则是为序列中任意两个位置之间的相对距离分配一个编码")]),_._v(', 这样可以表达"位置A在位置B之前或之后多少步"这样的信息。')]),_._v(" "),v("li",[v("strong",[_._v("相对位置编码有助于模型理解序列元素间的关系")]),_._v(", 特别是当"),v("strong",[_._v("序列很长")]),_._v("或者"),v("strong",[_._v("需要模型理解超越固定上下文长度的依赖时更为有效")]),_._v("。")]),_._v(" "),v("li",[_._v("相对位置编码的一个主要优点在于它能更好地处理变长序列, 并且在处理循环或有周期性模式的语言结构时更为灵活。")]),_._v(" "),v("li",[_._v("由于编码的是相对距离, 这种表示法在不同长度的序列中更加通用, 有助于模型在训练和预测时对长距离依赖有更好的捕捉能力。")])]),_._v(" "),v("h4",{attrs:{id:"示例分析"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#示例分析"}},[_._v("#")]),_._v(" 示例分析")]),_._v(" "),v("p",[_._v("eg: 句子 "),v("code",[_._v("我 爱 吃 苹果")]),_._v("，绝对位置编码就是 "),v("code",[_._v("[1,2,3,4]")]),_._v("。")]),_._v(" "),v("ul",[v("li",[_._v("相对位置编码\n逻辑更灵活：不纠结「自己是第几个」，而是关注「我和另一个人隔了几个位置」。\n比如：")]),_._v(" "),v("li",[_._v("对于 "),v("code",[_._v("吃")]),_._v(" 来说，它和 "),v("code",[_._v("爱")]),_._v(" 隔了 "),v("code",[_._v("1")]),_._v(" 个位置，和 "),v("code",[_._v("苹果")]),_._v(" 隔了 "),v("code",[_._v("1")]),_._v(" 个位置。")]),_._v(" "),v("li",[_._v("对于 "),v("code",[_._v("爱")]),_._v(" 来说，它和 "),v("code",[_._v("吃")]),_._v(" 隔了 "),v("code",[_._v("1")]),_._v(" 个位置，和 "),v("code",[_._v("苹果")]),_._v(" 隔了 "),v("code",[_._v("2")]),_._v(" 个位置。")])]),_._v(" "),v("p",[_._v("相对位置编码存的是**「相对距离值」**，而不是「绝对编号」。")]),_._v(" "),v("h4",{attrs:{id:"与绝对位置编码相比-优势有"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#与绝对位置编码相比-优势有"}},[_._v("#")]),_._v(" 与绝对位置编码相比，优势有：")]),_._v(" "),v("ol",[v("li",[_._v("更强的「长度泛化能力」：长短句子都能适应")]),_._v(" "),v("li",[_._v("更精准的「语义关联」：距离近的词关系更紧密。(在自然语言里，"),v("strong",[_._v("距离越近的词，语义关联越强")]),_._v("。)")]),_._v(" "),v("li",[_._v("对「截断/拼接」更友好：修改句子不用改编码")])]),_._v(" "),v("h3",{attrs:{id:"_4-旋转位置编码-rope"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-旋转位置编码-rope"}},[_._v("#")]),_._v(" 4. 旋转位置编码(ROPE)")]),_._v(" "),v("p",[v("strong",[_._v("旋转位置编码（Rotary Position Embedding, RoPE）")]),_._v(" 是一种"),v("strong",[_._v("位置感知编码方案")]),_._v("，核心思想是通过"),v("strong",[_._v("三角函数的旋转操作")]),_._v("将位置信息嵌入到Transformer模型的 token 表示中，解决了传统位置编码（如绝对位置编码、相对位置编码）在长文本、动态长度输入场景下的局限性。")]),_._v(" "),v("h3",{attrs:{id:"核心原理"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#核心原理"}},[_._v("#")]),_._v(" 核心原理")]),_._v(" "),v("p",[_._v("RoPE 的核心是"),v("strong",[_._v("将位置信息编码为向量空间中的旋转矩阵")]),_._v("，让 token 的表示随位置发生旋转，从而在自注意力计算中自然引入位置依赖。")]),_._v(" "),v("h3",{attrs:{id:"与传统位置编码的对比"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#与传统位置编码的对比"}},[_._v("#")]),_._v(" 与传统位置编码的对比")]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",[_._v("编码类型")]),_._v(" "),v("th",[_._v("核心方式")]),_._v(" "),v("th",[_._v("优点")]),_._v(" "),v("th",[_._v("缺点")])])]),_._v(" "),v("tbody",[v("tr",[v("td",[v("strong",[_._v("绝对位置编码")])]),_._v(" "),v("td",[_._v("为每个位置分配唯一向量，与 token 向量相加")]),_._v(" "),v("td",[_._v("实现简单")]),_._v(" "),v("td",[_._v("不支持超长文本；泛化性差")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("相对位置编码")])]),_._v(" "),v("td",[_._v("在自注意力计算中直接引入相对位置偏置")]),_._v(" "),v("td",[_._v("关注位置关系")]),_._v(" "),v("td",[_._v("依赖预定义位置范围；计算复杂度高")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("RoPE")])]),_._v(" "),v("td",[_._v("通过旋转矩阵嵌入位置信息")]),_._v(" "),v("td",[_._v("支持动态长度；相对位置不变；无额外参数")]),_._v(" "),v("td",[_._v("仅支持偶数维度（需 padding 适配奇数维度）")])])])]),_._v(" "),v("h3",{attrs:{id:"核心优势"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#核心优势"}},[_._v("#")]),_._v(" 核心优势")]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("无额外参数")]),_._v("：RoPE 是一种"),v("strong",[_._v("计算型编码")]),_._v("，不需要像绝对位置编码那样学习额外的位置向量，不增加模型参数量。")]),_._v(" "),v("li",[v("strong",[_._v("长文本适配性")]),_._v("：天然支持超长序列输入，不会因文本长度超过预训练长度而失效（可通过插值扩展到更长序列）。")]),_._v(" "),v("li",[v("strong",[_._v("相对位置感知")]),_._v("：自注意力计算直接反映 token 间的相对位置，更符合自然语言的上下文依赖逻辑。")]),_._v(" "),v("li",[v("strong",[_._v("跨尺度一致性")]),_._v("：在不同长度的文本上表现稳定，适合大模型的预训练和微调。")])]),_._v(" "),v("h3",{attrs:{id:"典型应用场景"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#典型应用场景"}},[_._v("#")]),_._v(" 典型应用场景")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("大模型预训练")]),_._v("：LLaMA、ChatGLM、GPT-NeoX 等模型均采用 "),v("code",[_._v("RoPE")]),_._v(" 作为位置编码方案。")]),_._v(" "),v("li",[v("strong",[_._v("长文本处理")]),_._v("：如文档摘要、法律文本分析等需要处理超长序列的任务。")]),_._v(" "),v("li",[v("strong",[_._v("动态输入场景")]),_._v("：如对话系统中，用户输入长度不固定的场景。")])])])}),[],!1,null,null,null);v.default=e.exports}}]);