(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{561:function(_,v,a){"use strict";a.r(v);var t=a(3),i=Object(t.a)({},(function(){var _=this,v=_._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("h2",{attrs:{id:"向量数据库"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#向量数据库"}},[_._v("#")]),_._v(" 向量数据库")]),_._v(" "),v("blockquote",[v("p",[_._v("是⼀种专⻔⽤来存储和查询向量嵌⼊数据的数据库。")])]),_._v(" "),v("h3",{attrs:{id:"_1-基础概念"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-基础概念"}},[_._v("#")]),_._v(" 1. 基础概念")]),_._v(" "),v("ul",[v("li",[_._v("向量是具有大小和方向的数学结构")]),_._v(" "),v("li",[_._v("可以将事物的特征用向量来表示")])]),_._v(" "),v("h3",{attrs:{id:"_2-向量数据库的核心思想"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-向量数据库的核心思想"}},[_._v("#")]),_._v(" 2. 向量数据库的核心思想")]),_._v(" "),v("blockquote",[v("p",[_._v("将文本转为向量，然后将向量存储到数据库中")])]),_._v(" "),v("ul",[v("li",[_._v("当用户输入问题时，将问题转换为向量")]),_._v(" "),v("li",[_._v("然后在数据库中搜索"),v("code",[_._v("最相似的向量")]),_._v("和"),v("code",[_._v("匹配最相似的几个上下文")])]),_._v(" "),v("li",[_._v("最后将文本返回给用户")])]),_._v(" "),v("h3",{attrs:{id:"_3-bm25、tf-idf-排序算法"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-bm25、tf-idf-排序算法"}},[_._v("#")]),_._v(" 3. BM25、TF-IDF，排序算法")]),_._v(" "),v("ul",[v("li",[_._v("本质还是基于文本的精确匹配")]),_._v(" "),v("li",[_._v("但对于语义搜索功能就非常弱")])]),_._v(" "),v("h3",{attrs:{id:"_4-vector-embedding"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-vector-embedding"}},[_._v("#")]),_._v(" 4. Vector Embedding")]),_._v(" "),v("blockquote",[v("p",[_._v("Vector Embedding 是由 AI 模型（例如大型语言模型 LLM）生成的，它会根据不同的算法生成高维度的向量数据，代表着数据的不同特征，这些特征代表了数据的不同维度。")])]),_._v(" "),v("p",[_._v("例如这份文档是客服培训资料或者操作手册，我们可以先将这份文档的所有内容转化成向量（这个过程称之为 Vector Embedding）")]),_._v(" "),v("ul",[v("li",[_._v("例如，对于文本，这些特征可能包括词汇、语法、语义、情感、情绪、主题、上下文等。对于音频，这些特征可能包括音调、节奏、音高、音色、音量、语音、音乐等。")])]),_._v(" "),v("h3",{attrs:{id:"_5-特征和向量"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_5-特征和向量"}},[_._v("#")]),_._v(" 5. 特征和向量")]),_._v(" "),v("ol",[v("li",[_._v("如何判断相似度?")])]),_._v(" "),v("ul",[v("li",[_._v("可以通过比较向量之间的距离来判断它们的相似度")])]),_._v(" "),v("h3",{attrs:{id:"_6-相似性搜索-similarity-search"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_6-相似性搜索-similarity-search"}},[_._v("#")]),_._v(" 6. 相似性搜索(Similarity Search)")]),_._v(" "),v("ul",[v("li",[_._v("高效的搜索算法有很多，其主要思想是通过两种方式提高搜索效率：")])]),_._v(" "),v("ol",[v("li",[v("p",[_._v("减少向量大小：通过降维或减少表示向量值的长度。")])]),_._v(" "),v("li",[v("p",[_._v("缩小搜索范围：可以通过"),v("code",[_._v("聚类")]),_._v("或"),v("code",[_._v("将向量组织成基于树形、图形结构来实现")]),_._v("，并限制搜索范围"),v("code",[_._v("仅在最接近的簇中")]),_._v("进行，或者通过最相似的分支进行过滤。")])])]),_._v(" "),v("h3",{attrs:{id:"_3-近似最近邻算法-approximate-nearest-neighbor"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-近似最近邻算法-approximate-nearest-neighbor"}},[_._v("#")]),_._v(" 3. 近似最近邻算法(Approximate Nearest Neighbor)")]),_._v(" "),v("ul",[v("li",[_._v("包含暴力搜索   和  聚类算法")])]),_._v(" "),v("blockquote",[v("p",[_._v("常见的聚类算法有 "),v("code",[_._v("K-Means")])])]),_._v(" "),v("p",[v("strong",[_._v("k-means 算法的基本步骤")])]),_._v(" "),v("ol",[v("li",[_._v("选择"),v("code",[_._v("k")]),_._v("个初始聚类中心。")]),_._v(" "),v("li",[_._v("将每个数据点分配到最近的聚类中心。")]),_._v(" "),v("li",[_._v("计算每个聚类的新中心。")]),_._v(" "),v("li",[_._v("重复步骤 2 和 3，直到聚类中心不再改变或达到最大迭代次数(趋于稳定)。")])]),_._v(" "),v("h3",{attrs:{id:"_4-pq-算法"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-pq-算法"}},[_._v("#")]),_._v(" 4. PQ 算法")]),_._v(" "),v("p",[_._v("出现的原因是： 在大规模数据集中，聚类算法最大的问题在于内存占用太大。")]),_._v(" "),v("ol",[v("li",[_._v("除了要保存每个向量的坐标")]),_._v(" "),v("li",[_._v("还需要维护聚类中心和每个向量的聚类中心索引，这也会占用大量的内存。")])]),_._v(" "),v("p",[_._v("对于问题1： 可以通过量化 (Quantization) 的方式解决，也就是常见的有损压缩。")]),_._v(" "),v("p",[_._v("降维：")]),_._v(" "),v("ul",[v("li",[_._v("比如将 128 维的向量分为 8 个 16 维的向量，")]),_._v(" "),v("li",[_._v("然后在 8 个 16 维的子向量上分别进行聚类，因为 16 维的子向量大概只需要 256 个聚类中心就能得到还不错的量化结果，所以就可以将码本的大小从 2^64 降低到 8 * 256 = 2048 个聚类中心，从而降低内存开销。")])]),_._v(" "),v("p",[_._v("这也就是乘积量化"),v("code",[_._v("（Product Quantization）")]),_._v("的原理。")]),_._v(" "),v("ul",[v("li",[_._v("使用 PQ 算法，可以显著的减少内存的开销，同时加快搜索的速度，")]),_._v(" "),v("li",[_._v("它唯一的问题是搜索的质量会有所下降，但就像我们刚才所讲，所有算法都是在内存、速度和质量上做一个权衡。")])]),_._v(" "),v("h3",{attrs:{id:"_5-hnsw"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_5-hnsw"}},[_._v("#")]),_._v(" 5. HNSW")]),_._v(" "),v("blockquote",[v("p",[_._v("Hierarchical Navigable Small Worlds: 分层导航的小世界")])]),_._v(" "),v("ul",[v("li",[_._v("除了聚类以外，也可以通过构建树或者构建图的方式来实现近似最近邻搜索。")]),_._v(" "),v("li",[_._v("这种方法的基本思想是每次将向量加到数据库中的时候，就先找到与它最相邻的向量，然后将它们连接起来，这样就构成了一个图。")]),_._v(" "),v("li",[_._v("当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻搜索和最短路径计算，直到找到最相似的向量。")])]),_._v(" "),v("p",[_._v("HNSW 算法是一种经典的空间换时间的算法，它的搜索质量和搜索速度都比较高，但是它的内存开销也比较大，因为不仅需要将所有的向量都存储在内存中。还需要维护一个图的结构，也同样需要存储。所以这类算法需要根据实际的场景来选择。")]),_._v(" "),v("h3",{attrs:{id:"_6-局部敏感哈希算法-lsh"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_6-局部敏感哈希算法-lsh"}},[_._v("#")]),_._v(" 6. 局部敏感哈希算法(LSH)")]),_._v(" "),v("ul",[v("li",[_._v("局部敏感哈希（Locality Sensitive Hashing）也是一种使用近似最近邻搜索的索引技术。")]),_._v(" "),v("li",[_._v("它的特点是快速，同时仍然提供一个近似、非穷举的结果。")]),_._v(" "),v("li",[_._v("LSH 使用一组哈希函数将相似向量映射到“桶”中，从而使相似向量具有相同的哈希值。")]),_._v(" "),v("li",[_._v("这样，就可以通过比较哈希值来判断向量之间的相似度。")])]),_._v(" "),v("h3",{attrs:{id:"_7-相似性测量-similarity-measurement"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_7-相似性测量-similarity-measurement"}},[_._v("#")]),_._v(" 7. 相似性测量 (Similarity Measurement)")]),_._v(" "),v("ol",[v("li",[_._v("欧几里得距离（Euclidean Distance）")]),_._v(" "),v("li",[_._v("余弦相似度（Cosine Similarity）")]),_._v(" "),v("li",[_._v("点积相似度 (Dot product Similarity)")])]),_._v(" "),v("ul",[v("li",[_._v("余弦距离，越⼤越相似")]),_._v(" "),v("li",[_._v("欧式距离，越⼩越相似")])]),_._v(" "),v("p",[_._v("在您的RAG系统中，文本嵌入通常使用"),v("strong",[_._v("余弦相似度")]),_._v("，因为"),v("strong",[_._v("它更关注语义方向")]),_._v("而非向量长度。")]),_._v(" "),v("h3",{attrs:{id:"_8-过滤-filtering"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_8-过滤-filtering"}},[_._v("#")]),_._v(" 8. 过滤 (Filtering)")]),_._v(" "),v("p",[_._v("在实际的业务场景中，往往不需要在整个向量数据库中进行相似性搜索，"),v("code",[_._v("而是通过部分的业务字段")]),_._v("进行过滤再进行查询。")]),_._v(" "),v("ul",[v("li",[_._v("所以存储在数据库的向量往往还需要包含"),v("code",[_._v("元数据")]),_._v("，例如用户 ID、文档 ID 等信息。\n-`这样就可以在搜索的时候，根据元数据来过滤搜索结果，从而得到最终的结果。")])]),_._v(" "),v("h3",{attrs:{id:"_8-如何选型向量数据库"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_8-如何选型向量数据库"}},[_._v("#")]),_._v(" 8. 如何选型向量数据库?")]),_._v(" "),v("h4",{attrs:{id:"milvus"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#milvus"}},[_._v("#")]),_._v(" Milvus")]),_._v(" "),v("p",[_._v("Milvus的关键特性包括：")]),_._v(" "),v("ul",[v("li",[_._v("毫秒级搜索万亿级向量数据集")]),_._v(" "),v("li",[_._v("简单管理⾮结构化数据")]),_._v(" "),v("li",[_._v("可靠的向量数据库，始终可⽤")]),_._v(" "),v("li",[_._v("⾼度可扩展和适应性强")]),_._v(" "),v("li",[_._v("混合搜索")]),_._v(" "),v("li",[_._v("统⼀的Lambda结构")]),_._v(" "),v("li",[_._v("受到社区⽀持，得到⾏业认可")])]),_._v(" "),v("h4",{attrs:{id:"chroma-db"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#chroma-db"}},[_._v("#")]),_._v(" Chroma db")]),_._v(" "),v("p",[_._v("Chroma的关键特性包括:")]),_._v(" "),v("ul",[v("li",[_._v("功能丰富：⽀持查询、过滤、密度估计等多种功能")]),_._v(" "),v("li",[_._v("即将添加的语⾔链（LangChain）、LlamaIndex等更多功能")]),_._v(" "),v("li",[_._v("相同的API可以在Python笔记本中运⾏，也可以扩展到集群，⽤于开发、测试和⽣产")])]),_._v(" "),v("h4",{attrs:{id:"faiss"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#faiss"}},[_._v("#")]),_._v(" Faiss")]),_._v(" "),v("p",[_._v("Faiss的关键特性包括：")]),_._v(" "),v("ul",[v("li",[v("p",[_._v("不仅返回最近的邻居，还返回第⼆近、第三近和第k近的邻居")])]),_._v(" "),v("li",[v("p",[_._v("可以同时搜索多个向量，⽽不仅仅是单个向量（批量处理）")])]),_._v(" "),v("li",[v("p",[_._v("使⽤最⼤内积搜索⽽不是最⼩欧⼏⾥得搜索")])]),_._v(" "),v("li",[v("p",[_._v("也⽀持其他距离度量，但程度较低。")])]),_._v(" "),v("li",[v("p",[_._v("返回查询位置附近指定半径内的所有元素（范围搜索）")])]),_._v(" "),v("li",[v("p",[_._v("可以将索引存储在磁盘上，⽽不仅仅是RAM中")])]),_._v(" "),v("li",[v("p",[_._v("如何选型？")])])]),_._v(" "),v("p",[_._v("需要根据项⽬的具体需求、团队的技术背景和资源情况来综合评估。")])])}),[],!1,null,null,null);v.default=i.exports}}]);