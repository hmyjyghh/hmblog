(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{565:function(t,a,s){"use strict";s.r(a);var n=s(3),r=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("参数高效微调的核心思想就是"),a("strong",[t._v("不更新整个大模型的参数，只更新一小部分新增的、轻量级的参数")]),t._v("。Adapter（适配器）正是实现这一思想的经典方法。")]),t._v(" "),a("p",[t._v("下面我为你详细讲解如何添加 Adapters，包括其"),a("strong",[t._v("核心思想、具体做法")]),t._v("以及"),a("strong",[t._v("代码示例")]),t._v("。")]),t._v(" "),a("hr"),t._v(" "),a("h3",{attrs:{id:"一、核心思想-在模型中插入-小模块"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、核心思想-在模型中插入-小模块"}},[t._v("#")]),t._v(" 一、核心思想：在模型中插入“小模块”")]),t._v(" "),a("p",[t._v("Adapter 的基本思路非常简单直观：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("冻结主干网络")]),t._v("：保持预训练大模型（如 BERT、GPT）的所有原始参数"),a("strong",[t._v("不变")]),t._v("（冻结），不进行梯度更新。")]),t._v(" "),a("li",[a("strong",[t._v("插入适配器模块")]),t._v("：在模型的某些层（通常是 Transformer 层的每个前馈神经网络之后）插入一些"),a("strong",[t._v("小型、全连接神经网络")]),t._v("。这些就是 Adapters。")]),t._v(" "),a("li",[a("strong",[t._v("只训练适配器")]),t._v("：在微调过程中，"),a("strong",[t._v("只更新这些新插入的 Adapter 模块的参数")]),t._v("，而原始模型参数保持固定。")])]),t._v(" "),a("p",[t._v("这样一来，你需要保存和训练的参数量就从整个模型的数十亿个，减少到了只有 Adapter 的几百万甚至几十万个，极大地节省了内存和计算资源。")]),t._v(" "),a("h3",{attrs:{id:"二、adapter-的具体结构与做法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、adapter-的具体结构与做法"}},[t._v("#")]),t._v(" 二、Adapter 的具体结构与做法")]),t._v(" "),a("p",[t._v("一个标准的 Adapter 模块通常是一个 "),a("strong",[t._v("“瓶颈结构”")]),t._v(" 的前馈神经网络。")]),t._v(" "),a("h4",{attrs:{id:"_1-结构分解"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-结构分解"}},[t._v("#")]),t._v(" 1. 结构分解")]),t._v(" "),a("p",[t._v("假设我们将 Adapter 插入到 Transformer 层的前馈网络之后。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("原始流程")]),t._v("："),a("code",[t._v("输入 -> Transformer Layer (包括Attention和FFN) -> 输出")])]),t._v(" "),a("li",[a("strong",[t._v("加入Adapter后流程")]),t._v("：\n"),a("ol",[a("li",[a("code",[t._v("输入 -> Transformer Layer -> 中间输出")])]),t._v(" "),a("li",[a("code",[t._v("中间输出 -> Adapter -> Adapter输出")])]),t._v(" "),a("li",[a("code",[t._v("最终输出 = 中间输出 + Adapter输出")]),t._v(" （这里有一个残差连接）")])])])]),t._v(" "),a("p",[a("strong",[t._v("Adapter 内部的“瓶颈结构”如下图所示：")])]),t._v(" "),a("p",[t._v("这个结构包含以下步骤：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("向下投影")]),t._v("：将一个高维特征（例如 "),a("code",[t._v("d_model=768")]),t._v("）通过一个全连接层投影到一个"),a("strong",[t._v("低维空间")]),t._v("（例如 "),a("code",[t._v("adapter_size=64")]),t._v("）。这是一个降维过程，大幅减少了参数量。\n"),a("ul",[a("li",[a("code",[t._v("公式：h_down = W_down * x")]),t._v(" （"),a("code",[t._v("W_down")]),t._v(" 是 "),a("code",[t._v("[d_model, adapter_size]")]),t._v(" 的矩阵）")])])]),t._v(" "),a("li",[a("strong",[t._v("非线性激活")]),t._v("：使用一个激活函数（如 ReLU 或 GELU）引入非线性。\n"),a("ul",[a("li",[a("code",[t._v("公式：h_nonlinear = GELU(h_down)")])])])]),t._v(" "),a("li",[a("strong",[t._v("向上投影")]),t._v("：将低维特征再投影回原始的高维空间。\n"),a("ul",[a("li",[a("code",[t._v("公式：h_up = W_up * h_nonlinear")]),t._v(" （"),a("code",[t._v("W_up")]),t._v(" 是 "),a("code",[t._v("[adapter_size, d_model]")]),t._v(" 的矩阵）")])])]),t._v(" "),a("li",[a("strong",[t._v("残差连接")]),t._v("：将 Adapter 的输出与原始输入相加。这确保了即使 Adapter 被随机初始化，也不会干扰模型原有的知识，保证了训练的稳定性。\n"),a("ul",[a("li",[a("code",[t._v("公式：output = x + h_up")])])])])]),t._v(" "),a("h4",{attrs:{id:"_2-插入位置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-插入位置"}},[t._v("#")]),t._v(" 2. 插入位置")]),t._v(" "),a("p",[t._v("通常，Adapter 被插入到 Transformer 层的两个核心子层之后：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("前馈网络之后")]),t._v("：这是最经典和常见的位置。")]),t._v(" "),a("li",[a("strong",[t._v("自注意力机制之后")]),t._v("：有时也会在这里插入，或者在两个位置都插入。")])]),t._v(" "),a("h3",{attrs:{id:"三、代码实现示例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三、代码实现示例"}},[t._v("#")]),t._v(" 三、代码实现示例")]),t._v(" "),a("p",[t._v("现在，我们来看如何用代码实现。我们将使用非常流行的 "),a("code",[t._v("transformers")]),t._v(" 库和 "),a("code",[t._v("adapter-transformers")]),t._v(" 库（现在已集成到主库中）。")]),t._v(" "),a("h4",{attrs:{id:"方法一-使用-peft-库-推荐-现代且统一"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法一-使用-peft-库-推荐-现代且统一"}},[t._v("#")]),t._v(" 方法一：使用 "),a("code",[t._v("peft")]),t._v(" 库（推荐，现代且统一）")]),t._v(" "),a("p",[a("code",[t._v("peft")]),t._v(" 库是 Hugging Face 官方推荐的参数高效微调库，它统一了 Adapter、LoRA 等多种方法。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainingArguments"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AdapterConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 加载预训练模型")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModelForSequenceClassification"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-uncased"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_labels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 定义 Adapter 配置")]),t._v("\nadapter_config "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdapterConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    mh_adapter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在多头注意力后添加Adapter")]),t._v("\n    output_adapter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在前馈网络后添加Adapter")]),t._v("\n    reduction_factor"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# “瓶颈”缩小因子。如果原始维度是768，则adapter隐藏层为768/16=48")]),t._v("\n    non_linearity"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gelu"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 非线性激活函数")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 将模型转换为 PEFT 模型，并添加Adapters")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_peft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adapter_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 打印可训练参数，你会发现它只占模型总参数量的很小一部分")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_trainable_parameters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出示例：trainable params: 894,976 || all params: 109,514,762 || trainable%: 0.8172%")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5. 正常定义训练参数和训练器")]),t._v("\ntraining_args "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainingArguments"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output_dir"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./output"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    learning_rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3e-4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Adapter学习率通常可以设高一点")]),t._v("\n    per_device_train_batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_train_epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ... 假设你已经加载了数据集 ‘dataset’")]),t._v("\n\ntrainer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    args"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("training_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    eval_dataset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6. 开始训练！只有Adapter参数会被更新")]),t._v("\ntrainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 7. 保存Adapter（非常小）")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_adapter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./my_saved_adapter"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_adapter_name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 未来加载时，可以先加载原模型，再加载Adapter")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# from transformers import AutoModelForSequenceClassification")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# from peft import PeftModel")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# base_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# model = PeftModel.from_pretrained(base_model, "./my_saved_adapter")')]),t._v("\n")])])]),a("h4",{attrs:{id:"方法二-使用-adapter-transformers-库-传统方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法二-使用-adapter-transformers-库-传统方法"}},[t._v("#")]),t._v(" 方法二：使用 "),a("code",[t._v("adapter-transformers")]),t._v(" 库（传统方法）")]),t._v(" "),a("p",[t._v("这是 Adapter 方法的原生实现库，提供了更精细的控制。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdapterConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdapterType\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 加载模型")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-uncased"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 动态添加Adapter配置")]),t._v("\nadapter_config "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdapterConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pfeiffer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" reduction_factor"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# "pfeiffer"是一种经典配置')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# 3. 为模型添加一个名为"my_adapter"的Adapter')]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_adapter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_adapter"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("adapter_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 激活该Adapter用于训练")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train_adapter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_adapter"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# 5. 现在，在训练过程中，只有"my_adapter"的参数会被更新')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ... (后续的TrainingArguments和Trainer定义与上面类似)")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6. 保存Adapter")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_adapter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./saved_adapter"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_adapter"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 7. 加载时")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_adapter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./saved_adapter"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"四、adapters-的优势与劣势"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#四、adapters-的优势与劣势"}},[t._v("#")]),t._v(" 四、Adapters 的优势与劣势")]),t._v(" "),a("h4",{attrs:{id:"优势"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#优势"}},[t._v("#")]),t._v(" 优势：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("参数高效")]),t._v("：极大地减少了可训练参数量，降低了显存需求。")]),t._v(" "),a("li",[a("strong",[t._v("模块化")]),t._v("：可以为不同任务训练不同的 Adapter，然后像插拔U盘一样在同一个基础模型上切换，实现多任务学习。")]),t._v(" "),a("li",[a("strong",[t._v("知识保留")]),t._v("：由于主干网络被冻结，模型不容易发生“灾难性遗忘”，保留了原有的强大语言能力。")]),t._v(" "),a("li",[a("strong",[t._v("训练稳定")]),t._v("：残差连接保证了训练的稳定性。")])]),t._v(" "),a("h4",{attrs:{id:"劣势"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#劣势"}},[t._v("#")]),t._v(" 劣势：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("推理速度略有下降")]),t._v("：虽然参数少了，但模型的前向传播路径增加了（多了Adapter的计算），会导致推理速度比原始模型稍慢一些。")]),t._v(" "),a("li",[a("strong",[t._v("需要设计结构")]),t._v("：需要决定插入位置、瓶颈维度等超参数。")])]),t._v(" "),a("h3",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),a("p",[t._v("添加 Adapters 进行参数高效微调，其本质是："),a("strong",[t._v("在预训练模型的特定层中，插入一个具有“瓶颈结构”的小型神经网络，并通过残差连接将其与主干网络集成。在微调时，只训练这些新增的小型网络，而冻结庞大的原始模型。")])]),t._v(" "),a("p",[t._v("这种方法让你可以在有限的硬件资源（如一张 Colab 的 T4 GPU）上，高效地微调一个巨大的模型，是当前大模型领域不可或缺的关键技术。")])])}),[],!1,null,null,null);a.default=r.exports}}]);