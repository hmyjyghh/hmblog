(window.webpackJsonp=window.webpackJsonp||[]).push([[131],{573:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"辅助理解transformer重要核心知识"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#辅助理解transformer重要核心知识"}},[t._v("#")]),t._v(" 辅助理解"),s("code",[t._v("Transformer")]),t._v("重要核心知识")]),t._v(" "),s("h2",{attrs:{id:"_1-多头注意力机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-多头注意力机制"}},[t._v("#")]),t._v(" 1. 多头注意力机制")]),t._v(" "),s("p",[s("strong",[t._v("多头注意力机制、自注意力机制、交叉注意力机制是相互正交的概念，它们之间是组合关系，而不是包含关系。")])]),t._v(" "),s("p",[t._v("我们可以从两个维度来理解它们：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("结构维度（怎么算）")]),t._v("："),s("strong",[t._v("单头 vs. 多头")])])]),t._v(" "),s("ul",[s("li",[t._v("这关心的是"),s("strong",[t._v("模型容量")]),t._v("，是用一组还是多组权重来计算注意力。")])]),t._v(" "),s("ol",{attrs:{start:"2"}},[s("li",[s("strong",[t._v("功能维度（算什么）")]),t._v("："),s("strong",[t._v("自注意力 vs. 交叉注意力")])])]),t._v(" "),s("ul",[s("li",[t._v("这关心的是"),s("strong",[t._v("数据来源")]),t._v("，是计算序列内部的关系还是两个序列之间的关系。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"图解它们的关系"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#图解它们的关系"}},[t._v("#")]),t._v(" 图解它们的关系")]),t._v(" "),s("p",[t._v("它们之间的关系更像一个组合矩阵，而不是层级结构：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}}),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("自注意力 (Self-Attention)")]),t._v(" "),s("br"),t._v(" (Q, K, V 来自"),s("strong",[t._v("同一个")]),t._v("序列)")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("交叉注意力 (Cross-Attention)")]),t._v(" "),s("br"),t._v(" (Q 来自序列 A, K, V 来自序列 B)")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("单头 (Single-Head)")]),t._v(" "),s("br"),t._v(" (一套QKV投影)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("单头自注意力")]),t._v(" "),s("br"),t._v(" (e.g., 原始Transformer论文中的基础形式)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("单头交叉注意力")]),t._v(" "),s("br"),t._v(" (e.g., 解码器层中连接编码器输出)")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("多头 (Multi-Head)")]),t._v(" "),s("br"),t._v(" (多套QKV投影)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("多头自注意力")]),t._v(" "),s("br"),t._v(" (现代模型的标准配置，如BERT, GPT)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("多头交叉注意力")]),t._v(" "),s("br"),t._v(" (现代编码器-解码器的标准配置，如T5)")])])])]),t._v(" "),s("p",[s("strong",[t._v("结论：")])]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("多头注意力")]),t._v("是一种"),s("strong",[t._v("结构")]),t._v("，它可以被应用到"),s("strong",[t._v("自注意力")]),t._v("或"),s("strong",[t._v("交叉注意力")]),t._v("的"),s("strong",[t._v("功能")]),t._v("上。")]),t._v(" "),s("li",[t._v("同样，"),s("strong",[t._v("自注意力")]),t._v("是一种"),s("strong",[t._v("功能")]),t._v("，它可以用"),s("strong",[t._v("单头")]),t._v("或"),s("strong",[t._v("多头")]),t._v("的"),s("strong",[t._v("结构")]),t._v("来实现。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"代码对比-一目了然"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#代码对比-一目了然"}},[t._v("#")]),t._v(" 代码对比：一目了然")]),t._v(" "),s("p",[t._v("让我们看代码如何体现这种组合关系。")]),t._v(" "),s("h4",{attrs:{id:"_1-多头自注意力-multi-head-self-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-多头自注意力-multi-head-self-attention"}},[t._v("#")]),t._v(" 1. 多头自注意力 (Multi-Head Self-Attention)")]),t._v(" "),s("p",[t._v("这是Transformer"),s("strong",[t._v("编码器")]),t._v("的核心。Q, K, V 都来自同一输入 "),s("code",[t._v("x")]),t._v("（比如一个句子）。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 伪代码")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadSelfAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Q, K, V 都来自同一个输入x")]),t._v("\n        output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output\n")])])]),s("h4",{attrs:{id:"_2-多头交叉注意力-multi-head-cross-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-多头交叉注意力-multi-head-cross-attention"}},[t._v("#")]),t._v(" 2. 多头交叉注意力 (Multi-Head Cross-Attention)")]),t._v(" "),s("p",[t._v("这是Transformer"),s("strong",[t._v("解码器")]),t._v("的核心。Q 来自解码器自身，而 K, V 来自编码器的输出。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 伪代码")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadCrossAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" decoder_hidden"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoder_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Q 来自解码器，K, V 来自编码器")]),t._v("\n        output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            query"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("decoder_hidden"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Q: 来自解码器")]),t._v("\n            key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("encoder_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("     "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# K: 来自编码器")]),t._v("\n            value"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("encoder_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# V: 来自编码器")]),t._v("\n            mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mask\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output\n")])])]),s("p",[t._v("请注意，这两个类都"),s("strong",[t._v("使用了同一个底层模块 "),s("code",[t._v("MultiHeadAttention")])]),t._v("。这个模块内部包含了多套线性层来生成多组的Q, K, V。功能的区别仅仅在于调用时传入的 "),s("code",[t._v("query")]),t._v(", "),s("code",[t._v("key")]),t._v(", "),s("code",[t._v("value")]),t._v(" 参数是否来自同一个源。")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"总结与类比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#总结与类比"}},[t._v("#")]),t._v(" 总结与类比")]),t._v(" "),s("p",[t._v("用一个简单的类比来理解：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("注意力机制")]),t._v("就像"),s("strong",[t._v("做饭")]),t._v("。")]),t._v(" "),s("li",[s("strong",[t._v("单头/多头")]),t._v("：就像是用"),s("strong",[t._v("一口锅")]),t._v("做饭还是用"),s("strong",[t._v("多口不同的锅")]),t._v("（炒锅、炖锅、蒸锅）同时做饭。多头（多口锅）可以同时处理不同的风味，能力更强。")]),t._v(" "),s("li",[s("strong",[t._v("自注意力/交叉注意力")]),t._v("：就像是"),s("strong",[t._v("炒一个菜内部的食材融合")]),t._v("（自注意力） vs. "),s("strong",[t._v("用一个菜的汤汁去炖另一个菜")]),t._v("（交叉注意力）。这是两种完全不同的烹饪方法。")])]),t._v(" "),s("p",[t._v("所以：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("多头自注意力")]),t._v(" = 用多口锅来炒一个菜（让食材内部更充分地融合）。")]),t._v(" "),s("li",[s("strong",[t._v("多头交叉注意力")]),t._v(" = 用多口锅，一口锅做汤底，另一口锅用这个汤底来炖别的菜。")])]),t._v(" "),s("p",[s("strong",[t._v("自注意力功能和交叉注意力功能都可以通过多头结构来实现")]),t._v("。")]),t._v(" "),s("h2",{attrs:{id:"transformer-架构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#transformer-架构"}},[t._v("#")]),t._v(" transformer 架构")]),t._v(" "),s("blockquote",[s("p",[t._v("Transformer 模型包含两个主要部分:Encoder(编码器)和Decoder (解码器）")])]),t._v(" "),s("ul",[s("li",[t._v("Encoder - Decoder 架构")]),t._v(" "),s("li",[t._v("Transformer模型包含两个主要部分:Encoder (编码器)和Decoder (解码器)")])]),t._v(" "),s("p",[t._v("是一种用于"),s("strong",[t._v("理解数据序列的神经网络架构")]),t._v(", 广泛应用于NLP任务。因为其可扩展性和处理长距离依赖的能力而闻名。")]),t._v(" "),s("h3",{attrs:{id:"架构图"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#架构图"}},[t._v("#")]),t._v(" 架构图")]),t._v(" "),s("p",[s("img",{attrs:{src:"/hmblog/images/llm/transformer/transformer.png",alt:"transformer"}})]),t._v(" "),s("h3",{attrs:{id:"encoder-编码器"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#encoder-编码器"}},[t._v("#")]),t._v(" Encoder(编码器)")]),t._v(" "),s("ol",[s("li",[t._v("步骤1: Encoder接收输入序列(例如一种语言的句子)并将其转换为连续表示序列。该序列将作为Decoder生成\n输出的基础。"),s("strong",[t._v("在标准架构图中, Encoder位于左侧模块。")])]),t._v(" "),s("li",[t._v("步骤2: 输入文本首先被转换为word embeddings, 这些向量量表示能够捕捉词汇的语义信息。")]),t._v(" "),s("li",[t._v("步骤3: 向embeddings注入positional encoding, 以标识词汇在序列中的位置信息及词问相对距离。")]),t._v(" "),s("li",[t._v("步骤4: 每个编码块包含两个核心子模块:multi-head attention机制与positionwise feed-forward network\n(原论文采用6个这样的块结构)。")]),t._v(" "),s("li",[t._v("步骤5: 需特别注意,每个子模块均采用"),s("strong",[t._v("残差连接")]),t._v("并接入normalization层,该设计能有效促进深层模型训练并稳定学习过程。")])]),t._v(" "),s("p",[t._v("上述流程描述的是单个Encoder的处理过程。Encoder最终输出的向量维度与输入维度保持一致,实际应用中可将多个Encoder进行堆叠使用。")]),t._v(" "),s("h3",{attrs:{id:"decoder-解码器"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#decoder-解码器"}},[t._v("#")]),t._v(" Decoder(解码器)")]),t._v(" "),s("ol",[s("li",[t._v("步骤1: Decoder生成输出序列(如另一种语言的句子),"),s("strong",[t._v("一次生成一个词")]),t._v("。它基于encoder输出的连续表示序列和先前生成的符号来生成下一个符号。"),s("strong",[t._v("Decoder在架构图的右侧。")])]),t._v(" "),s("li",[t._v("步骤2: Decoder也由多个相同的层组成(paper中设计为6层),但额外添加了一个多头注意力机制来关注编码器的输出。")]),t._v(" "),s("li",[t._v("步骤3: 首个子模块采用masked multi-head self-attention机制, 通过"),s("code",[t._v("masking")]),t._v("确保对特定位置的预测不会依赖后续位置信息。")]),t._v(" "),s("li",[t._v('步骤4: 架构图中可见接收Encoder输出的multi-head attenticon模块,该模块被称作"encoder-decoder attention", 其keys与values向量源于Encoder。')]),t._v(" "),s("li",[t._v('步骤5: 第二个子模块对Encoder输出执行multi-head attenticn计算-keys与values取自Encoder输出, 而queries来自前一层Decoder的输出。例如预测"am"时,"I"将作为query与key,values则来自Encoder')]),t._v(" "),s("li",[t._v('步骤6: 以生成翻译结果"Iam..."为例,Decoder首先生成"I",随后将该结果回传至Decoder网络以生成后续词汇"am"(对应架构图中Decoder模块的"Outputs"流向)。')]),t._v(" "),s("li",[t._v("步骤7: Decoder中的multi-head attention机制存在特殊约束:由于输出序列不完整,注意力仅允许关注输出\n序列中已生成的位置(通过将未来位置得分设置为负无穷实现)。")]),t._v(" "),s("li",[t._v('步骤8: Decoder内的"encoder-decoder attention"运作方式与常规attention类似, 但其query矩阵来自下层Decoder输出, keys与values向量则取自Encoder堆栈的最终输出。')])]),t._v(" "),s("h3",{attrs:{id:"最终层和softmax层"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#最终层和softmax层"}},[t._v("#")]),t._v(" 最终层和Softmax层")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("logits、概率分布 等等字眼")])]),t._v(" "),s("li",[s("p",[t._v("步骤1:Decoder堆栈的最终输出为一个浮点数向量。")])]),t._v(" "),s("li",[s("p",[t._v("步骤2:通过未端Linear层与Softmax层将该向量转换为具体单词。")])]),t._v(" "),s("li",[s("p",[t._v("步骤3:Linear层作为基础神经网络层,将Decoder输出向量转换为维度更大的logits向量。")])]),t._v(" "),s("li",[s("p",[t._v('步骤4:假设模型从训练数据中学得10,000个唯一英语单词,这些词洞汇构成模型的"输出词汇表"。')])]),t._v(" "),s("li",[s("p",[t._v("步骤5:此时logits向量将包含10,000个数值位,每个位置对)应输出词汇表中一个特定词的得分。")])]),t._v(" "),s("li",[s("p",[t._v("步骤6:Softmax层将这些得分转换为概率分布:所有概率值为正数且总和为1.0。")])]),t._v(" "),s("li",[s("p",[t._v("步骤7:选择概率最高的位置,该位置对应的单词即作为当前步骤的最终输出。")])]),t._v(" "),s("li",[s("p",[t._v("步骤8:当模型识别到"),s("code",[t._v("<end of sentence>")]),t._v("标记时, "),s("strong",[t._v("此生成过程终止")])])])]),t._v(" "),s("h3",{attrs:{id:"损失函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#损失函数"}},[t._v("#")]),t._v(" 损失函数")]),t._v(" "),s("p",[t._v("在已理解Transformer模型训练机制及端到端翻译流程的基础上, 我们现在重点解析如何通过损失函数计算，来优化模型训练:")]),t._v(" "),s("ul",[s("li",[t._v('步骤1: 模型训练从简单样本开始,例如将法语词"merci"翻译为英语词"thanks"')]),t._v(" "),s("li",[t._v('步骤2: 期望模型输出能明确指示目标词"thanks"的概率分布,但由于模型尚未充分训练,此目标暂时无法实现')]),t._v(" "),s("li",[t._v("步骤3: 模型参数(权重)经随机初始化后,未训练模型会为每个词汇生成具有随机数值的概率分布。通过"),s("strong",[t._v("反向传播")]),t._v("算法,我们将比较"),s("strong",[t._v("实际输出与期望输出")]),t._v("的差异,并据此调整模型权重使输出逐步逼近目标")]),t._v(" "),s("li",[t._v("步骤4: 两个概率分布的比较采用差值计算方法, (此为简化说明,具体实现可参考"),s("strong",[t._v("交叉熵")]),t._v("与 "),s("strong",[t._v("Kullback-Leibler散度(KL 散度)")]),t._v(" 等概念)")]),t._v(" "),s("li",[t._v('步骤5: 实际训练场景通常使用多词长句,例如输入"je suis étudiant"时期望输出"Iamastudent"。要求模型成功输出特定维度(词汇表大小,通常为30,000至50,000量级)的概率布向量')]),t._v(" "),s("li",[t._v('步骤6: 每个概率分布应在序列正确词位呈现最高概率值(如第一个分布中"i"概率最高,第二个分布中"am"概率最高,依此类推)')]),t._v(" "),s("li",[t._v("步骤7: 在足够数据集上经过充分训练后,我们期望模型输出正确翻译。注意,即使某些词在当前时间步输出可能性极低,它们仍会获得微小概率值--这是softmax函数的优势特性,有助于训练过程稳定进行")]),t._v(" "),s("li",[t._v('步骤8: 模型以逐时间步方式生成输出。若始终从概率分布中选择最高概率词项而忽略其他选项,该方法称为"贪心解码"')]),t._v(" "),s("li",[t._v('步骤9: 另一种"集束搜索"方法会'),s("strong",[t._v("保留前K个候选词")]),t._v("(例如top2),在下一时间步分别进行多次前向推理(假设首输\n出为'T或'a'), 综合评估多个位置的误差后保留最优路径。该方法始终在内存中维护两个局部假设(未完整翻译结\n果),最终返回两个翻译版本。保留的候选词数量与返回结果数量"),s("strong",[t._v("均为可调整的超参数")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);