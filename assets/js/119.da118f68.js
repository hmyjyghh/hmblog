(window.webpackJsonp=window.webpackJsonp||[]).push([[119],{563:function(_,t,v){"use strict";v.r(t);var i=v(3),n=Object(i.a)({},(function(){var _=this,t=_._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[t("h3",{attrs:{id:"_1-什么是prefix-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-什么是prefix-tuning"}},[_._v("#")]),_._v(" 1. 什么是Prefix tuning?")]),_._v(" "),t("ul",[t("li",[_._v("核心思想：在每一层之前添加可学习的“前缀”。")]),_._v(" "),t("li",[_._v("相较于"),t("code",[_._v("Prompt-Tuning")]),_._v("和"),t("code",[_._v("P-tuning")]),_._v(",\n"),t("ul",[t("li",[t("code",[_._v("Prefix-Tuning")]),_._v("不再将Prompt加在输入的Embedding层,")]),_._v(" "),t("li",[_._v("而是将其作为可学习的前缀, "),t("strong",[_._v("放置在Transsformer模型中的每一层中")]),_._v(", 具体表现形式为past_key_values。")])])])]),_._v(" "),t("p",[t("img",{attrs:{src:"/hmblog/images/fine-tuning/Prefix-Tuning.png",alt:"Prefix Tuning"}})]),_._v(" "),t("ul",[t("li",[_._v("图中的Prefix Encoder 不会跟 右边的 Embedding 拼起来，而是会放到Transformer Blocks 层里，参与计算，")]),_._v(" "),t("li",[_._v("它是通过past_key_values 形式放进去的")])]),_._v(" "),t("p",[t("img",{attrs:{src:"/hmblog/images/fine-tuning/past_key_values.png",alt:"past_key_values"}})]),_._v(" "),t("ul",[t("li",[t("strong",[_._v("普通Prompt-Tuning")]),_._v("：只在模型的"),t("strong",[_._v("输入嵌入层（Input Embedding Layer）")]),_._v(" 添加可训练的提示向量。这相当于在对话开始时给模型一个总体的指令。")]),_._v(" "),t("li",[t("strong",[_._v("Prefix-Tuning")]),_._v("：不仅在输入层，而是在"),t("strong",[_._v("模型的每一层（或某几层）的激活（activation）之前")]),_._v("，都添加一组可训练的前缀向量。这相当于在模型思考的每一个步骤、每一个阶段都不断地进行引导和提醒，确保它不偏离轨道。")])]),_._v(" "),t("h3",{attrs:{id:"_2-介绍一下lora微调"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-介绍一下lora微调"}},[_._v("#")]),_._v(" 2. 介绍一下LORA微调")]),_._v(" "),t("ul",[t("li",[_._v("在原始权重旁边，新起一条分支")]),_._v(" "),t("li",[_._v("这个分支的话，是由两个小矩阵组成, "),t("code",[_._v("LoRA_A")]),_._v("  和  "),t("code",[_._v("LoRA_B")])]),_._v(" "),t("li",[_._v("更新的时候只更新这两个小矩阵")]),_._v(" "),t("li",[_._v("训练完成之后，再把它合并回去，这就是LoRA")])]),_._v(" "),t("p",[t("img",{attrs:{src:"/hmblog/images/fine-tuning/Lora1.png",alt:"LoRA 的思想"}})]),_._v(" "),t("ul",[t("li",[_._v("LoRA 原理")])]),_._v(" "),t("p",[t("img",{attrs:{src:"/hmblog/images/fine-tuning/Lora2.png",alt:"Lora"}})]),_._v(" "),t("h3",{attrs:{id:"_3-什么是稀疏微调"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-什么是稀疏微调"}},[_._v("#")]),_._v(" 3. 什么是稀疏微调?")]),_._v(" "),t("ul",[t("li",[_._v("稀疏微调在第二阶段将稀疏性引入到模型权重中。稀疏性是指参数值为零, 不需要任何计算。权重以结构化方式修剪,将某些块或行/列设置为零,而不是随机删除各个权重。这呆持了可以有效利用的常规稀疏模式。")]),_._v(" "),t("li",[_._v("理想情况下, 可以在"),t("strong",[_._v("对模型精度影响最小的情况下引入高稀疏性")]),_._v("(例如75-90%)。由此产生的稀疏模型需要更少的计算, 从而实现更快、更高效的推理。")])]),_._v(" "),t("p",[t("img",{attrs:{src:"/hmblog/images/fine-tuning/xishu-tuning.jpg",alt:"Lora"}})]),_._v(" "),t("h3",{attrs:{id:"_4-微调阶段样本量规模增大导致的oom错误-怎么解决"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-微调阶段样本量规模增大导致的oom错误-怎么解决"}},[_._v("#")]),_._v(" 4. 微调阶段样本量规模增大导致的OOM错误, 怎么解决?")]),_._v(" "),t("p",[_._v("当样本量规模增大时,可能会出现OOM(Out of Memory)错误, "),t("strong",[_._v("这是因为模型需要更多的内存来存储和处理数据")]),_._v("。为了解决这个问题,可以尝试以下方法:")]),_._v(" "),t("ul",[t("li",[_._v("减小批量大小: 这可以减少每次训练需要处理的数据量, 从而减少内存使用。")]),_._v(" "),t("li",[_._v("使用梯度累积: 这种方法可以在"),t("strong",[_._v("不减小批量大小的情况下, 减少内存使用")]),_._v("。")]),_._v(" "),t("li",[_._v("使用模型并行: 这种方法可以将模型的不同部分放在不同的的设备上进行训练,从而减少每个设备需要的内存。")])]),_._v(" "),t("h3",{attrs:{id:"_5-attention计算复杂度以及如何改进"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-attention计算复杂度以及如何改进"}},[_._v("#")]),_._v(" 5. Attention计算复杂度以及如何改进")]),_._v(" "),t("blockquote",[t("p",[_._v("在标准的Transformer中, Attention计算的时间复杂度为O(N^2), 其中N是输入序列的长度。为了降低计算复杂度,可以采用以下几种方法:")])]),_._v(" "),t("ul",[t("li",[_._v("使用自注意力机制, 减少计算复杂度。"),t("strong",[_._v("自注意力机制不需要计算输入序列之间的交叉关系")]),_._v(", 而是计算每个输入向量与自身之间的关系, 从而减少计算量。")]),_._v(" "),t("li",[_._v("使用局部注意力机制, 只计算输入序列中与当前位置相关的的子序列的交互, 从而降低计算复杂度。")]),_._v(" "),t("li",[_._v("采用基于近似的方法, 例如使用随机化和采样等方法来近似计算, 从而降低计算复杂度。")]),_._v(" "),t("li",[_._v("使用压缩注意力机制,通过将输入向量映射到低维空间来减少计算量, 例如使用哈希注意力机制和低秩注意力机制等。")])])])}),[],!1,null,null,null);t.default=n.exports}}]);