(window.webpackJsonp=window.webpackJsonp||[]).push([[51],{533:function(a,t,e){"use strict";e.r(t);var r=e(3),i=Object(r.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"相关英语词汇"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#相关英语词汇"}},[a._v("#")]),a._v(" 相关英语词汇")]),a._v(" "),t("h2",{attrs:{id:"transformer-相关英语词汇"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#transformer-相关英语词汇"}},[a._v("#")]),a._v(" transformer 相关英语词汇")]),a._v(" "),t("ol",[t("li",[a._v("Multi-H: Multi-Head Attention: 多头注意力")]),a._v(" "),t("li",[a._v("FFN: Feed Forward Network：前馈神经网络")]),a._v(" "),t("li",[a._v("PE: Positional Encoding: 位置编码")]),a._v(" "),t("li",[a._v("LN: Layer Normalization: 层归一化")]),a._v(" "),t("li",[a._v("RC: Residual Connection: 残差连接")]),a._v(" "),t("li",[a._v("Add & Norm: 残差和标准化")]),a._v(" "),t("li",[a._v("Softmax: softmax 函数")]),a._v(" "),t("li",[a._v("Dropout: 随机失活")]),a._v(" "),t("li",[a._v("缩放点积注意力机制：Scaled Dot-Product Attention")]),a._v(" "),t("li",[a._v("掩码机制（Masked Attention）")]),a._v(" "),t("li",[a._v("交叉注意力（Cross-Attention）")]),a._v(" "),t("li",[a._v("交叉熵损失（Cross-Entropy Loss）")]),a._v(" "),t("li",[a._v("Pre-LN: Pre-Layer Normalization")]),a._v(" "),t("li",[a._v("Post-LN: Post-Layer Normalization")])]),a._v(" "),t("h2",{attrs:{id:"其他相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他相关"}},[a._v("#")]),a._v(" 其他相关")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("RNN：循环神经网络(Recurrent Neural Network)")])]),a._v(" "),t("li",[t("p",[a._v("LSTM：长短期记忆网络（LSTM）架构 or 长短期记忆网络（Long Short-Term Memory）")])]),a._v(" "),t("li",[t("p",[a._v("NLP，Natural Language Processing 自然语言处理")])]),a._v(" "),t("li",[t("p",[a._v("NMT，Neural Machine Translation 神经机器翻译")])]),a._v(" "),t("li",[t("p",[a._v("BERT，Bidirectional Encoder Representations from Transformers 双向编码器表示从 Transformers")])]),a._v(" "),t("li",[t("p",[a._v("GPT，Generative Pre-trained Transformer 生成式预训练 Transformer")])]),a._v(" "),t("li",[t("p",[a._v("缩放点积注意力机制：Scaled Dot-Product Attention")])]),a._v(" "),t("li",[t("p",[a._v("卷积神经网络（CNN）")])]),a._v(" "),t("li",[t("p",[a._v("卷积核（Kernel）")])]),a._v(" "),t("li",[t("p",[a._v("RLHF：强化学习从人类反馈中学习")])])]),a._v(" "),t("h2",{attrs:{id:"其他"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他"}},[a._v("#")]),a._v(" 其他")]),a._v(" "),t("h3",{attrs:{id:"_1-epoch-和-epochs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-epoch-和-epochs"}},[a._v("#")]),a._v(" 1. epoch 和 epochs")]),a._v(" "),t("h3",{attrs:{id:"_2-batch-和-batches"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-batch-和-batches"}},[a._v("#")]),a._v(" 2. batch 和 batches")]),a._v(" "),t("ol",[t("li",[a._v("kernel: 内核 jupyter 内核")])]),a._v(" "),t("h2",{attrs:{id:"标记"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#标记"}},[a._v("#")]),a._v(" 标记")]),a._v(" "),t("ol",[t("li",[a._v("Start of Sequence： "),t("SOS")],1),a._v(" "),t("li",[a._v("Beginning of Sequence: "),t("BOS")],1),a._v(" "),t("li",[a._v("填充掩码（Padding Mask）")]),a._v(" "),t("li",[a._v("未来信息掩码（Look-ahead Mask）")])]),a._v(" "),t("h2",{attrs:{id:"rag-相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rag-相关"}},[a._v("#")]),a._v(" RAG 相关")]),a._v(" "),t("ol",[t("li",[a._v("RAG(Retrieval Augmented Generation)-检索增强⽣成")])]),a._v(" "),t("blockquote",[t("p",[a._v("将⼤模型应⽤于实际业务场景时会发现，通⽤的基础⼤模型基本⽆法满⾜我们的实际业务需求，主要有以下⼏⽅⾯原因")])]),a._v(" "),t("ul",[t("li",[a._v("LLM的知识不是实时的，不具备知识更新")]),a._v(" "),t("li",[a._v("LLM可能不知道你私有的领域/业务知识")]),a._v(" "),t("li",[a._v("LLM有时会在回答中⽣成看似合理但实际上是错误的信息")])]),a._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[a._v("Chain of Thought: COT 思维链")]),a._v(" "),t("li")]),a._v(" "),t("h2",{attrs:{id:"langchain-框架"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#langchain-框架"}},[a._v("#")]),a._v(" Langchain 框架")]),a._v(" "),t("ol",[t("li",[t("code",[a._v("LCEL")]),a._v(" 是 LangChain Expression Language 的缩写，即 LangChain 表达式语言。")])]),a._v(" "),t("h2",{attrs:{id:"rag-相关-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rag-相关-2"}},[a._v("#")]),a._v(" RAG 相关")]),a._v(" "),t("ol",[t("li",[a._v("Reciprocal Rank Fusion (RRF): 使用 Reciprocal Rank Fusion (RRF) 算法合并所有检索结果, 结果融合")])]),a._v(" "),t("p",[a._v("Reciprocal Rank Fusion (RRF) : 倒数秩序融合")]),a._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[a._v("Self-Reflective Retrieval-Augmented Generation(Self RAG)")])]),a._v(" "),t("h3",{attrs:{id:"核心思想"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#核心思想"}},[a._v("#")]),a._v(" 核心思想")]),a._v(" "),t("p",[a._v("RRF 的核心思想是：一个文档在多个排名列表中的位置都很靠前，那么它本身就应该是一个非常相关、非常重要的文档。")]),a._v(" "),t("div",{staticClass:"language-Text extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("Reciprocal_rank_fusion that takes multiple lists of ranked documents \nand an optional parameter k used in the RRF formula\n\nReciprocal_rank_fusion，接受多个排名文档列表, 以及RRF公式中使用的可选参数k\n")])])]),t("ol",[t("li",[a._v("RAG Fusion（RAG 融合）")])]),a._v(" "),t("h2",{attrs:{id:"mcp"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mcp"}},[a._v("#")]),a._v(" MCP")]),a._v(" "),t("ol",[t("li",[a._v("MCP (Model Context Protocol)")])]),a._v(" "),t("h2",{attrs:{id:"其他-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他-2"}},[a._v("#")]),a._v(" 其他")]),a._v(" "),t("ol",[t("li",[a._v("Hallucination 幻觉")]),a._v(" "),t("li",[a._v("SELF-RAG（Self-Reflective Retrieval-Augmented Generation）")])]),a._v(" "),t("h2",{attrs:{id:"fine-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning"}},[a._v("#")]),a._v(" Fine Tuning")]),a._v(" "),t("ol",[t("li",[a._v("PEFT 的全称是 Parameter-Efficient Fine-Tuning")]),a._v(" "),t("li",[a._v("IA3 的全称是 Infused Adapter by Inhibiting and Amplifying Inner Activations, 3个I， 3个A，所以叫IA3")]),a._v(" "),t("li",[a._v("稀疏微调（Sparse Fine-tuning）")])]),a._v(" "),t("h2",{attrs:{id:"评估指标相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#评估指标相关"}},[a._v("#")]),a._v(" 评估指标相关")]),a._v(" "),t("h3",{attrs:{id:"evaluate"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluate"}},[a._v("#")]),a._v(" Evaluate")]),a._v(" "),t("ol",[t("li",[a._v("accuracy 准确性、准确度")]),a._v(" "),t("li",[a._v("f1")]),a._v(" "),t("li",[a._v("metric: 指标")]),a._v(" "),t("li",[a._v("exact_match: 精确匹配")])]),a._v(" "),t("h2",{attrs:{id:"大模型相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型相关"}},[a._v("#")]),a._v(" 大模型相关")]),a._v(" "),t("ol",[t("li",[a._v('在自然语言处理中，"logits" 通常指模型输出的原始预测值（未归一化的概率分数）。')]),a._v(" "),t("li",[a._v("Continuous Batching（连续批处理）")])]),a._v(" "),t("h2",{attrs:{id:"向量数据库"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#向量数据库"}},[a._v("#")]),a._v(" 向量数据库")]),a._v(" "),t("ol",[t("li",[a._v("近似最近邻(ANN: Approximate Nearest Neighbor)")]),a._v(" "),t("li",[a._v("相似性搜索(Similarity Search)")])])])}),[],!1,null,null,null);t.default=i.exports}}]);