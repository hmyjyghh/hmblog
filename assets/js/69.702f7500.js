(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{513:function(_,v,t){"use strict";t.r(v);var l=t(3),a=Object(l.a)({},(function(){var _=this,v=_._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("h3",{attrs:{id:"_1-大语言模型是什么"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-大语言模型是什么"}},[_._v("#")]),_._v(" 1. 大语言模型是什么？")]),_._v(" "),v("p",[_._v("LLM是高级的自然语言处理(NLP)模型, 经过海量数据集的训练, 能够"),v("strong",[_._v("理解和生成")]),_._v("类似人类语言的文本。")]),_._v(" "),v("ul",[v("li",[_._v("一种能够"),v("strong",[_._v("识别、预测和生成人类语言")]),_._v("的机器学习算法。")]),_._v(" "),v("li",[_._v("预训练规模达到PB级的文本数据集, 通常包含数十亿到数千亿个参数的超大模型。")]),_._v(" "),v("li",[_._v("LLM通常先在大规模数据集上进行预训练, 然后再针对特定任务进行微调。")])]),_._v(" "),v("h3",{attrs:{id:"_2-llm-是如何训练的"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-llm-是如何训练的"}},[_._v("#")]),_._v(" 2. LLM 是如何训练的?")]),_._v(" "),v("ul",[v("li",[_._v("预训练")]),_._v(" "),v("li",[_._v("SFT")]),_._v(" "),v("li",[_._v("强化学习(RL)")])]),_._v(" "),v("h3",{attrs:{id:"_3-llms-中token-的概念"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-llms-中token-的概念"}},[_._v("#")]),_._v(" 3. LLMs 中token 的概念")]),_._v(" "),v("blockquote",[v("p",[_._v("大模型中的token，"),v("strong",[_._v("是指文本的最小处理单位")]),_._v("。")])]),_._v(" "),v("ol",[v("li",[_._v("token是模型"),v("strong",[_._v("读取或生成的文本单位")]),_._v("。")]),_._v(" "),v("li",[_._v("Token可以是单词、字符 or 子词 等")])]),_._v(" "),v("p",[_._v("通常情况下, 一个token可以是一个"),v("strong",[_._v("单词")]),_._v("、"),v("strong",[_._v("一个标点符号")]),_._v("、"),v("strong",[_._v("一个数字")]),_._v(", 或者是其他更小的文本单元, 如子词或字符。")]),_._v(" "),v("ul",[v("li",[v("p",[v("strong",[_._v("通过对文本做成一个一个的token, LLM模型能够更好地理解和处理语言")]),_._v(", 从而实现任务如文本生成、机器\n翻译、文本分类等。")])]),_._v(" "),v("li",[v("p",[_._v('因此, 现在主流的大模型都会自带一个tokenizer, 也就是自动将输入文本解析成一个一个的token, 然后做编码(就是查字典, 转换成数字),  作为大模型真正的"输入"。')])])]),_._v(" "),v("h3",{attrs:{id:"_4-语言模型中常见的几种token-分词的方法"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-语言模型中常见的几种token-分词的方法"}},[_._v("#")]),_._v(" 4. 语言模型中常见的几种token 分词的方法")]),_._v(" "),v("ul",[v("li",[_._v("基于单词的")]),_._v(" "),v("li",[_._v("基于字符的")]),_._v(" "),v("li",[_._v("基于子词的")])]),_._v(" "),v("h3",{attrs:{id:"_5-温度系数"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_5-温度系数"}},[_._v("#")]),_._v(" 5. 温度系数")]),_._v(" "),v("ul",[v("li",[_._v("是 0~1 之间的数值")]),_._v(" "),v("li",[_._v("温度系数越小，模型回答越稳定，反之，模型回答越发散。")])]),_._v(" "),v("h3",{attrs:{id:"_6-输出token的不同解码策略有哪些"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_6-输出token的不同解码策略有哪些"}},[_._v("#")]),_._v(" 6. 输出token的不同解码策略有哪些?")]),_._v(" "),v("ul",[v("li",[_._v("Greedy Search")]),_._v(" "),v("li",[_._v("Beam Search")]),_._v(" "),v("li",[_._v("Top-K Sampling")]),_._v(" "),v("li",[_._v("Top-p (Nucleus) Sampling")])]),_._v(" "),v("h3",{attrs:{id:"_7-llms-中复读机问题"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_7-llms-中复读机问题"}},[_._v("#")]),_._v(" 7. LLMs 中复读机问题")]),_._v(" "),v("blockquote",[v("p",[_._v("是指大型语言模型在生成文本时, 出现重复、冗长或循环的内容。")])]),_._v(" "),v("ol",[v("li",[_._v("如何缓解 LLM 复读机问题？")])]),_._v(" "),v("ul",[v("li",[_._v("多样性训练数据: "),v("strong",[_._v("在训练阶段, 尽量使用多样性的语料库来训练模型")]),_._v(", 避免数据偏差和重复文本的问题。")]),_._v(" "),v("li",[_._v("引入噪声: 在生成文本时, 可以引入一些随机性或噪声, eg: 通过采样不同的词或短语, 或者引入随机的变换操作, 以增加生成文本的多样性。")]),_._v(" "),v("li",[_._v("温度参数调整: 温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值, 可以控制生成文本的独创性和多样性, 从而减少复读机问题的出现。")]),_._v(" "),v("li",[_._v("后处理和过滤: 对生成的文本进行后处理和过滤, 去除重复的句子子或短语, 以提高生成文本的质量和多样性。")]),_._v(" "),v("li",[_._v("人工干预和控制: 对于关键任务或敏感场景, 可以引入人工干预和控制机制, 对生成的文本进行审查和筛选, 确保生成结果的准确性和多样性。")])]),_._v(" "),v("h3",{attrs:{id:"_8-大语言模型中的幻觉"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_8-大语言模型中的幻觉"}},[_._v("#")]),_._v(" 8. 大语言模型中的幻觉？")]),_._v(" "),v("p",[v("strong",[_._v("有2种形式")])]),_._v(" "),v("ul",[v("li",[_._v("内在幻觉")]),_._v(" "),v("li",[_._v("篡改已有信息")]),_._v(" "),v("li",[_._v("外在幻觉")]),_._v(" "),v("li",[_._v("生成错误的信息，捏造事实。")])]),_._v(" "),v("h3",{attrs:{id:"_9-过拟合和欠拟合"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_9-过拟合和欠拟合"}},[_._v("#")]),_._v(" 9. 过拟合和欠拟合")]),_._v(" "),v("ol",[v("li",[_._v("过拟合是指"),v("strong",[_._v("模型在训练数据上表现非常好")]),_._v(", "),v("strong",[_._v("但在未见过的测试数据或新数据上表现较差")]),_._v("。")])]),_._v(" "),v("ul",[v("li",[_._v("这种情况发生，是因为模型过于复杂, 学习了训练数据中的噪声或异常点, 而没有捕捉到数据的真实模式。")]),_._v(" "),v("li",[_._v("过拟合模型往往对训练数据中的噪声过于敏感, 导致泛化能力下降。")])]),_._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[_._v("欠拟合: "),v("strong",[_._v("是模型对训练数据和测试数据的拟合都较差")]),_._v(", 这通常是因为模型过于简单, 无法捕获数据的复杂结构。")])]),_._v(" "),v("ul",[v("li",[_._v("欠拟合模型在训练和测试数据上的表现都低于预期, 说明模型还需要学习更多的特征或模式。")])]),_._v(" "),v("blockquote",[v("p",[_._v("如何避免过拟合和欠拟合?")])]),_._v(" "),v("ol",[v("li",[_._v("增加数据量: 获取更多的训练数据可以提供更多的样本信息, 帮助模型更好地学习数据的真实模式, 减少过拟合和欠拟合的风险。")]),_._v(" "),v("li",[_._v("特征选择和降维: 选择对预测目标最有影响的特征,减少冗余特征, 可以降低模型复杂度, 防止过拟合。")]),_._v(" "),v("li",[_._v("正则化: 如L1和L2正则化,通过添加惩罚项来限制模型参数女的大小, 防止过拟合。")]),_._v(" "),v("li",[_._v("Dropout: 在神经网络中, 随机关闭部分神经元, 避免模型过度依赖某些特征, 增强模型的泛化能力。")]),_._v(" "),v("li",[_._v("早停法: 在验证集上监控模型性能, 当验证集性能开始下降时停止训练, 防止过拟合")]),_._v(" "),v("li",[_._v("模型复杂度调整: 增加或减少模型的复杂度(如神经网络的层数或宽度), 找到适合数据的复杂度, 防止欠拟合和过拟合。")]),_._v(" "),v("li",[_._v("交叉验证:通过使用K折交叉验证来评估模型性能,更准确地评估模型泛化能力,帮助避免过拟合。")]),_._v(" "),v("li",[_._v("集成学习:结合多个模型的预测,如随机森林、梯度提升机等,提高模型的泛化性能")]),_._v(" "),v("li",[_._v("早始化:使用合适的初始化方法,如Xavier初始化或He初始台化,帮助模型在训练初期就能学习到有意义的特征。")]),_._v(" "),v("li",[_._v("优化算法和学习率: 选择合适的优化算法(如Adam、RMSprop等)和学习率调整策略,以更有效地学习数据的模式。")])]),_._v(" "),v("h3",{attrs:{id:"_10-什么是dropout"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_10-什么是dropout"}},[_._v("#")]),_._v(" 10. 什么是Dropout?")]),_._v(" "),v("ul",[v("li",[_._v("一种正则化技术，减轻模型过拟合")]),_._v(" "),v("li",[_._v("在神经网络中, 随机关闭部分神经元, 避免模型过度依赖某些特征, 增强模型的泛化能力。")])]),_._v(" "),v("h3",{attrs:{id:"_11-llm-中-因果语言建模-clm-和-屏蔽语言建模-mlm"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_11-llm-中-因果语言建模-clm-和-屏蔽语言建模-mlm"}},[_._v("#")]),_._v(" 11. LLM 中 因果语言建模(CLM) 和 屏蔽语言建模(MLM)")])])}),[],!1,null,null,null);v.default=a.exports}}]);