(window.webpackJsonp=window.webpackJsonp||[]).push([[95],{536:function(a,t,v){"use strict";v.r(t);var _=v(3),i=Object(_.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"大模型相关英语词汇"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型相关英语词汇"}},[a._v("#")]),a._v(" 大模型相关英语词汇")]),a._v(" "),t("p",[t("strong",[a._v("NLP 包括 NLU 和 NLG。")])]),a._v(" "),t("ol",[t("li",[a._v("NLU：Natural Language Understanding，自然语言理解")]),a._v(" "),t("li",[a._v("NLP：Natural Language Processing, 自然语言处理。这是一个更广泛的领域，涵盖了所有让计算机处理、理解、生成人类语言的技术。NLP 包括 NLU 和 NLG。")]),a._v(" "),t("li",[a._v("NLG: Natural Language Generation，自然语言生成。")])]),a._v(" "),t("p",[a._v("简单来说，如果 NLU 是语言的“输入”和理解，那么 NLG 就是语言的“输出”和生成。它的目标是让机器用人类自然语言的形式，生成有意义、连贯且符合语境的文本。")]),a._v(" "),t("ol",{attrs:{start:"4"}},[t("li",[a._v("Hallucination: 幻觉")]),a._v(" "),t("li",[a._v("Stop Sequence： 停止序列，个人觉得可以叫： 停止词")])]),a._v(" "),t("h2",{attrs:{id:"transformer-相关英语词汇"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#transformer-相关英语词汇"}},[a._v("#")]),a._v(" transformer 相关英语词汇")]),a._v(" "),t("ol",[t("li",[a._v("Multi-H: Multi-Head Attention: 多头注意力")]),a._v(" "),t("li",[a._v("FFN: Feed Forward Network：前馈神经网络")]),a._v(" "),t("li",[a._v("PE: Positional Encoding: 位置编码")]),a._v(" "),t("li",[a._v("LN: Layer Normalization: 层归一化")]),a._v(" "),t("li",[a._v("RC: Residual Connection: 残差连接")]),a._v(" "),t("li",[a._v("Add & Norm: 残差和标准化")]),a._v(" "),t("li",[a._v("Softmax: softmax 函数")]),a._v(" "),t("li",[a._v("Dropout: 随机失活")]),a._v(" "),t("li",[a._v("缩放点积注意力机制：Scaled Dot-Product Attention")]),a._v(" "),t("li",[a._v("掩码机制（Masked Attention）")]),a._v(" "),t("li",[a._v("交叉注意力（Cross-Attention）")]),a._v(" "),t("li",[a._v("交叉熵损失（Cross-Entropy Loss）")]),a._v(" "),t("li",[a._v("Pre-LN: Pre-Layer Normalization")]),a._v(" "),t("li",[a._v("Post-LN: Post-Layer Normalization")]),a._v(" "),t("li",[a._v("POPE: Rotary Position Embedding, 旋转位置编码")]),a._v(" "),t("li",[a._v("KL: Kullback-Leibler 散度, KL 散度")])]),a._v(" "),t("h2",{attrs:{id:"其他相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他相关"}},[a._v("#")]),a._v(" 其他相关")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("RNN：循环神经网络(Recurrent Neural Network)")])]),a._v(" "),t("li",[t("p",[a._v("LSTM：长短期记忆网络（LSTM）架构 or 长短期记忆网络（Long Short-Term Memory）")])]),a._v(" "),t("li",[t("p",[a._v("NMT，Neural Machine Translation 神经机器翻译")])]),a._v(" "),t("li",[t("p",[a._v("BERT，Bidirectional Encoder Representations from Transformers 双向编码器表示从 Transformers")])]),a._v(" "),t("li",[t("p",[a._v("GPT，Generative Pre-trained Transformer 生成式预训练 Transformer")])]),a._v(" "),t("li",[t("p",[a._v("缩放点积注意力机制：Scaled Dot-Product Attention")])]),a._v(" "),t("li",[t("p",[a._v("卷积神经网络（CNN）")])]),a._v(" "),t("li",[t("p",[a._v("卷积核（Kernel）")])]),a._v(" "),t("li",[t("p",[a._v("RLHF：基于人类反馈的强化学习")])])]),a._v(" "),t("h2",{attrs:{id:"其他"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他"}},[a._v("#")]),a._v(" 其他")]),a._v(" "),t("h2",{attrs:{id:"项目相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#项目相关"}},[a._v("#")]),a._v(" 项目相关：")]),a._v(" "),t("ol",[t("li",[a._v("Arbitration history：仲裁历史")]),a._v(" "),t("li",[a._v("Arbitration：仲裁")]),a._v(" "),t("li",[a._v("confidence：信心、置信度、得分值")]),a._v(" "),t("li",[a._v("Response time percentiles (approximated): 响应时间百分位数（近似值）")])]),a._v(" "),t("h3",{attrs:{id:"_1-epoch-和-epochs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-epoch-和-epochs"}},[a._v("#")]),a._v(" 1. epoch 和 epochs")]),a._v(" "),t("h3",{attrs:{id:"_2-batch-和-batches"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-batch-和-batches"}},[a._v("#")]),a._v(" 2. batch 和 batches")]),a._v(" "),t("ol",[t("li",[a._v("kernel: 内核 jupyter 内核")]),a._v(" "),t("li",[a._v("Thought: 思想")]),a._v(" "),t("li",[a._v("COT：Chain-of-Thought, 思维链")]),a._v(" "),t("li",[a._v("In-Context Learning (ICL): 一段提示词，让模型根据提示词里的指令，去生成回答")])]),a._v(" "),t("h2",{attrs:{id:"标记"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#标记"}},[a._v("#")]),a._v(" 标记")]),a._v(" "),t("ol",[t("li",[a._v("Start of Sequence： "),t("code",[a._v("<SOS>")])]),a._v(" "),t("li",[a._v("Beginning of Sequence: "),t("code",[a._v("<BOS>")])]),a._v(" "),t("li",[a._v("填充掩码（Padding Mask）")]),a._v(" "),t("li",[a._v("未来信息掩码（Look-ahead Mask）")])]),a._v(" "),t("h2",{attrs:{id:"rag-相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rag-相关"}},[a._v("#")]),a._v(" RAG 相关")]),a._v(" "),t("ol",[t("li",[a._v("RAG(Retrieval Augmented Generation)-检索增强⽣成")])]),a._v(" "),t("blockquote",[t("p",[a._v("将⼤模型应⽤于实际业务场景时会发现，通⽤的基础⼤模型基本⽆法满⾜我们的实际业务需求，主要有以下⼏⽅⾯原因")])]),a._v(" "),t("ul",[t("li",[a._v("LLM的知识不是实时的，不具备知识更新")]),a._v(" "),t("li",[a._v("LLM可能不知道你私有的领域/业务知识")]),a._v(" "),t("li",[a._v("LLM有时会在回答中⽣成看似合理但实际上是错误的信息")])]),a._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[a._v("Chain of Thought: COT 思维链")]),a._v(" "),t("li")]),a._v(" "),t("h2",{attrs:{id:"langchain-框架"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#langchain-框架"}},[a._v("#")]),a._v(" Langchain 框架")]),a._v(" "),t("ol",[t("li",[t("code",[a._v("LCEL")]),a._v(" 是 LangChain Expression Language 的缩写，即 LangChain 表达式语言。")])]),a._v(" "),t("h2",{attrs:{id:"rag-相关-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rag-相关-2"}},[a._v("#")]),a._v(" RAG 相关")]),a._v(" "),t("ol",[t("li",[a._v("Reciprocal Rank Fusion (RRF): 倒数排名融合")]),a._v(" "),t("li",[a._v("Self-Reflective Retrieval-Augmented Generation(Self RAG)")]),a._v(" "),t("li",[a._v("RAG Fusion（RAG 融合）")])]),a._v(" "),t("h2",{attrs:{id:"检索相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#检索相关"}},[a._v("#")]),a._v(" 检索相关")]),a._v(" "),t("ol",[t("li",[a._v("Vector Search Results：向量搜索结果")]),a._v(" "),t("li",[a._v("Lexical Search results：关键字搜索")]),a._v(" "),t("li",[a._v("Lexical：词汇")]),a._v(" "),t("li",[a._v("Sparse: 稀疏，BGE-M3: Sparse 召回")]),a._v(" "),t("li",[a._v("Dense: 稠密，Qwen3-Embedding 稠密召回")])]),a._v(" "),t("h2",{attrs:{id:"mcp"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mcp"}},[a._v("#")]),a._v(" MCP")]),a._v(" "),t("ol",[t("li",[a._v("MCP (Model Context Protocol)")])]),a._v(" "),t("h2",{attrs:{id:"其他-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他-2"}},[a._v("#")]),a._v(" 其他")]),a._v(" "),t("ol",[t("li",[a._v("Hallucination 幻觉")]),a._v(" "),t("li",[a._v("SELF-RAG（Self-Reflective Retrieval-Augmented Generation）")])]),a._v(" "),t("h2",{attrs:{id:"fine-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning"}},[a._v("#")]),a._v(" Fine Tuning")]),a._v(" "),t("ol",[t("li",[a._v("PEFT 的全称是 Parameter-Efficient Fine-Tuning")]),a._v(" "),t("li",[a._v("IA3 的全称是 Infused Adapter by Inhibiting and Amplifying Inner Activations, 3个I， 3个A，所以叫IA3")]),a._v(" "),t("li",[a._v("稀疏微调（Sparse Fine-tuning）")])]),a._v(" "),t("h2",{attrs:{id:"评估指标相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#评估指标相关"}},[a._v("#")]),a._v(" 评估指标相关")]),a._v(" "),t("h3",{attrs:{id:"rag-检索系统"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rag-检索系统"}},[a._v("#")]),a._v(" RAG 检索系统")]),a._v(" "),t("ol",[t("li",[a._v("nDCG (Normalized Discounted Cumulative Gain)：归一化折损累积增益, CG: 累积增益")])]),a._v(" "),t("p",[t("strong",[a._v("RAG场景中的核心作用")]),a._v("：")]),a._v(" "),t("ul",[t("li",[a._v("评估检索模块的"),t("strong",[a._v("排序质量")]),a._v("（而非仅召回率）：例如在文档检索阶段，判断模型是否将最相关的文档排在前N位（直接影响后续大模型回答的准确性）。")]),a._v(" "),t("li",[a._v("常见于RAG流水线的"),t("strong",[a._v("检索器调优")]),a._v("（如向量数据库参数调整、Embedding模型选型）和"),t("strong",[a._v("端到端评估")]),a._v("。")])]),a._v(" "),t("p",[a._v("它既考虑返回项的相关性, 也考虑"),t("strong",[a._v("它们在排序列表中的位置")]),a._v("。其核心思想是"),t("strong",[a._v("高度相关的项应该更早地出现在列表中")]),a._v("。")]),a._v(" "),t("h3",{attrs:{id:"evaluate"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#evaluate"}},[a._v("#")]),a._v(" Evaluate")]),a._v(" "),t("ol",[t("li",[t("p",[a._v("accuracy 准确性、准确度")])]),a._v(" "),t("li",[t("p",[a._v("f1")])]),a._v(" "),t("li",[t("p",[a._v("metric: 指标")])]),a._v(" "),t("li",[t("p",[a._v("exact_match: 精确匹配")])]),a._v(" "),t("li",[t("p",[a._v("semantic: 语义")])]),a._v(" "),t("li",[t("p",[a._v("semantic search: 语义搜索")])]),a._v(" "),t("li",[t("p",[a._v("ground_truth: 基准真值 / 标准答案")])])]),a._v(" "),t("h3",{attrs:{id:"评测相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#评测相关"}},[a._v("#")]),a._v(" 评测相关")]),a._v(" "),t("ol",[t("li",[a._v("End-to-End Evaluation: 端到端评测")])]),a._v(" "),t("h2",{attrs:{id:"大模型相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型相关"}},[a._v("#")]),a._v(" 大模型相关")]),a._v(" "),t("ol",[t("li",[a._v('在自然语言处理中，"logits" 通常指模型输出的原始预测值（未归一化的概率分数）。')]),a._v(" "),t("li",[a._v("Continuous Batching（连续批处理）")])]),a._v(" "),t("h2",{attrs:{id:"向量数据库"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#向量数据库"}},[a._v("#")]),a._v(" 向量数据库")]),a._v(" "),t("ol",[t("li",[a._v("近似最近邻(ANN: Approximate Nearest Neighbor)")]),a._v(" "),t("li",[a._v("相似性搜索(Similarity Search)")]),a._v(" "),t("li",[a._v("倒排索引技术，IVF")])]),a._v(" "),t("h3",{attrs:{id:"倒排索引技术"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#倒排索引技术"}},[a._v("#")]),a._v(" 倒排索引技术")]),a._v(" "),t("p",[t("strong",[a._v("倒排索引")]),a._v("是一种将“内容（如单词、关键词）”映射到“其所在文档/位置”的索引技术，核心是从“找内容”反向到“找内容在哪”，"),t("strong",[a._v("是搜索引擎、数据库全文检索的核心技术")]),a._v("。"),t("br"),a._v("\n例如在文档中搜索“人工智能”，它能直接返回所有包含该词的文档列表，而非逐篇扫描所有文档。")]),a._v(" "),t("h3",{attrs:{id:"ivf-倒排文件-inverted-file"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ivf-倒排文件-inverted-file"}},[a._v("#")]),a._v(" IVF（倒排文件，Inverted File）")]),a._v(" "),t("p",[t("strong",[a._v("IVF是倒排索引的一种经典存储实现")]),a._v("，将倒排索引的核心结构（关键词词典、倒排列表）以文件形式高效存储，是早期信息检索系统中实现倒排索引的主流方式，为快速查询提供了底层数据支撑。")]),a._v(" "),t("h2",{attrs:{id:"大模型相关-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型相关-2"}},[a._v("#")]),a._v(" 大模型相关")]),a._v(" "),t("ol",[t("li",[a._v("Proximal Policy Optimization(PPO 算法)")]),a._v(" "),t("li",[a._v("critic 模型（价值网络）扮演着重要角色，它主要用于评估策略的好坏，计算优势函数（Advantage Function）")]),a._v(" "),t("li",[a._v("推理：inference，项目里，也总是会用 infer.py 当做文件名  or  定义 def inference() 函数")]),a._v(" "),t("li",[a._v("Self-consistency: 自我一致性")]),a._v(" "),t("li",[a._v("ground_truth: 真实值")]),a._v(" "),t("li",[a._v("confidence：可信度、置信度")])]),a._v(" "),t("h2",{attrs:{id:"批处理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#批处理"}},[a._v("#")]),a._v(" 批处理")]),a._v(" "),t("ol",[t("li",[a._v("latency: 延迟")]),a._v(" "),t("li",[a._v("throughput: 吞吐量")])]),a._v(" "),t("h2",{attrs:{id:"多模态"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多模态"}},[a._v("#")]),a._v(" 多模态")]),a._v(" "),t("ol",[t("li",[a._v("Diffusion：扩散")]),a._v(" "),t("li",[a._v("Stable Diffusion: 稳定扩散")]),a._v(" "),t("li",[a._v("Vision Transformer：视觉编码器、图像编码器")]),a._v(" "),t("li",[a._v("DDPM: Denoising Diffusion Probabilistic Models")]),a._v(" "),t("li",[a._v("multimodal: 多模态，简称：MM, MM-RLHF: 它是大名鼎鼎的 RLHF 在多模态领域的扩展和升级。")])]),a._v(" "),t("h3",{attrs:{id:"影像生成模型"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#影像生成模型"}},[a._v("#")]),a._v(" 影像生成模型")]),a._v(" "),t("ol",[t("li",[a._v("Fréchet Inception Distance：FID，衡量生成图像分布与真实图像分布的相似度。")]),a._v(" "),t("li",[a._v("Peak Signal-to-Noise Ratio：PSNR， 评估生成图像与参考图像（真实图像）的像素级相似度，反映失真程度。")])]),a._v(" "),t("h3",{attrs:{id:"最近整理英文汇总-后面再分类到上面的类别里"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#最近整理英文汇总-后面再分类到上面的类别里"}},[a._v("#")]),a._v(" 最近整理英文汇总，"),t("code",[a._v("后面再分类到上面的类别里")])]),a._v(" "),t("ol",[t("li",[a._v("Dola")]),a._v(" "),t("li",[a._v("英文，COVE: 先回答，再质疑，再核查，再改正")]),a._v(" "),t("li",[a._v("ReAct Prompting")]),a._v(" "),t("li",[a._v("BLEU: Bilingual Evaluation Understudy，双语评估替补")]),a._v(" "),t("li",[a._v("Synergy: 协同")]),a._v(" "),t("li",[a._v("Enhanced Interactivity：增强与交互性、增强的互动性")]),a._v(" "),t("li",[a._v("Interpretability： 可解释性")]),a._v(" "),t("li",[a._v("Trustworthiness：可信度")]),a._v(" "),t("li",[a._v("Superior Decision-Making")]),a._v(" "),t("li",[a._v("系统级评估(SystemEvaluation)")]),a._v(" "),t("li",[a._v("人工评估(Human Evaluation/Qualitative)")]),a._v(" "),t("li",[a._v("Self-consistency: 自我一致性")])]),a._v(" "),t("h3",{attrs:{id:"rag"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rag"}},[a._v("#")]),a._v(" RAG")]),a._v(" "),t("ol",[t("li",[a._v("Ragas: Retrieval-Augmented Generation Assessment, 是专为 RAG 系统设计的开源评估框架")]),a._v(" "),t("li",[a._v("context_precision: 上下文精确率")]),a._v(" "),t("li",[a._v("context_recall: 上下文召回率")]),a._v(" "),t("li",[a._v("faithfulness: 忠实度")]),a._v(" "),t("li",[a._v("answer_relevancy: 答案相关性")])]),a._v(" "),t("h3",{attrs:{id:"推理优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#推理优化"}},[a._v("#")]),a._v(" 推理优化")]),a._v(" "),t("ol",[t("li",[a._v("memory: 内存、记忆")])])])}),[],!1,null,null,null);t.default=i.exports}}]);