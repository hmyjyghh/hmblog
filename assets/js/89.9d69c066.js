(window.webpackJsonp=window.webpackJsonp||[]).push([[89],{546:function(v,_,t){"use strict";t.r(_);var a=t(3),n=Object(a.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("h2",{attrs:{id:"llm-文本生成阶段"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#llm-文本生成阶段"}},[v._v("#")]),v._v(" LLM 文本生成阶段")]),v._v(" "),_("ul",[_("li",[v._v("如何一次为单个请求高效生成文本?")]),v._v(" "),_("li",[v._v("如何逐个标记地，为单个输入生成下一个标记")])]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_5.png",alt:"KV Cache 优化"}})]),v._v(" "),_("blockquote",[_("p",[v._v("预测下一个词阶段的优化")])]),v._v(" "),_("p",[_("strong",[v._v("KV缓存, 是LLM推断中最基本的优化之一")]),v._v(", 导致了我们所说的标记生成的两个不同阶段之间的分离。")]),v._v(" "),_("ol",[_("li",[v._v("首先是我们所说的预填充阶段(Prefill), 即使用所有输入生成第一个标记时发生的情况。")]),v._v(" "),_("li",[v._v("然后是我们所说的解码阶段(Decode), "),_("strong",[v._v("即使用缓存的K和V值生成后续标记的地方")]),v._v("。")])]),v._v(" "),_("p",[v._v("过去的键值是为该特定输入计算然后存储的K和V值,这样我们就不必再次计算\n它们。")]),v._v(" "),_("p",[v._v("但是加入KV Cache 之后,因为现在我们"),_("strong",[v._v("每次只传递一个标记而不是整个输入")]),v._v(",所以在解码步骤中,每个标记的时间显著下降,这就是这种大幅度下降的外观。")]),v._v(" "),_("p",[v._v("这实际上是优化LLMS文本生成的第一个重要里程碑, 这是这些推理系统真正核心的部分。")]),v._v(" "),_("p",[v._v("但在生成第一个标记之后,出现了一个非常有趣的优化, 即生成后续标记时, "),_("strong",[v._v("我们实际上只需要将新标记作为输入提供")]),v._v(",\n然后我们唯一需要重新引入先前输入信息的时间是在"),_("strong",[v._v("进行这个注意力计算时")]),v._v(",具体来说是对这些矩阵K和V,我们需要这些矩阵的值,这些值是为先前的标记计算出来的, 现在计算这个值将帮助我们生成下一个标记。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_2.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[v._v("因此,你会发现,实际上,"),_("strong",[v._v("先前标记的这些K和V的值可以被缓存,")]),v._v(" "),_("strong",[v._v("这样我们就不必在每次调用这个计算之间重新计算它们")]),v._v(", 然后只需要进行一个非常轻量级的连接操作,\n在这个操作中"),_("strong",[v._v("我们获取缓存的值, 附加新标记的新K和V值,进行注意力计算,重新缓存这些值,然后生成下一个标记。")])]),v._v(" "),_("p",[v._v("这就是KV缓存的整体概念，是LLM推断中最基本的优化之一, 导致了我们所说的标记生成的两个不同阶段之间的分离。")]),v._v(" "),_("ol",[_("li",[v._v("首先是我们所说的预填充阶段(Prefill), 即使用所有输入生成第一个标记时发生的情况。")]),v._v(" "),_("li",[v._v("然后是我们所说的解码阶段(Decode), "),_("strong",[v._v("即使用缓存的K和V值生成后续标记的地方")]),v._v("。")])]),v._v(" "),_("p",[_("strong",[v._v("这实际上使我们能够避免为输入序列中的所有过去的标记一遍又一遍地执行所有那些冗余的计算。")])]),v._v(" "),_("p",[v._v("在这之上,还有很多复杂技术, 比如页面侦测是一个非常流行的技术, "),_("strong",[v._v("它真的试图优化KV缓存的利用")]),v._v(", 无论是在内存和计算上还是在CUDA层发生了什么,所以我们可以在此基础上添加更多的优化层,像Lora这样的系统实现了这些优化。")]),v._v(" "),_("h3",{attrs:{id:"使用kv-cache-vs-不使用kv-cache"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#使用kv-cache-vs-不使用kv-cache"}},[v._v("#")]),v._v(" 使用KV Cache VS 不使用KV Cache")]),v._v(" "),_("ul",[_("li",[v._v("是哪里产生的大量耗时，注意力计算")]),v._v(" "),_("li",[v._v("通过 duration_s  和  duration_cached_s")]),v._v(" "),_("li",[v._v("用 matplotlib.pyplot 绘制曲线图，看效果对比")])]),v._v(" "),_("div",{staticClass:"language-python extra-class"},[_("pre",{pre:!0,attrs:{class:"language-python"}},[_("code",[v._v("duration_s "),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("[")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("]")]),v._v("\nduration_cached_s "),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("[")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("]")]),v._v("\nplt"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(".")]),v._v("plot"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("duration_s"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\nplt"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(".")]),v._v("plot"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("duration_cached_s"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\nplt"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(".")]),v._v("show"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n")])])]),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_1.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("橙线代表的是在有缓存的情况下生成标记所需的时间。")]),v._v(" "),_("li",[v._v("但是加入KV Cache 之后,因为现在我们每次只传递一个标记而不是整个输入,所以在解码步骤中, 每个标记的时间显著下降,这就是这种大幅度下降的外观。")]),v._v(" "),_("li",[v._v("时间节省的主要来源")])]),v._v(" "),_("h3",{attrs:{id:"多头注意力机制-矩阵乘法-qk、v-相乘"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#多头注意力机制-矩阵乘法-qk、v-相乘"}},[v._v("#")]),v._v(" 多头注意力机制， 矩阵乘法，QK、V 相乘")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("矩阵的大小，最终与输入序列的大小成正比。")]),v._v(" "),_("li",[v._v("然后我们计算总体注意力计算。或者我们进行一些矩阵乘法，我们应用掩码，我们应用softmex操作并生成输出。")])]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_3.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("对于任何的Transformer 模型来说，最大的计算瓶颈，无疑就是 注意力计算。")]),v._v(" "),_("li")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_4.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("h2",{attrs:{id:"批处理阶段优化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#批处理阶段优化"}},[v._v("#")]),v._v(" 批处理阶段优化")]),v._v(" "),_("blockquote",[_("p",[v._v("可以了解到，批处理是如何通过增加吞吐量来降低延迟的？")])]),v._v(" "),_("ul",[_("li",[v._v("将多个请求一起批处理， 如何在处理更多请求的同时（称为吞吐量） 和快速响应 任何一个请求(称为延迟)之间进行权衡。")]),v._v(" "),_("li",[v._v("批处理是一种我们可以使用的技术，专们用于提高吞吐量，这在我们想要构建一个能够处理多个并发请求的服务器系统时非常重要，而不是像我们在这里处理的单个请求一样。")])]),v._v(" "),_("h3",{attrs:{id:"引入填充标记、-注意力蒙版"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#引入填充标记、-注意力蒙版"}},[v._v("#")]),v._v(" 引入填充标记、 注意力蒙版")]),v._v(" "),_("p",[v._v("在左侧引入填充标记，目的是确保：整体上矩阵的形状是一致的。")]),v._v(" "),_("ul",[_("li",[v._v('tokenizer 可以处理提示词列表: ["query1", "query2"]')]),v._v(" "),_("li")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/kv_cache_5.jpg",alt:"KV Cache 优化"}})]),v._v(" "),_("ul",[_("li",[v._v("在左侧，填充标记，确保整体上矩阵上的形状是一样的")])]),v._v(" "),_("p",[v._v("huggingface tokenizer 支持处理 prompts 列表")]),v._v(" "),_("div",{staticClass:"language-python extra-class"},[_("pre",{pre:!0,attrs:{class:"language-python"}},[_("code",[v._v("tokenizer"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("prompts"),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" padding"),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),_("span",{pre:!0,attrs:{class:"token boolean"}},[v._v("True")]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" return_tensors"),_("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),_("span",{pre:!0,attrs:{class:"token string"}},[v._v('"pt"')]),_("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n")])])]),_("p",[v._v("shape: torch.Size([3, 7]), 形状是： 批量大小是3, 所有提供的输入中的最大序列长度为7 的  张量")]),v._v(" "),_("p",[v._v("填充后，注意力蒙版，有0，有1， 0：对应着我们引入的相同的填充标记。 后续不参与计算（告诉模型不应该关注，不要考虑填充标记）")]),v._v(" "),_("p",[v._v("应该忽略填充标记，不要影响想要生成的整体输出")]),v._v(" "),_("p",[v._v("在处理批量输入时，我们要介绍的一项新功能是位置ID的概念： position_ids")]),v._v(" "),_("p",[v._v("这些位置 ID 是与 HuggingFace Transformer实现中的某些特定内容相关的，但本质上只是告诉模型输入序列中每个标记的序数位置。")]),v._v(" "),_("p",[v._v("这只是个从Q到N的列表，对于N个标退，但是对于批量推断。我们确实需要将其填充为零，以使镇充标己在序列开始处被归零。从而官们不会对递增序列产生影响。")]),v._v(" "),_("p",[v._v("因此，我们将直先执行此操作，您可以以看到这里有一些额外的填充标退，但是在我们越过填充标记局。序列开始正式启动")]),v._v(" "),_("p",[v._v("现在,在选择下一个标记ID之前,我们所做的是取第一个批次,然后取最后一个序列元素,然后取所有的词汇可能性,\n然后计算argmax, 但是这一次, 我们要做的是改变这个最后一次的逻辑计算,\n而是要选择所有的批次, 然后不再取全局argmax, 而是要返回一个下一个标记ID的向量,\n每个批次一个,为了做到这一点,我们将对这个维度取argmax。")]),v._v(" "),_("p",[v._v("所以这一次让我们改变一下，让我们处理批次中的所有元素而不仅仅是第一个，并且我们将计算下—个标记ID而不是单个的下一个标记ID，并且我们将沿着维度试算下一个标這ID，然后像以前一样返回下一个标记旧以及我们的passkey值。")]),v._v(" "),_("blockquote",[_("p",[v._v("代码讲解，可参考代码")])]),v._v(" "),_("ul",[_("li",[v._v("例如，这将是我们想要为批次中的每一行生成的10个标记")]),v._v(" "),_("li",[v._v("我们将使用它来生成位置ID,排除注意力掩码中对应于填充标记的零的元素。")])]),v._v(" "),_("p",[v._v("然后,我们将扩展我们的批次,包括那些位置ID,以及我们最初想要传递给模型的所有其他关键字参数。")]),v._v(" "),_("p",[v._v("现在让我们继续做有趣的部分,根据先前的输入和我们刚刚生成的新标记集构建下一个输入批次。")]),v._v(" "),_("p",[v._v("因此,这里我们使用了KV缓存版本的这个过程,在我们称之为预填充步骤期间,我们会在一开始丢弃原始批次的输入。")]),v._v(" "),_("p",[v._v("对于位置ID,我们想要做一些类似的事情,即取位置ID\n的最后一个元素并将其增加一,你在这里看到了,然后我们\n将有效地继续丢弃序列中的所有前面的元素。")]),v._v(" "),_("p",[v._v("因此, 这将告诉我们我们正在提供的下一个标记集的位置ID是多少。")]),v._v(" "),_("p",[v._v("对于注意力蒙版,我们将做和之前完全一样的事情。")]),v._v(" "),_("p",[v._v("但是这里有一个稍微不同的额外注意事项,不同之处在于我们需要添加一个形状与批处理维度相等的全为1的向量到注意力掩码中。")]),v._v(" "),_("p",[v._v("现在让我们继续取出我们在这个特定迭代中生成的下一个标记ID向量,并将其转换为一个字符串列表,每个批次一个。")]),v._v(" "),_("p",[v._v("此时,我们有一个名为generateBatch的辅助函数,它接受一个来自标记器的输入字典和我们要生成的最大新标记数。")]),v._v(" "),_("p",[v._v("为此,我们将做一些有趣的事情,即我们将以红色实际呈现生成的标记,以便我们可以在视觉上将它们与原始输入区分开来。")]),v._v(" "),_("p",[v._v("13:30")]),v._v(" "),_("p",[v._v("因此,在这一点上,我们可以相当自信地认为该模型正在有效地正确地进行批处理,特别是因为我们的第一个序列与\n我们在为单个批次执行此操作时生成的完全相同,我们知道通过引入这种批处理,我们仍然在最终获得相同的输出。")]),v._v(" "),_("p",[v._v("14:07")]),v._v(" "),_("p",[v._v("批处理的一个明确目标是增加系统的吞吐量, 即在一段时间内我们可以生成的标记数量,在多个请求同时到达的情况下。")]),v._v(" "),_("p",[v._v("我们将观察到存在吞吐量和延迟之间的基本权衡。")]),v._v(" "),_("p",[v._v("14:34，")]),v._v(" "),_("p",[v._v("为了说明这一点,让我们从一个例子开始,我们只处理延迟\n优化系统,在这个系统中,每次收到请求时,我们将贪婪地处理它,而不进行任何批处理。")]),v._v(" "),_("p",[v._v("我们用不同的颜色标注时间轴上处理这个特定输入的部分, 从它空闲时开始。")]),v._v(" "),_("p",[v._v("下一个请求进来了,它需要在原始输入仍在处理时空闲一段时间。")]),v._v(" "),_("p",[v._v("然后再次,它贪婪地立即处理,第三个请求也是如此。")]),v._v(" "),_("p",[v._v("这旨在优化我们的系统以降低延迟,我们试图尽量缩短任何单个请求的等待时间。")]),v._v(" "),_("p",[v._v("所以在这里我们看到,我们的平均请求延迟为1.2秒,但我们的吞吐量只有每秒一个请求, 因为我们没有进行任何批处理。")]),v._v(" "),_("p",[v._v("15:25\n但在批处理的情况下,我们可以考虑如何在延迟和吞吐量之间权衡,优先考虑吞吐量而不是延迟。")]),v._v(" "),_("p",[v._v("一个我们可以做的事情是选择等待处理请求,直到达到一定数量的请求或者达到一定的时间限制。")]),v._v(" "),_("p",[v._v("15:44")]),v._v(" "),_("p",[v._v("在这种情况下,我们实际上会稍微降低我们的延迟,因为最初请求的用户必须稍等一会儿,但我们处理的请求总数在\n特定时间间隔内实际上是增加的。")]),v._v(" "),_("p",[v._v("在这种情况下,我们现在能够每秒处理。1.2个请求,\n而之前我们只能每秒处理一个请求。")]),v._v(" "),_("p",[v._v("所以这是一个很好的例子,尝试优先批处理可以帮助我们\n获得更好的吞吐量,但会牺牲我们可以提供给用户的延迟")]),v._v(" "),_("p",[v._v("好的,现在我们想要买际地研究延迟与吞吐量之间的这种\n影响,并试图了解等待不同批次大小对总吞吐量的影响,我\n们能够每秒生成多少个标记,以及平均延迟,或者每个标记\n平均生成需要多少秒。")]),v._v(" "),_("p",[v._v("从16:42 开始看")]),v._v(" "),_("p",[v._v("代码讲解, 结合代码去看")]),v._v(" "),_("p",[v._v("在这种情况下,我们将只设置一个常量,用于我们想要生成\n的标记数,这个数量将是10。")]),v._v(" "),_("p",[v._v("16：51\n然后,我们将定义一些数据结构来衡量我们的观察结果,比\n如持续时间,即每个样本处理所花费的时间,吞吐量,即每\n个实验样本的吞吐量,以及延迟,即每个样本的平均延迟。")]),v._v(" "),_("p",[v._v("在这种情况下,我们将尝试不同的二的幂次方,基本上从1\n到128变化,并看看这对吞吐量和延迟的影响。")]),v._v(" "),_("p",[v._v("17:28\n为了运行我们的实验,我们将首先迭代我们列表中的每个\n批处理大小,并在我们执行这些操作时进行一些简单的调\n试,我们将在每个步骤打印出批处理大小。")]),v._v(" "),_("p",[v._v("好的, 接下来我们将为每个批次生成标记并记录持续时间")]),v._v(" "),_("p",[v._v("第二步,我们将从一组大小为批处理大小的提示中形成一个批次。")]),v._v(" "),_("p",[v._v("所以我们要说的是,对于范围内的i值,我们将从我们最初\n的三个提示的原始列表中获取一个提示,并且我们将按顺\n序进行,所以这里的模运算确保我们可以为任何给定的i值\n获取一个提示。")]),v._v(" "),_("p",[v._v("这只是为了确保我们在发送给批处理的提示中有一些变化\n,而不是一遍又一遍地说同样的话。")]),v._v(" "),_("p",[v._v("生成批次,所以我们将生成最终输出作为字符串,然后最终\n记录整个过程花费的时间,单位为秒。")]),v._v(" "),_("p",[v._v("18:34， 接下来, 我们将继续记录这个特定批次大小的吞吐量和平均延迟的观察结果。")]),v._v(" "),_("p",[v._v("18:42\n所以首先,我们要计算总共生成了多少个标记,这是批次大小乘以最大标记数。")]),v._v(" "),_("p",[v._v("18：51\n然后通过将标记数除以持续时间(以秒为单位)来计算吞吐量。")]),v._v(" "),_("p",[v._v("18:57\n然后这里的平均延迟可以通过将持续时间(以秒为单位)除以最大标记数来计算。")]),v._v(" "),_("p",[v._v("19:24, 运行看结果")]),v._v(" "),_("p",[v._v("所以这里唯一的区别实际上是, 这里的结束标记值不影响平均延迟的计算。")]),v._v(" "),_("p",[v._v("19:28, 重要")]),v._v(" "),_("p",[v._v("如果你只是凭眼力观察,你会发现随着批处理大小的增加, 延迟从低点开始, 随着时间的推移而增加, 而吞吐量也开始增加。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/though-%E5%90%9E%E5%90%90%E9%87%8F.jpg",alt:"批处理"}})]),v._v(" "),_("p",[v._v("因此,你可能还记得我们想要更高的吞吐量,更高的吞吐量是好的,我们想要更低的延迟。")]),v._v(" "),_("p",[v._v("19:49\n事实上,我们观察到了我们预期的权衡,随着时间的推移, 变得越来越好的吞吐量开始降低我们的延迟。")]),v._v(" "),_("p",[v._v("好的, 现在让我们定义一个函数来为我们绘制这种关系。")]),v._v(" "),_("p",[v._v("20:06,\n我们将向我们的渲染图函数传递一些东西,批处理大小,它将是x轴,吞吐量和延迟,它们将是我们两个重叠的y轴")]),v._v(" "),_("p",[v._v("然后我们只需为这些不同的轴传递一些标签。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/batching-1.jpg",alt:"批处理"}})]),v._v(" "),_("ul",[_("li",[v._v("从图中你可以看到,吞吐量在这里以红色开始相当低, 然后随着我们继续"),_("strong",[v._v("提供越来越大的批处理大小而开始线性增加")])])]),v._v(" "),_("p",[v._v("20：41\n而延迟也开始上升,虽然相对于吞吐量而言要慢一些,直到\n我们开始达到延迟变得非常高而吞吐量无法跟上的程度,\n你可以合理地说继续使用更大的批处理大小没有任何好处\n,因为延迟的权衡非常严重。")]),v._v(" "),_("p",[v._v("但对于这个范围内的几乎任何事情,你可以说你正在探索\n一个非常合理的权衡,在这里,吞吐量正在增加,延迟仍然\n很低,尽管在增加,真正取决于你个人的用例和判断,即什\n么是你要为其优化的正确批处理大小。")]),v._v(" "),_("p",[v._v("21:25, 所以批处理就是要获得更好的吞吐量,并愿意牺牲一定量\n的潜在延迟,以此来提高系统中多个用户或同时进入系统的多个请求的整体服务质量。")]),v._v(" "),_("p",[v._v("21:43, 在下一课中学习连续批处理,这是一种针对批处理\n的优化,试图解决延迟增加而吞吐量也增加的问题,\n试图保持高吞吐量的同时, 最小化生成下一个标记的延迟。")]),v._v(" "),_("h2",{attrs:{id:"连续批处理阶段优化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#连续批处理阶段优化"}},[v._v("#")]),v._v(" 连续批处理阶段优化")]),v._v(" "),_("h3",{attrs:{id:"continuous-batching"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#continuous-batching"}},[v._v("#")]),v._v(" Continuous Batching")]),v._v(" "),_("ul",[_("li",[v._v("在上一课中,您看到了批处理如何通过增加吞吐量来降低延迟。")]),v._v(" "),_("li",[v._v("在本课中,您将实现一种称为连续批处理的技术,利用迭代的逐词文本生成过程来获得高吞吐量和低延迟推理的最佳效果。")])]),v._v(" "),_("p",[v._v("00：32\n首先,让我们重新审视之前的同步批处理情况,即我们有多\n个不同时间到达的请求,然后我们希望将它们一起批处理到一个单独的批次中,以提高吞吐量。")]),v._v(" "),_("p",[v._v("正如我们所看到的,当我们这样做时,我们的吞吐量会增加\n,但我们的延迟会受到重大打击。")]),v._v(" "),_("p",[v._v("00：43\n然而,我们可以观察到的一点是,在同步批处理情况下,即\n使我们将所有这些请求放在一个单独的批次中并一起处理\n到最后,实际上,我们生成的每个标记都可以被视为一个单\n独的、独立的操作,可以从其他步骤中分解和分离出来。")]),v._v(" "),_("p",[v._v("这是因为,正如您所记得的,这些自回归语言模型实际上是\n逐标记生成的。")]),v._v(" "),_("p",[v._v("01:13,")]),v._v(" "),_("p",[v._v("这就引出了连续批处理的概念,即如果我们实际上按照请\n求的到达顺序贪婪地逐标记处理这些请求,但是在看到一\n个新请求时,当我们完成一个特定标记后,我们决定是否要\n继续将该请求合并到我们现有的批处理中,这样它们就可\n以继续一起生成标记,从而获得吞吐量的优势。")]),v._v(" "),_("p",[v._v("01:41,")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/continuous-batching-1.jpg",alt:"动态批处理"}})]),v._v(" "),_("p",[v._v("进一步说,如果您的请求在不同时间完成,因为它们要么在\n较早时间开始,要么要生成的标记数较少,或者它们遇到停\n止标记,那么您可以有效地从批处理中删除其中一个序列,\n并将其替换为另一个正在等待轮到的请求。")]),v._v(" "),_("p",[v._v("02:07\n因此,不断地在批处理中移动元素的想法,即使有些元素可\n能会在某段时间内保留,也是我们所称的连续批处理的核心。")]),v._v(" "),_("p",[v._v("还有一件需要注意的事情是,再次强调这里的预填充与解\n码的概念是很重要的。")]),v._v(" "),_("p",[v._v("02:25,\n例如,您会注意到在图中,第一个标记的估计时间比后续标记长一些。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/continuous-batching-2.jpg",alt:"动态批处理"}})]),v._v(" "),_("p",[v._v("02:34, 因此,在连续批处理系统中,我们通常会将这些预加载和解\n码步骤概念上作为单独的批处理来处理。")]),v._v(" "),_("p",[v._v("02:40\n因此,如果我们同时收到多个需要进行预加载的请求,我们\n将一起处理它们,但否则,我们将尝试将所有解码请求保持\n在单个批处理中,以便我们可以限制我们必须做的过量填\n充,并且可以提高系统的吞吐量和延迟。")]),v._v(" "),_("p",[v._v("因此,在这个假设的例子中,您可以看到我们有点像两全其美。")]),v._v(" "),_("p",[v._v("03:04, 我们的延迟非常低：")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/continuous-batching-3.jpg",alt:"动态批处理"}})]),v._v(" "),_("p",[v._v("03:06, 事实上,它甚至比我们一次处理一个请求的情况下还要低,\n并且在这种情况下,我们的吞吐量与同步批处理的情况下一样好。")]),v._v(" "),_("p",[v._v("但是,如果我们进一步扩展到生成不同数量标记的请求的\n情况,我们甚至可能会通过连续批处理来提高吞吐量。")]),v._v(" "),_("p",[v._v("03:36， 开始讲解代码, 参考代码")]),v._v(" "),_("p",[v._v("13:10, 当我们将它们连接起来时,实质上我们想要一个新的张量,\n其中包含两者的元素,并在必要时引入一些额外的填充,以\n确保形状对齐。")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/continuous-batching-4-merge-batch.jpg",alt:"动态批处理"}})]),v._v(" "),_("p",[v._v("14:11,")]),v._v(" "),_("p",[v._v("它将首先从批处理中删除已达到终端状态的元素,要么是\n因为终止标记,要么是因为已经生成了最大数量的标记,然\n后它将执行与合并步骤相反的操作,从开始处删除多余的\n填充标记,因为这会增加计算和内存开销。")]),v._v(" "),_("p",[v._v("14:20,")]),v._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/continuous-batching-5-filtering-batches.jpg",alt:"动态批处理"}})]),v._v(" "),_("p",[v._v("14:34\n所以,当我们调用过滤批处理函数时,我们得到两个结果,\n一个是可能只剩下需要生成的内容的较小的缓存批次,另\n一个是在此步骤中被移除的批次中的索引列表。")]),v._v(" "),_("p",[v._v("14:57, 有一个initBatch函数,我们有generateNextToken函数\n,mergeBatches函数和filterBatch函数。")]),v._v(" "),_("p",[v._v("filter_batch 函数，所以,如果你想了解更多关于细节的信息,这些信息更多地\n与PyTorch有关,我邀请你看一看那个文件,它将包含有关\n每个步骤发生了什么的注释。")]),v._v(" "),_("p",[v._v("15:26， 重要，对比，做对比\n最后,让我们继续使用之前用于同步分批处理的相同输入\n来运行我们的连续分批处理过程,并查看是否有任何运行时的改进。")]),v._v(" "),_("p",[v._v("15:40\n"),_("img",{attrs:{src:"/hmblog/images/llm/vllm-kvcache/continuous-batching-6.jpg.jpg",alt:"动态批处理"}}),v._v("\n很好,完成时间约为21秒,您可能还记得,之前的实现大约\n需要71秒,因此整体端到端运行时显著减少了。")]),v._v(" "),_("p",[v._v("15:55, 您可能也注意到,在最后几个元素时需要更长一点的时间,\n因此可以想象这对应于我们想要生成100个标记的那些较长的序列。")]),v._v(" "),_("p",[v._v("很重要，很重要， Continuous Batching 重要原理，实现\n16:11, 因此,我们能够在最开始将它们添加到批处理中,然后在整\n个过程中继续添加和删除那些10个标记的较小序列。")]),v._v(" "),_("p",[v._v("16:19, 然后那些落后的、那些100个标记序列最终在未尾完成而\n不会在整个过程中造成瓶颈。")]),v._v(" "),_("p",[v._v("16:27， 这基本上就是连续批处理能够为我们做到的事情。")]),v._v(" "),_("p",[v._v("16:34，\n而这最终不仅是提高LLM推断的吞吐量和延迟的关键驱动\n因素之一,还是您常见的LLM推断系统中看到的标记流式处\n理能力的关键,因为我们能够及时处理它们,并从系统中逐\n个获取标记,我们能够非常迅速地将这些结果返回给用户,\n而不必等待这些大的批处理步骤。")]),v._v(" "),_("p",[v._v("17:03， 在接下来的课程中,我们将超越批处理,开始讨论其他可以\n提高系统效率的方法,包括量化,它帮助我们减少运行更大\n模型所需的内存压力,以及我们可以使用的技术来高效地\n提供经过精细调整的模型,例如低秩适应。")]),v._v(" "),_("p",[v._v("量化的目的，减少显存的使用，")]),v._v(" "),_("h2",{attrs:{id:"暂未整理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#暂未整理"}},[v._v("#")]),v._v(" 暂未整理")]),v._v(" "),_("p",[v._v("这就引出了连续批处理的概念,即如果我们实际上按照请求的到达顺序贪婪地逐标记处理这些请求,但是在看到一个新请求时,当我们完成一个特定标记后,我们决定是否要继续将该请求合并到我们现有的批处理中,这样它们就可以继续一起生成标记,从而获得吞吐量的优势。")]),v._v(" "),_("p",[v._v("因此,如果我们同时收到多个需要进行预加载的请求,我们将一起处理它们,但否则,我们将尝试将所有解码请求保持在单个批处理中,以便我们可以限制我们必须做的过量填充,并且可以提高系统的吞吐量和延迟。")]),v._v(" "),_("p",[v._v("进一步扩展到: 生成不同数量标记的请求的情况,我们甚至可能会"),_("strong",[v._v("通过连续批处理来提高吞吐量。")])]),v._v(" "),_("p",[v._v("该课程能学到:")]),v._v(" "),_("ul",[_("li",[_("strong",[v._v("了解自回归大型语言模型如何逐个标记生成文本")]),v._v("。")]),v._v(" "),_("li",[v._v("使用代码实现现代LLM推理堆栈的基本组成部分，"),_("strong",[v._v("包括KV缓存、连续批处理和模型量化，并对其对推理吞吐量和延迟的影响进行基准测试。")])]),v._v(" "),_("li",[v._v("探索LoRA适配器的工作细节，并学习批处理技术如何使不同的LoRA适配器能够同时为多个客户提供服务。")]),v._v(" "),_("li",[v._v("通过实际使用Predibase的LoRAX框架推理服务器，亲身实践这些优化技术在真实世界的LLM推理服务器中的应用。")])]),v._v(" "),_("blockquote",[_("p",[_("strong",[v._v("了解LLM服务器在内部是如何运作的将极大地增强您提高LLM应用程序性能和效率的选择。")])])]),v._v(" "),_("h3",{attrs:{id:"其他-暂未整理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#其他-暂未整理"}},[v._v("#")]),v._v(" 其他，暂未整理")]),v._v(" "),_("p",[_("strong",[v._v("将多个请求批处理")]),v._v("在一起"),_("strong",[v._v("是提高 LLM 推理系统吞吐量的关键技术之一。")])]),v._v(" "),_("h2",{attrs:{id:"其他-暂未整理-2"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#其他-暂未整理-2"}},[v._v("#")]),v._v(" 其他，暂未整理")]),v._v(" "),_("p",[v._v("KV Cache\n还有像KV Cache 这样的技术,通过在每个标记生成后将变压器网络的注意力计算结果存储在内存中来加快推理速度,\n这样在生成后续标记时，就不必重新执行这些计算")]),v._v(" "),_("p",[v._v("KV Cache 是如何大幅减少每个后续标记的延迟。")]),v._v(" "),_("p",[v._v("通过同时实施这些技术中的几种,您可以提高延迟(用户接收到响应所需的时间)以及吞吐量(服务器处理请求的速率）。")]),v._v(" "),_("ol",[_("li",[v._v("首先了解 自回归大型语言模型 如何一次生成一个标记的文本。")]),v._v(" "),_("li",[v._v("还将学习如何将提示批处理为单个张量，以便LLM 可以同时处理多个输入。")]),v._v(" "),_("li",[v._v("然后扩展这个想法，到一种：连续批处理的技术（Continuous Batching），它允许你在新请求到达时，和 旧请求完成时，动态更新批处理。")]),v._v(" "),_("li",[v._v("像 Credabase 这样的LLM 托管服务，能够同时为许多客户提供良好的延迟和吞吐量。")]),v._v(" "),_("li",[v._v("量化函数")]),v._v(" "),_("li",[v._v("LoRa, 还将多个 Lora 和 连续批处理技术相结合，以同时为数百个微调模型提供服务，同时保持吞吐量和低延迟。")]),v._v(" "),_("li",[v._v("Predabase")])]),v._v(" "),_("p",[v._v("了解这些基础知识很重要， 为你的项目或者应用 做决策")]),v._v(" "),_("h2",{attrs:{id:"模型架构"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#模型架构"}},[v._v("#")]),v._v(" 模型架构")]),v._v(" "),_("h3",{attrs:{id:"bert-模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#bert-模型"}},[v._v("#")]),v._v(" BERT 模型")]),v._v(" "),_("ul",[_("li",[v._v("基于编码器的模型")])]),v._v(" "),_("h3",{attrs:{id:"gpt-2"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#gpt-2"}},[v._v("#")]),v._v(" GPT-2")]),v._v(" "),_("ul",[_("li",[v._v("仅解码器模型，Decoder Only， 是没有编码器的。")])]),v._v(" "),_("h2",{attrs:{id:"动态批处理-vs-连续批处理-continuous-batching"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#动态批处理-vs-连续批处理-continuous-batching"}},[v._v("#")]),v._v(" 动态批处理 VS 连续批处理(Continuous Batching)")]),v._v(" "),_("p",[v._v("好的，这是一个非常核心且重要的问题。简单来说，"),_("strong",[v._v("连续批处理是动态批处理的一种更高级、更极致的演进形式")]),v._v("，它解决了动态批处理在长文本、高并发等场景下的根本性瓶颈。")]),v._v(" "),_("p",[v._v("我们来详细拆解一下两者的区别。")]),v._v(" "),_("hr"),v._v(" "),_("h3",{attrs:{id:"_1-动态批处理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-动态批处理"}},[v._v("#")]),v._v(" 1. 动态批处理")]),v._v(" "),_("p",[v._v("动态批处理是相对于"),_("strong",[v._v("静态批处理")]),v._v("而言的。")]),v._v(" "),_("ul",[_("li",[_("p",[_("strong",[v._v("静态批处理")]),v._v("：在服务启动或处理前，将请求预先分组，形成一个或多个固定大小的批次。处理过程中，一个批次内的所有请求必须同时开始、同时结束。"),_("strong",[v._v("缺点很明显")]),v._v("：如果一批次里有一个请求处理很慢（比如生成长文本），那么整个批次的其他请求即使完成了，也要等待这个慢请求，造成资源浪费（著名的“木桶效应”）。")])]),v._v(" "),_("li",[_("p",[_("strong",[v._v("动态批处理")]),v._v("：")]),v._v(" "),_("ul",[_("li",[_("strong",[v._v("核心思想")]),v._v("：当一个推理实例（如GPU）空闲时，它不会等待一个固定的批次，而是"),_("strong",[v._v("实时地从请求队列中取出尽可能多的请求，组成一个新批次")]),v._v("进行处理。批次大小 "),_("code",[v._v("batch_size")]),v._v(" 不再是固定的，而是根据当前队列长度动态变化的。")]),v._v(" "),_("li",[_("strong",[v._v("工作流程")]),v._v("：\n"),_("ol",[_("li",[v._v("请求到达服务器，进入等待队列。")]),v._v(" "),_("li",[v._v("GPU完成上一批任务后，调度器查看队列。")]),v._v(" "),_("li",[v._v("调度器根据预设的"),_("strong",[v._v("最大批次大小")]),v._v("，将当前队列中的多个请求（例如N个）组合成一个新批次。")]),v._v(" "),_("li",[_("strong",[v._v("整个批次")]),v._v("的输入Tokens一起通过模型的计算层（如Transformer的Attention和FFN），"),_("strong",[v._v("同时开始，同时结束")]),v._v("。")]),v._v(" "),_("li",[v._v("输出所有结果后，再开始处理下一个动态批次。")])])]),v._v(" "),_("li",[_("strong",[v._v("优点")]),v._v("：\n"),_("ul",[_("li",[v._v("相比静态批处理，提升了GPU利用率和吞吐量。")]),v._v(" "),_("li",[v._v("能更好地应对请求流的波动。")])])]),v._v(" "),_("li",[_("strong",[v._v("缺点/瓶颈")]),v._v("：\n"),_("ul",[_("li",[_("strong",[v._v("依然存在“木桶效应”")]),v._v("：一个批次内，如果某个请求需要生成的输出很长（比如1000个token），而其他请求很短（比如10个token），那么生成长文本的请求会拖累整个批次，导致短请求也要等待很久才能返回。")])])])])])]),v._v(" "),_("hr"),v._v(" "),_("h3",{attrs:{id:"_2-连续批处理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2-连续批处理"}},[v._v("#")]),v._v(" 2. 连续批处理")]),v._v(" "),_("p",[v._v("连续批处理是为了解决动态批处理的“木桶效应”而生的，它是当前大模型推理服务的核心技术。")]),v._v(" "),_("ul",[_("li",[_("p",[_("strong",[v._v("核心思想")]),v._v("："),_("strong",[v._v("打破“批次”的界限")]),v._v("。不再要求一个批次内的请求同时开始、同时结束。它允许已经完成输出的请求提前离开批次，并将新的等待请求加入当前正在运行的批次中，从而实现"),_("strong",[v._v("批次的“连续”或“流动”")]),v._v("。")])]),v._v(" "),_("li",[_("p",[_("strong",[v._v("关键机制")]),v._v("："),_("code",[v._v("Iteration")]),v._v("级别的调度。大模型的生成过程是自回归的，每次迭代只生成一个Token。连续批处理在"),_("strong",[v._v("每一次生成Token的迭代中")]),v._v("，都会重新决策批次的组成。")])]),v._v(" "),_("li",[_("p",[_("strong",[v._v("工作流程")]),v._v("：")]),v._v(" "),_("ol",[_("li",[v._v("初始时，GPU上有一个批次（Batch）的请求，每个请求都处于“正在生成”状态。")]),v._v(" "),_("li",[_("strong",[v._v("每一步迭代（生成一个Token）")]),v._v("：\n"),_("ul",[_("li",[_("strong",[v._v("生成")]),v._v("：当前批次中的所有请求并行地通过模型计算，生成下一个Token。")]),v._v(" "),_("li",[_("strong",[v._v("检查")]),v._v("：检查每个请求的状态。\n"),_("ul",[_("li",[v._v("如果某个请求生成了结束符（"),_("code",[v._v("<eos>")]),v._v("），或者达到了最大生成长度，则标记为“完成”。")])])]),v._v(" "),_("li",[_("strong",[v._v("更新批次")]),v._v("：\n"),_("ul",[_("li",[_("strong",[v._v("释放")]),v._v("：将已“完成”的请求从当前活动批次中移除，并返回结果给用户。")]),v._v(" "),_("li",[_("strong",[v._v("加入")]),v._v("：将等待队列中的"),_("strong",[v._v("新请求")]),v._v("加入到当前活动批次中。这些新请求从第一步（迭代）开始计算。")])])])])]),v._v(" "),_("li",[v._v("这个过程在每一步迭代中循环进行，批次成员动态变化。")])])]),v._v(" "),_("li",[_("p",[_("strong",[v._v("优点")]),v._v("：")]),v._v(" "),_("ul",[_("li",[_("strong",[v._v("彻底解决了“木桶效应”")]),v._v("：短请求可以快速完成并返回，不会被长请求阻塞。")]),v._v(" "),_("li",[_("strong",[v._v("极高的GPU利用率")]),v._v("：GPU几乎在每个时刻都在处理满负荷的计算，因为新的请求可以立即填补已完成请求留下的空位。")]),v._v(" "),_("li",[_("strong",[v._v("显著降低延迟")]),v._v("：尤其是对于短文本交互场景，用户体验大幅提升。")])])]),v._v(" "),_("li",[_("p",[_("strong",[v._v("挑战")]),v._v("：")]),v._v(" "),_("ul",[_("li",[_("strong",[v._v("实现复杂")]),v._v("：需要深度学习框架和推理引擎在底层进行大量优化，以支持动态变化的内存管理和计算调度。")]),v._v(" "),_("li",[_("strong",[v._v("KVCache管理")]),v._v("：需要高效地分配和释放每个请求的Key-Value缓存。")])])])]),v._v(" "),_("hr"),v._v(" "),_("h3",{attrs:{id:"一个生动的比喻"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#一个生动的比喻"}},[v._v("#")]),v._v(" 一个生动的比喻")]),v._v(" "),_("ul",[_("li",[_("strong",[v._v("静态批处理")]),v._v("：像一辆"),_("strong",[v._v("固定发车时间和大小的巴士")]),v._v("。人满发车，不到站不停。即使有人只坐一站，也要等所有乘客到终点。")]),v._v(" "),_("li",[_("strong",[v._v("动态批处理")]),v._v("：像一辆"),_("strong",[v._v("人满就发车的巴士")]),v._v("。车的大小可以变化，但还是一辆车把所有乘客"),_("strong",[v._v("同时")]),v._v("送到各自目的地。如果有人要去很远的地方，全车人都得等着。")]),v._v(" "),_("li",[_("strong",[v._v("连续批处理")]),v._v("：像一列"),_("strong",[v._v("地铁")]),v._v("。"),_("strong",[v._v("站点（迭代）")]),v._v(" 非常密集。每个站点（每生成一个Token）都有人"),_("strong",[v._v("下车（请求完成）")]),v._v("，也允许新的人"),_("strong",[v._v("上车（新请求加入）")]),v._v("。每个人都可以在离自己目的地最近的站点下车，无需等待其他人。")])]),v._v(" "),_("h3",{attrs:{id:"总结"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[v._v("#")]),v._v(" 总结")]),v._v(" "),_("blockquote",[_("p",[v._v("连续批处理不是对动态批处理的简单否定，而是一次质的飞跃。")])]),v._v(" "),_("ul",[_("li",[_("p",[v._v("它通过将调度粒度从“请求级”细化到“Token级”，实现了计算资源的近乎完美的利用，并大幅降低了用户感知的延迟。")])]),v._v(" "),_("li",[_("p",[v._v("目前，主流的推理框架如 "),_("strong",[v._v("NVIDIA TensorRT-LLM, vLLM, TGI")]),v._v(" 等，"),_("strong",[v._v("其高性能的核心正是实现了连续批处理技术")]),v._v("。")])]),v._v(" "),_("li",[_("p",[v._v("对于需要服务大规模、交互式用户请求的大模型应用来说，连续批处理是必不可少的技术。")])])])])}),[],!1,null,null,null);_.default=n.exports}}]);