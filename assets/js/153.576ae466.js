(window.webpackJsonp=window.webpackJsonp||[]).push([[153],{598:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("对于 "),s("strong",[t._v("T5")]),t._v(" (Text-to-Text Transfer Transformer) 模型，包括 "),s("code",[t._v("t5-base")]),t._v("，其核心思想是"),s("strong",[t._v("将所有任务都转换为“文本到文本”的格式")]),t._v("。")]),t._v(" "),s("p",[t._v("将输入格式化为 "),s("code",[t._v('"question: {question} context: {context}"')]),t._v(" 是一种非常常见且正确的做法，特别是在"),s("strong",[t._v("阅读理解 (Question Answering)")]),t._v(" 或"),s("strong",[t._v("基于上下文的问答")]),t._v("任务中。")]),t._v(" "),s("h3",{attrs:{id:"t5-输入格式详解"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#t5-输入格式详解"}},[t._v("#")]),t._v(" T5 输入格式详解")]),t._v(" "),s("p",[t._v("T5 模型通过特定的“前缀”(prefix)或“提示”(prompt)来理解它需要执行什么任务。"),s("code",[t._v('"question: ... context: ..."')]),t._v(" 就是这样一个提示，它告诉模型：“这是一个需要你根据给定的上下文来回答问题的问题。”")]),t._v(" "),s("h4",{attrs:{id:"_1-标准格式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-标准格式"}},[t._v("#")]),t._v(" 1. 标准格式")]),t._v(" "),s("p",[t._v("对于"),s("strong",[t._v("抽取式问答")]),t._v("（从上下文中提取答案片段），最常见的输入格式就是：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('"question: {question} context: {context}"\n')])])]),s("p",[s("strong",[t._v("例如：")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("input_text "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"question: What is the capital of France? context: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower."')]),t._v("\n")])])]),s("p",[t._v("模型的目标是生成文本片段 "),s("code",[t._v('"Paris"')]),t._v("。")]),t._v(" "),s("h4",{attrs:{id:"_2-其他常见任务的输入格式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-其他常见任务的输入格式"}},[t._v("#")]),t._v(" 2. 其他常见任务的输入格式")]),t._v(" "),s("p",[t._v("T5 的灵活性在于你可以通过改变前缀来指定不同的任务：")]),t._v(" "),s("ul",[s("li",[s("p",[s("strong",[t._v("翻译 (Translation):")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('"translate English to German: The house is wonderful."\n')])])]),s("p",[t._v("输出："),s("code",[t._v('"Das Haus ist wunderbar."')])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("摘要 (Summarization):")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('"summarize: The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which succeeded in landing the first humans on the Moon from 1969 to 1972."\n')])])]),s("p",[t._v("输出："),s("code",[t._v('"The Apollo program was a NASA program that landed humans on the moon between 1969 and 1972."')])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("语法纠错 (Grammatical Correction):")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('"cola sentence: He went to the store."\n')])])]),s("p",[t._v("(COLA 是一个语法可接受性任务)")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("文本分类 (Sentiment Analysis):")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('"sst2 sentence: I loved the movie!"\n')])])]),s("p",[t._v("输出："),s("code",[t._v('"positive"')])])])]),t._v(" "),s("h3",{attrs:{id:"如何在实际中使用-代码示例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#如何在实际中使用-代码示例"}},[t._v("#")]),t._v(" 如何在实际中使用（代码示例）")]),t._v(" "),s("p",[t._v("以下是使用 "),s("code",[t._v("transformers")]),t._v(" 库和 "),s("code",[t._v("t5-base")]),t._v(" 模型进行问答的示例代码：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" T5ForConditionalGeneration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T5Tokenizer\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载模型和分词器")]),t._v("\nmodel_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"t5-base"')]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" T5Tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" T5ForConditionalGeneration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 定义你的问题和上下文")]),t._v("\nquestion "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"What is the capital of France?"')]),t._v("\ncontext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower."')]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 按照标准格式构建输入")]),t._v("\ninput_text "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"question: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(" context: ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对输入进行编码")]),t._v("\ninput_ids "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pt"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truncation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 生成输出")]),t._v("\noutputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("generate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  input_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 生成答案的最大长度")]),t._v("\n  num_beams"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("          "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用beam search提高质量")]),t._v("\n  early_stopping"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 遇到结束符则停止生成")]),t._v("\n  no_repeat_ngram_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 避免重复的n-gram")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 解码生成的输出")]),t._v("\nanswer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skip_special_tokens"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"Input: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("input_text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"Generated Answer: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# 期望输出: "Paris"')]),t._v("\n")])])]),s("h3",{attrs:{id:"重要注意事项"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#重要注意事项"}},[t._v("#")]),t._v(" 重要注意事项")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("任务前缀的一致性")]),t._v("：虽然 "),s("code",[t._v('"question: ... context: ..."')]),t._v(" 是通用格式，但有些专门在特定问答数据集（如 SQuAD）上微调过的 T5 模型可能使用了略微不同的前缀（例如 "),s("code",[t._v('"question: ... answer: ..."')]),t._v("）。最好查看你使用的"),s("strong",[t._v("特定模型卡（Model Card）")]),t._v(" 或其训练代码，以确认其使用的最佳前缀。但对于通用的 "),s("code",[t._v("t5-base")]),t._v("（未针对QA微调的原生模型），你使用的格式是完全正确的。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("输入长度")]),t._v("：T5 模型有最大输入长度限制（"),s("code",[t._v("t5-base")]),t._v(" 通常是 512 个 token）。如果 "),s("code",[t._v("{context}")]),t._v(" 很长，你需要对其进行截断或分段处理。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("微调 (Fine-tuning)")]),t._v("：如果你在自己的问答数据集上微调 T5，你"),s("strong",[t._v("必须")]),t._v("在训练和推理时使用"),s("strong",[t._v("完全相同的输入格式")]),t._v("（即 "),s("code",[t._v('"question: {question} context: {context}"')]),t._v("），否则模型会表现不佳。")])])]),t._v(" "),s("p",[s("strong",[t._v("总结：是的，"),s("code",[t._v('"question: {question} context: {context}"')]),t._v(" 是为 T5 模型构建问答任务输入的经典、正确且有效的格式。")])])])}),[],!1,null,null,null);s.default=e.exports}}]);