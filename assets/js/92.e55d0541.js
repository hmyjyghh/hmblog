(window.webpackJsonp=window.webpackJsonp||[]).push([[92],{534:function(t,v,_){"use strict";_.r(v);var r=_(3),s=Object(r.a)({},(function(){var t=this,v=t._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h3",{attrs:{id:"面试版简洁回答-核心版"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#面试版简洁回答-核心版"}},[t._v("#")]),t._v(" 面试版简洁回答（核心版）")]),t._v(" "),v("p",[t._v("关于多模态GRPO，可以从三个层面来概括：")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("定位：")]),t._v(" 它是一种"),v("strong",[t._v("无需训练奖励模型")]),t._v("的强化学习方法，直接用来"),v("strong",[t._v("对齐和优化多模态模型")]),t._v("的输出，比如让文生图模型生成更符合人类审美的图片。")])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("核心流程：")]),t._v(" 它的工作流程可以概括为 "),v("strong",[t._v("“采样-评分-优化”")]),t._v(" 三步循环：")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("采样：")]),t._v(" 让当前模型为一批提示生成大量输出（如图片）。")]),t._v(" "),v("li",[v("strong",[t._v("评分：")]),t._v(" 用一个"),v("strong",[t._v("预定义的、不可微的奖励函数")]),t._v("（比如一个图像审美打分模型）直接为这些输出打分。")]),t._v(" "),v("li",[v("strong",[t._v("优化：")]),t._v(" 使用类似PPO的策略梯度算法，"),v("strong",[t._v("朝着高分方向更新模型，同时约束模型不偏离原始能力太远")]),t._v("。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("价值与趋势：")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("它的最大优势是流程简化，避免了训练奖励模型的成本和复杂性。")])]),t._v(" "),v("li",[t._v("在当前实践中，与其思想一脉相承但更稳定、更流行的方法是 "),v("strong",[t._v("DPO")]),t._v("，它已经成为多模态对齐领域的主流技术之一。")])])])]),t._v(" "),v("p",[t._v("总结来说，多模态GRPO/DPO的核心思想，就是"),v("strong",[t._v("用直接的偏好信号作为“罗盘”，来高效地引导大模型朝着我们期望的方向进化。")])]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"回答分解与亮点-供你准备使用"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#回答分解与亮点-供你准备使用"}},[t._v("#")]),t._v(" 回答分解与亮点（供你准备使用）")]),t._v(" "),v("p",[v("strong",[t._v("为什么这个回答好？")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("结构清晰：")]),t._v(" 采用“定位-流程-价值”的三段式结构，符合认知逻辑。")]),t._v(" "),v("li",[v("strong",[t._v("通俗易懂：")]),t._v(" 使用了“对齐”、“采样-评分-优化”、“罗盘”等比喻，即使非技术背景的面试官也能抓住精髓。")]),t._v(" "),v("li",[v("strong",[t._v("体现深度：")]),t._v(" "),v("ul",[v("li",[t._v("提到了关键前提“无需训练奖励模型”，这直接点出了其相对于RLHF的优势。")]),t._v(" "),v("li",[t._v("提到了“不可微的奖励函数”，这是一个技术关键词，体现了你的理解。")]),t._v(" "),v("li",[t._v("提到了“约束模型不偏离原始能力”，这指代了KL散度惩罚这个核心技巧。")]),t._v(" "),v("li",[t._v("最后提到了更前沿的"),v("strong",[t._v("DPO")]),t._v("，表明你不仅了解经典方法，还关注技术动态，这是巨大的加分项。")])])])]),t._v(" "),v("p",[v("strong",[t._v("如果面试官追问，你可以进一步展开：")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("问流程细节：")]),t._v(" “您想了解优化步骤中具体是如何平衡‘追求高分’和‘防止偏离’的吗？”")]),t._v(" "),v("li",[v("strong",[t._v("问应用场景：")]),t._v(" “除了图像审美，它还可以用于提升生成图像的安全性和提示跟随能力。”")]),t._v(" "),v("li",[v("strong",[t._v("问与DPO的区别：")]),t._v(" “GRPO/PPO是在生成过程中进行优化，而DPO用了一个更巧妙的损失函数，直接在高分和低分样本上做对比学习，因此更稳定高效。”")])]),t._v(" "),v("p",[v("strong",[t._v("记住这个回答的精髓：")])]),t._v(" "),v("p",[v("strong",[t._v("“用规则打分，用RL优化，最终目的是让模型的输出更懂人心。”")])]),t._v(" "),v("p",[t._v("这个回答能让你在1-2分钟内展现出对技术本质的清晰理解和良好的概括能力，给面试官留下深刻印象。")]),t._v(" "),v("p",[v("strong",[t._v("MM-RLHF")]),t._v(" 是一个非常重要且前沿的技术，它代表了 "),v("strong",[t._v("基于人类反馈的多模态强化学习")]),t._v("。")]),t._v(" "),v("p",[t._v("简单来说，它是大名鼎鼎的 "),v("strong",[t._v("RLHF")]),t._v(" 在多模态领域的扩展和升级。RLHF 是让 ChatGPT 等大语言模型变得如此“听话”和“有用”的核心技术，而 MM-RLHF 的目标是让能够理解和生成"),v("strong",[t._v("图像、视频、音频")]),t._v("等多模态内容的模型也具备同样的能力。")]),t._v(" "),v("p",[t._v("下面我将分步为您详细拆解 MM-RLHF 是如何工作的。")]),t._v(" "),v("h3",{attrs:{id:"核心思想"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#核心思想"}},[t._v("#")]),t._v(" 核心思想")]),t._v(" "),v("p",[t._v("MM-RLHF 的核心思想与 RLHF 一脉相承："),v("strong",[t._v("我们不直接告诉模型什么是“好”的答案，而是通过人类对模型不同输出的偏好（喜欢哪个、不喜欢哪个）来间接地“教会”模型我们的价值观和标准。")])]),t._v(" "),v("h3",{attrs:{id:"mm-rlhf-的三个关键步骤"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#mm-rlhf-的三个关键步骤"}},[t._v("#")]),t._v(" MM-RLHF 的三个关键步骤")]),t._v(" "),v("p",[t._v("整个过程通常包含三个主要阶段，下图清晰地展示了其工作流程：")]),t._v(" "),v("div",{staticClass:"language-mermaid extra-class"},[v("pre",{pre:!0,attrs:{class:"language-mermaid"}},[v("code",[v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("flowchart")]),t._v(" TD\n    "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("subgraph")]),t._v(" S1 "),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[阶段一：监督微调]")]),t._v("\n        A"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[多模态模型]")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--")]),t._v(" 使用高质量“指令-回答”数据 "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" B"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[SFT模型]")]),t._v("\n    "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("end")]),t._v("\n\n    "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("subgraph")]),t._v(" S2 "),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[阶段二：训练奖励模型]")]),t._v("\n        C"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“模型生成的多模态回答对”]")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" D"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("{“人类标注<br>偏好排序”}")]),t._v("\n        D "),v("span",{pre:!0,attrs:{class:"token inter-arrow-label"}},[v("span",{pre:!0,attrs:{class:"token arrow-head arrow operator"}},[t._v("--")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token label property"}},[t._v("构建偏好数据集")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")])]),t._v(" E"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[奖励模型]")]),t._v("\n        E "),v("span",{pre:!0,attrs:{class:"token inter-arrow-label"}},[v("span",{pre:!0,attrs:{class:"token arrow-head arrow operator"}},[t._v("--")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token label property"}},[t._v("训练目标是<br>区分优劣回答")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")])]),t._v(" F"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[训练好的RM]")]),t._v("\n    "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("end")]),t._v("\n\n    "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("subgraph")]),t._v(" S3 "),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[阶段三：强化学习优化]")]),t._v("\n        B "),v("span",{pre:!0,attrs:{class:"token inter-arrow-label"}},[v("span",{pre:!0,attrs:{class:"token arrow-head arrow operator"}},[t._v("--")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token label property"}},[t._v("生成回答")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")])]),t._v(" G"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[PPO等算法]")]),t._v("\n        F "),v("span",{pre:!0,attrs:{class:"token inter-arrow-label"}},[v("span",{pre:!0,attrs:{class:"token arrow-head arrow operator"}},[t._v("--")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token label property"}},[t._v("计算奖励分数")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")])]),t._v(" G\n        G "),v("span",{pre:!0,attrs:{class:"token inter-arrow-label"}},[v("span",{pre:!0,attrs:{class:"token arrow-head arrow operator"}},[t._v("--")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token label property"}},[t._v("更新策略")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")])]),t._v(" B\n    "),v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("end")]),t._v("\n    \n    S1 "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" S2 "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" S3\n")])])]),v("h4",{attrs:{id:"阶段一-监督微调"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段一-监督微调"}},[t._v("#")]),t._v(" 阶段一：监督微调")]),t._v(" "),v("p",[t._v("这一步的目标是创建一个"),v("strong",[t._v("基础的多模态模型")]),t._v("。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("基础模型")]),t._v("：使用一个已经预训练好的、具备多模态能力的模型。例如，一个能够理解图像并根据文本指令生成图像描述的模型（如 BLIP-2、LLaVA），或者一个文生图模型（如 Stable Diffusion）。")]),t._v(" "),v("li",[v("strong",[t._v("微调数据")]),t._v("：收集一个高质量的、由人类精心编写的“指令-回答”对数据集。\n"),v("ul",[v("li",[v("strong",[t._v("示例")]),t._v("：（指令：“请描述这张图片中发生的核心事件”， 图片：[一张新闻图片]， 回答：“一场森林大火正在蔓延，消防员正在奋力灭火。”）")])])]),t._v(" "),v("li",[v("strong",[t._v("过程")]),t._v("：用这个数据集对基础模型进行监督学习。这使得模型初步学会遵循人类的指令格式，并生成相关、有用的内容。")])]),t._v(" "),v("p",[v("strong",[t._v("这一步之后，模型已经是个“好学生”了，但还不够“聪明”和“贴心”，不知道什么样的回答是更优质、更安全、更符合人类偏好的。")])]),t._v(" "),v("h4",{attrs:{id:"阶段二-训练奖励模型"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段二-训练奖励模型"}},[t._v("#")]),t._v(" 阶段二：训练奖励模型")]),t._v(" "),v("p",[t._v("这是 RLHF 的灵魂。我们训练一个独立的"),v("strong",[t._v("奖励模型")]),t._v("来充当“人类偏好”的代理。")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("数据收集")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("对于同一个多模态指令（例如，“根据这段描述画一幅画：‘一只戴着礼帽的柯基犬在月球上散步’”），让第一阶段微调后的模型生成多个不同的回答（例如，生成4张不同的图片）。")]),t._v(" "),v("li",[t._v("将这些回答（图片）成对地展示给人类标注员，让他们选择哪个更好。标注标准可以包括："),v("strong",[t._v("相关性、美观度、安全性、创造性、无害性")]),t._v("等。")]),t._v(" "),v("li",[t._v("这样就构建了一个庞大的"),v("strong",[t._v("偏好数据集")]),t._v("，格式为：（指令， 获胜回答， 失败回答）。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("模型训练")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("我们选择一个模型（通常结构与SFT模型类似）作为奖励模型。")]),t._v(" "),v("li",[t._v("训练目标很简单：对于数据中的每一对回答，确保奖励模型给“获胜”回答打的分远高于“失败”回答的打分。常用的损失函数是"),v("strong",[t._v("对比损失")]),t._v("。")])])])]),t._v(" "),v("p",[v("strong",[t._v("训练完成后，这个奖励模型就学会了如何根据人类的偏好，给任何模型输出打一个“质量分”。")])]),t._v(" "),v("h4",{attrs:{id:"阶段三-强化学习优化"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段三-强化学习优化"}},[t._v("#")]),t._v(" 阶段三：强化学习优化")]),t._v(" "),v("p",[t._v("这是最关键的迭代优化阶段。我们将SFT模型和奖励模型连接起来，形成一个强化学习循环。")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("设定")]),t._v("：")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("智能体")]),t._v("：第一阶段得到的SFT模型。现在它被称为"),v("strong",[t._v("策略模型")]),t._v("。")]),t._v(" "),v("li",[v("strong",[t._v("环境")]),t._v("：接收指令并生成回答的任务环境。")]),t._v(" "),v("li",[v("strong",[t._v("动作")]),t._v("：生成每一个词元（对于文本）或图像块（对于图像）。")]),t._v(" "),v("li",[v("strong",[t._v("奖励")]),t._v("：由第二阶段训练好的奖励模型提供。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("优化过程（以PPO算法为例）")]),t._v("：")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("生成")]),t._v("：策略模型接收一个指令，生成一个回答。")]),t._v(" "),v("li",[v("strong",[t._v("评分")]),t._v("：奖励模型对这个回答进行评分，给出一个主要的奖励信号。")]),t._v(" "),v("li",[v("strong",[t._v("约束")]),t._v("：为了防止模型在优化奖励时“走火入魔”（例如，为了得高分而生成无意义的乱码，或者忘记原有的语言能力），通常会加入一个"),v("strong",[t._v("KL散度惩罚")]),t._v("。这个惩罚项确保优化后的新策略模型与原始的SFT模型不会偏离太远。")]),t._v(" "),v("li",[v("strong",[t._v("更新")]),t._v("："),v("strong",[t._v("PPO")]),t._v(" 等强化学习算法根据“总奖励 = 奖励模型分 + β * KL惩罚”来计算最终奖励，然后更新策略模型的参数，使得它未来更有可能生成能获得高奖励的回答。")])])])]),t._v(" "),v("p",[t._v("这个过程会重复数百万甚至上亿次，模型在不断地“试错”和“接收反馈”中，逐渐将其行为与复杂、微妙的人类偏好对齐。")]),t._v(" "),v("h3",{attrs:{id:"mm-rlhf-面临的独特挑战"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#mm-rlhf-面临的独特挑战"}},[t._v("#")]),t._v(" MM-RLHF 面临的独特挑战")]),t._v(" "),v("p",[t._v("与纯文本的 RLHF 相比，MM-RLHF 更加复杂：")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("偏好标注更难、更主观")]),t._v("：判断两张图片哪张更好，可能比判断两段文字更难，因为涉及更多美学、风格等主观因素。标注成本极高。")]),t._v(" "),v("li",[v("strong",[t._v("奖励模型更难训练")]),t._v("：捕捉图像质量、艺术风格、与文本的一致性等细微差别，比判断文本质量更具挑战性。")]),t._v(" "),v("li",[v("strong",[t._v("生成过程的复杂性")]),t._v("：文生图等模型的生成过程是高度非自回归的，行动空间（像素或潜在空间）巨大且连续，这给强化学习优化带来了巨大困难。")]),t._v(" "),v("li",[v("strong",[t._v("评估困难")]),t._v("：如何量化评估生成模型是否“对齐”了，本身就是一个开放性问题。")])]),t._v(" "),v("h3",{attrs:{id:"应用实例"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#应用实例"}},[t._v("#")]),t._v(" 应用实例")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("Google 的 Imagen")]),t._v("： 在其论文中提到了使用类似 RLHF 的技术来优化图像质量和文本遵循度。")]),t._v(" "),v("li",[v("strong",[t._v("OpenAI 的 DALL-E 3")]),t._v("： 虽然未完全公开细节，但普遍认为它通过类似 MM-RLHF 的技术，极大地提升了对用户指令的遵循能力和生成内容的质量。")]),t._v(" "),v("li",[v("strong",[t._v("多模态助手")]),t._v("： 如 GPT-4V，其背后的对齐技术很可能也包含了 MM-RLHF 的变体，以确保模型生成的文本描述不仅准确，而且有用、无害。")])]),t._v(" "),v("p",[t._v("总结来说，"),v("strong",[t._v("MM-RLHF 是将RLHF的成功范式应用于多模态领域的关键技术。它通过“人类偏好标注 -> 训练奖励模型 -> 强化学习优化”这一套组合拳，让多模态AI模型变得更懂人心，生成更安全、更有用、更符合人类价值观的内容。")]),t._v(" 尽管挑战巨大，但它是通往下一代强大、可控、可靠的多模态AI的必经之路。")]),t._v(" "),v("p",[t._v("好的，这是一个非常核心的话题。多模态大模型处理视频输入是一个复杂但飞速发展的领域，其核心思想是 "),v("strong",[t._v("将高维、冗余的视频数据高效地压缩和编码成LLM能够理解的“语言”")]),t._v("。")]),t._v(" "),v("p",[t._v("下面我将从核心挑战、主流技术路线、具体流程和未来方向四个方面来详细解释。")]),t._v(" "),v("h3",{attrs:{id:"核心挑战"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#核心挑战"}},[t._v("#")]),t._v(" 核心挑战")]),t._v(" "),v("p",[t._v("与图像相比，处理视频的难点在于：")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("海量数据")]),t._v("：一秒30帧的1080p视频，数据量是单张图像的30倍。直接处理所有帧计算成本无法承受。")]),t._v(" "),v("li",[v("strong",[t._v("时间冗余")]),t._v("：连续帧之间内容高度相似，存在大量冗余信息。")]),t._v(" "),v("li",[v("strong",[t._v("时序建模")]),t._v("：模型必须理解帧与帧之间的时序关系，才能捕捉动作、因果和故事线。")]),t._v(" "),v("li",[v("strong",[t._v("计算瓶颈")]),t._v("：无论是视觉编码还是LLM理解，处理长序列视频 token 对硬件要求极高。")])]),t._v(" "),v("h3",{attrs:{id:"主流技术路线"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#主流技术路线"}},[t._v("#")]),t._v(" 主流技术路线")]),t._v(" "),v("p",[t._v("目前，没有单一的标准方法，但主流路线可以归结为以下几类，其演进过程如下图所示：")]),t._v(" "),v("div",{staticClass:"language-mermaid extra-class"},[v("pre",{pre:!0,attrs:{class:"language-mermaid"}},[v("code",[v("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("flowchart")]),t._v(" TD\n    A"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“视频输入<br>高维/冗余/含时序信息”]")]),t._v(" "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" B"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("{处理路线}")]),t._v("\n\n    B "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" C"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“路线一: 稀疏采样<br>与图像模型融合”]")]),t._v("\n    B "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" D"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“路线二: 时空编码<br>统一处理帧与时序”]")]),t._v("\n    B "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" E"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“路线三: 视频-语言<br>压缩表示”]")]),t._v("\n\n    C "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" F"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“代表模型<br>Video-LLaVA, LLaVA-NeXT”]")]),t._v("\n    D "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" G"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“代表模型<br>VideoChat, Video-ChatGPT”]")]),t._v("\n    E "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" H"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“代表模型<br>V-JEPA, LVM”]")]),t._v("\n\n    F "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" G "),v("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" H "),v("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" I"),v("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[“统一输入LLM<br>进行理解与生成”]")]),t._v("\n")])])]),v("h4",{attrs:{id:"路线一-稀疏采样-图像编码器-主流且实用"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#路线一-稀疏采样-图像编码器-主流且实用"}},[t._v("#")]),t._v(" 路线一：稀疏采样 + 图像编码器（主流且实用）")]),t._v(" "),v("p",[t._v("这是目前最流行、最实用的方法，许多开源模型（如 "),v("strong",[t._v("Video-LLaVA")]),t._v("、"),v("strong",[t._v("LLaVA-NeXT")]),t._v("）都采用此路线。")]),t._v(" "),v("ul",[v("li",[v("p",[v("strong",[t._v("思路")]),t._v("：既然视频帧冗余度高，那就不全部处理，只"),v("strong",[t._v("智能地选取少量关键帧")]),t._v("。")])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("具体做法")]),t._v("：")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("均匀采样")]),t._v("：最简单的方法，每隔N帧取一帧。")]),t._v(" "),v("li",[v("strong",[t._v("稀疏采样")]),t._v("：使用更复杂的策略（如场景变化检测）来选取信息量最大的帧。")]),t._v(" "),v("li",[v("strong",[t._v("处理")]),t._v("：将选取的每一帧都视为独立的图像，通过一个"),v("strong",[t._v("预训练的图像编码器")]),t._v("（如 "),v("strong",[t._v("CLIP的ViT")]),t._v("）进行编码，得到每个帧的视觉特征。")]),t._v(" "),v("li",[v("strong",[t._v("输入LLM")]),t._v("：将这些帧的视觉特征序列（可能加上一个可学习的帧位置嵌入）与文本指令一起输入给大语言模型。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("优点")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("充分利用了成熟的、强大的图像编码器（如CLIP）。")]),t._v(" "),v("li",[t._v("计算高效，实现相对简单。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("缺点")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("可能会丢失一些快速变化或细微的时序信息。")]),t._v(" "),v("li",[t._v("对长时间视频的支持仍然有限。")])])])]),t._v(" "),v("h4",{attrs:{id:"路线二-时空融合的视觉编码器-更专业"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#路线二-时空融合的视觉编码器-更专业"}},[t._v("#")]),t._v(" 路线二：时空融合的视觉编码器（更专业）")]),t._v(" "),v("p",[t._v("这种方法专门为视频设计，旨在同时捕捉"),v("strong",[t._v("空间")]),t._v("（每帧内的内容）和"),v("strong",[t._v("时间")]),t._v("（帧间运动）信息。")]),t._v(" "),v("ul",[v("li",[v("p",[v("strong",[t._v("思路")]),t._v("：扩展传统的图像编码器，使其能够处理时间维度的关系。")])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("具体做法")]),t._v("：")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("3D卷积")]),t._v("：早期方法，使用3D卷积核在空间和时间维度上同时滑动，但计算量巨大。")]),t._v(" "),v("li",[v("strong",[t._v("因子化时空注意力")]),t._v("：这是更现代的方法。在Vision Transformer中，交替使用"),v("strong",[t._v("空间注意力")]),t._v("（在一帧内计算patch之间的关系）和"),v("strong",[t._v("时间注意力")]),t._v("（在不同帧的相同空间位置patch之间计算关系）。")]),t._v(" "),v("li",[v("strong",[t._v("视频专用编码器")]),t._v("：使用在大型视频数据集（如Kinetics）上预训练的视频编码器，如 "),v("strong",[t._v("VideoCLIP")]),t._v("、"),v("strong",[t._v("InternVideo")]),t._v(" 或 "),v("strong",[t._v("TimeSformer")]),t._v("。这些模型能更好地理解动作。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("优点")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("显式地建模时序信息，对动作识别、行为分析等任务更有效。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("缺点")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("需要专门的预训练，计算成本更高。")]),t._v(" "),v("li",[t._v("模型更复杂，不易集成到现有的多模态框架中。")])])])]),t._v(" "),v("h4",{attrs:{id:"路线三-视频-语言的压缩表示-面向未来"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#路线三-视频-语言的压缩表示-面向未来"}},[t._v("#")]),t._v(" 路线三：视频-语言的压缩表示（面向未来）")]),t._v(" "),v("p",[t._v("这是一种更根本的变革，旨在创建一个高度压缩的、抽象的视频表示。")]),t._v(" "),v("ul",[v("li",[v("p",[v("strong",[t._v("思路")]),t._v("：不直接处理像素，而是先学习一个视频的“语义概念”潜在空间。")])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("具体做法")]),t._v("：")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("视觉词汇")]),t._v("：像构建文本词汇表一样，构建一个“视觉词汇表”。将视频片段或patches映射到离散的token序列。")]),t._v(" "),v("li",[t._v("** masked建模训练**：在大量未标注视频上，以类似BERT的方式训练模型，通过预测被mask掉的视觉token来学习视频的时空结构。"),v("strong",[t._v("Google的V-JEPA")]),t._v("和"),v("strong",[t._v("Meta的LVM")]),t._v(" 是这方面的先驱。")]),t._v(" "),v("li",[v("strong",[t._v("输入LLM")]),t._v("：将视频转化为一个离散的token序列（类似于文本），然后直接输入给LLM。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("优点")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("极度高效，可以将很长的视频压缩成较短的token序列。")]),t._v(" "),v("li",[t._v("真正将视频和文本在“符号”层面统一起来，与LLM的配合更自然。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("缺点")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("技术非常前沿，还不成熟。")]),t._v(" "),v("li",[t._v("可能会丢失一些低层次的视觉细节。")])])])]),t._v(" "),v("h3",{attrs:{id:"一个典型模型的处理流程-以路线一为例"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#一个典型模型的处理流程-以路线一为例"}},[t._v("#")]),t._v(" 一个典型模型的处理流程（以路线一为例）")]),t._v(" "),v("p",[t._v("以询问一个模型“视频里的人在做什么运动？”为例：")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("视频预处理")]),t._v("：将原始视频解码为帧序列，并调整分辨率。")]),t._v(" "),v("li",[v("strong",[t._v("帧采样")]),t._v("：使用均匀采样或智能采样，选出K个关键帧（例如，K=8）。")]),t._v(" "),v("li",[v("strong",[t._v("视觉编码")]),t._v("：\n"),v("ul",[v("li",[t._v("将每一帧输入到CLIP的ViT编码器。")]),t._v(" "),v("li",[t._v("ViT将每张图像切分成N个patch，并输出一个视觉特征序列 "),v("code",[t._v("[V1, V2, ..., VN]")]),t._v("（对于每一帧）。")]),t._v(" "),v("li",[t._v("现在，我们有 "),v("code",[t._v("K x N")]),t._v(" 个视觉token。")])])]),t._v(" "),v("li",[v("strong",[t._v("投影与融合")]),t._v("：\n"),v("ul",[v("li",[t._v("通过一个可训练的投影层（通常是MLP），将这些视觉token映射到与LLM的文本嵌入空间相同的维度。")]),t._v(" "),v("li",[t._v("为了区分不同帧，可能会给每个帧的token加上一个“帧位置嵌入”。")])])]),t._v(" "),v("li",[v("strong",[t._v("LLM理解与生成")]),t._v("：\n"),v("ul",[v("li",[t._v("将处理后的视觉token序列与文本指令的token序列拼接在一起，形成最终的输入序列："),v("code",[t._v("[V1, V2, ..., V_K*N, T1, T2, ..., T_M]")]),t._v("。")]),t._v(" "),v("li",[t._v("输入给LLM。LLM会基于视觉上下文和文本指令，自回归地生成答案，例如：“他正在打篮球。”")])])])]),t._v(" "),v("h3",{attrs:{id:"总结与未来方向"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结与未来方向"}},[t._v("#")]),t._v(" 总结与未来方向")]),t._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[t._v("特性")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("稀疏采样+图像编码器")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("时空融合编码器")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("视频-语言压缩表示")])])]),t._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("成熟度")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("高")]),t._v("（当前主流）")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("中")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("低")]),t._v("（前沿研究）")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("时序理解")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("弱")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("强")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("潜在强")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("计算效率")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("高")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("中-低")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("极高")]),t._v("（潜力）")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("实现难度")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("低")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("中")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("高")])])])])]),t._v(" "),v("p",[v("strong",[t._v("未来方向")]),t._v("：")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("更高效的采样策略")]),t._v("：如何从长视频中智能选取最具信息量的片段。")]),t._v(" "),v("li",[v("strong",[t._v("统一架构")]),t._v("：像"),v("strong",[t._v("Mamba")]),t._v("这样的状态空间模型，可能比Transformer更高效地处理极长的视频序列。")]),t._v(" "),v("li",[v("strong",[t._v("世界模型")]),t._v("：让模型不仅能理解视频，还能预测接下来会发生什么，这是通向更通用AI的关键一步。")])]),t._v(" "),v("p",[t._v("总而言之，多模态大模型通过 "),v("strong",[t._v("“降维”和“抽象”")]),t._v(" 来处理视频：要么"),v("strong",[t._v("稀疏地看")]),t._v("，要么"),v("strong",[t._v("用更聪明的编码器看")]),t._v("，要么"),v("strong",[t._v("学会用另一种‘语言’来描述它")]),t._v("，最终目的是将浩瀚的视频宇宙，翻译成大语言模型能够理解的“星辰文”。")])])}),[],!1,null,null,null);v.default=s.exports}}]);