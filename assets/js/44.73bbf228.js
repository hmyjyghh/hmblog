(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{526:function(t,s,n){"use strict";n.r(s);var a=n(3),e=Object(a.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"langchain-实现构造数据集"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#langchain-实现构造数据集"}},[t._v("#")]),t._v(" langchain 实现构造数据集")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[t._v("response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" agent_4o\\\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("role"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("column_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("instruct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"1. 分析{text}中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"result"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"speaker"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"str"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"为发言人生成的中文姓名(不含角色)"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"content"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"str"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"发言的详细内容"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"is_fraud"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"boolean"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"关于{content}中是否存在明确具体、不可辩驳的欺诈内容"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nprint_json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"将上述写法改造为langchain-的写法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#将上述写法改造为langchain-的写法"}},[t._v("#")]),t._v(" 将上述写法改造为langchain 的写法")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用LCEL 链式调用")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output_parsers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" JsonOutputParser\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prompts "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ChatPromptTemplate\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建输出解析器")]),t._v("\noutput_parser "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" JsonOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建提示词模板")]),t._v("\nprompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"system"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家。你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n    \n要求：\n1. 分析案发过程，识别诈骗者和受害者角色，为每个角色生成中文姓名\n2. 以对话形式循序渐进展开案发过程\n3. 诈骗者发言要伪装得正规可信\n4. 分析每条发言是否存在明确欺诈内容"""')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"请分析以下诈骗案例：{text}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"请以指定JSON格式输出：{format_instructions}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建链")]),t._v("\nchain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" llm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" output_parser\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用链")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_with_chain")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"format_instructions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" output_parser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_format_instructions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" Exception "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"分析失败: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用示例")]),t._v("\ntext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("column_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" analyze_fraud_with_chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"主要变化说明"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#主要变化说明"}},[t._v("#")]),t._v(" 主要变化说明：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("角色定义")]),t._v("：从 "),s("code",[t._v(".role()")]),t._v(" 改为 SystemMessage")]),t._v(" "),s("li",[s("strong",[t._v("输入处理")]),t._v("：从 "),s("code",[t._v(".input()")]),t._v(" 改为 HumanMessage 中的变量替换")]),t._v(" "),s("li",[s("strong",[t._v("指令")]),t._v("：从 "),s("code",[t._v(".instruct()")]),t._v(" 改为提示词中的具体要求列表")]),t._v(" "),s("li",[s("strong",[t._v("输出格式")]),t._v("：从 "),s("code",[t._v(".output()")]),t._v(" 改为在提示词中描述JSON格式")]),t._v(" "),s("li",[s("strong",[t._v("执行")]),t._v("：从 "),s("code",[t._v(".start()")]),t._v(" 改为 "),s("code",[t._v(".invoke()")])])]),t._v(" "),s("hr"),t._v(" "),s("blockquote",[s("p",[t._v("实操遇到的问题记录")])]),t._v(" "),s("div",{staticClass:"language-Text extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n请以以下JSON格式输出结果：\n{{\n    "result": [\n        {{\n            "speaker": "发言人的中文姓名(不含角色)",\n            "content": "发言的详细内容", \n            "is_fraud": "true或false，表示该发言中是否存在明确具体、不可辩驳的欺诈内容"\n        }}\n    ]\n}}\n\n\n\n我用这个提示词 生成的json 数据里，有一条这样子的错误格式，\n\'content\': \\ "请您放心，这是标准的关闭流程。为了更高效地指导您，建议您下载\'云视通\'APP，加入我们的视频会议，我会远程指导您完成所有操作。\\"\n\n带上了 \\" \\"  这种转义词，你可以通过修改提示词的形式，帮我修正这个问题吗\n')])])]),s("p",[t._v("是的，可以通过修改提示词来解决这个问题。问题在于模型在生成 JSON 时对引号进行了转义。以下是修正后的提示词：")]),t._v(" "),s("h2",{attrs:{id:"修正后的提示词"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#修正后的提示词"}},[t._v("#")]),t._v(" 修正后的提示词")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("system_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n重要格式要求：\n- 所有字符串值都使用标准的双引号，不要使用转义字符\n- 确保JSON格式完全正确，可以直接被解析\n- 内容中的引号保持原样，不要转义\n\n请以以下JSON格式输出结果：\n{\n    "result": [\n        {\n            "speaker": "发言人的中文姓名",\n            "content": "发言的详细内容，包含正常引号而不是转义引号", \n            "is_fraud": true\n        }\n    ]\n}"""')]),t._v("\n\nhuman_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""请分析以下诈骗案例文本：\n\n{text}"""')]),t._v("\n")])])]),s("h2",{attrs:{id:"更严格的版本-强调格式要求"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#更严格的版本-强调格式要求"}},[t._v("#")]),t._v(" 更严格的版本（强调格式要求）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("system_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n严格的格式要求：\n- 输出必须是完全有效的JSON格式\n- 所有字符串使用双引号，不要使用反斜杠转义\n- 内容中的单引号、双引号保持原样，不要转义\n- 确保布尔值是 true 或 false（小写）\n- JSON格式必须能够直接被 json.loads() 解析\n\n请输出纯净的JSON，不要包含任何其他文字或markdown标记：\n\n{\n    "result": [\n        {\n            "speaker": "姓名",\n            "content": "对话内容，引号保持原样不要转义",\n            "is_fraud": false\n        }\n    ]\n}"""')]),t._v("\n\nhuman_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""请分析以下诈骗案例文本：\n\n{text}"""')]),t._v("\n")])])]),s("h2",{attrs:{id:"使用-langchain-的输出解析器-推荐"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#使用-langchain-的输出解析器-推荐"}},[t._v("#")]),t._v(" 使用 Langchain 的输出解析器（推荐）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output_parsers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" JsonOutputParser\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prompts "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ChatPromptTemplate\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建输出解析器")]),t._v("\noutput_parser "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" JsonOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建提示词")]),t._v("\nprompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"system"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家。任务是将诈骗案发过程以口语对话形式还原。\n\n要求：\n1. 识别诈骗者和受害者角色，生成中文姓名\n2. 以对话形式展开案发过程\n3. 诈骗者发言要伪装正规\n4. 分析每条发言是否存在欺诈内容\n\n格式要求：\n- 输出纯净JSON，无需markdown包装\n- 字符串引号不要转义\n- 确保JSON可直接解析"""')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"分析案例：{text}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"格式要求：{format_instructions}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建链")]),t._v("\nchain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" llm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" output_parser\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_case")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"format_instructions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" output_parser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_format_instructions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" Exception "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"分析失败: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n")])])]),s("h2",{attrs:{id:"关键修改点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#关键修改点"}},[t._v("#")]),t._v(" 关键修改点：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("明确禁止转义")]),t._v('：在提示词中强调"不要使用转义字符"')]),t._v(" "),s("li",[s("strong",[t._v("具体示例")]),t._v("：在示例中展示正确的引号用法")]),t._v(" "),s("li",[s("strong",[t._v("格式强调")]),t._v('：要求输出"纯净的JSON"')]),t._v(" "),s("li",[s("strong",[t._v("使用解析器")]),t._v("：通过 "),s("code",[t._v("JsonOutputParser")]),t._v(" 自动处理格式问题")])]),t._v(" "),s("p",[t._v("这样修改后，模型会生成更干净的 JSON 格式，避免引号转义问题。")]),t._v(" "),s("hr"),t._v(" "),s("ul",[s("li",[t._v("遇到的问题2：")])]),t._v(" "),s("div",{staticClass:"language-Text extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v('system_template = """你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n严格的JSON格式要求：\n- 必须使用双引号包围所有字符串键和值\n- 布尔值必须使用小写 true 或 false\n- 不要使用单引号\n- 确保是有效的JSON格式\n\n输出示例：\n{\n    "result": [\n        {\n            "speaker": "张三",\n            "content": "对话内容",\n            "is_fraud": true\n        }\n    ]\n}\n\n重要：直接输出纯净JSON，不要任何其他文字！"""\n\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n\n\n# 组合成完整的 ChatPromptTemplate\n      chat_prompt = ChatPromptTemplate.from_messages([\n        system_message_prompt,\n        human_message_prompt\n      ])\n\n      # 使用chain\n      practice_chain = (\n        chat_prompt\n        | llm\n        | JsonOutputParser()\n      )\n\n\n生成对话时出错: \'Input to ChatPromptTemplate is missing variables {\\\'\\\\n    "result"\\\'}.  Expected: [\\\'\\\\n    "result"\\\', \\\'text\\\'] Received: [\\\'text\\\']\\nNote: if you intended {\\n    "result"} to be part of the string and not a variable, please escape it with double curly braces like: \\\'{{\\n    "result"}}\\\'.\\nFor troubleshooting, visit:\n')])])]),s("p",[t._v("这个错误是因为在 "),s("code",[t._v("system_template")]),t._v(" 中使用了变量占位符，但实际调用时没有提供对应的变量值。从错误信息看，系统模板期望收到 "),s("code",[t._v('"result"')]),t._v(" 和 "),s("code",[t._v("text")]),t._v(" 变量，但实际只收到了 "),s("code",[t._v("text")]),t._v(" 变量。")]),t._v(" "),s("p",[t._v("问题出现在 "),s("code",[t._v("system_template")]),t._v(" 中的变量引用。让我提供一个修正版本：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("system_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n严格的JSON格式要求：\n- 必须使用双引号包围所有字符串键和值\n- 布尔值必须使用小写 true 或 false\n- 不要使用单引号\n- 确保是有效的JSON格式\n\n输出示例：\n{{\n    "result": [\n        {{\n            "speaker": "张三",\n            "content": "对话内容",\n            "is_fraud": true\n        }}\n    ]\n}}\n\n重要：直接输出纯净JSON，不要任何其他文字！"""')]),t._v("\n\nsystem_message_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SystemMessagePromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("system_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建 human message prompt（这里需要定义）")]),t._v("\nhuman_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{text}"')]),t._v("\nhuman_message_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" HumanMessagePromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("human_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 组合成完整的 ChatPromptTemplate")]),t._v("\nchat_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    system_message_prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    human_message_prompt\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用chain")]),t._v("\npractice_chain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    chat_prompt\n    "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" llm\n    "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" JsonOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("主要修改：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("转义输出示例中的花括号")]),t._v("：将 "),s("code",[t._v("{")]),t._v(" 改为 "),s("code",[t._v("{{")]),t._v("，"),s("code",[t._v("}")]),t._v(" 改为 "),s("code",[t._v("}}")]),t._v("，这样它们就不会被识别为变量占位符")]),t._v(" "),s("li",[s("strong",[t._v("明确 human template")]),t._v("：定义了 "),s("code",[t._v("human_template")]),t._v(" 只包含 "),s("code",[t._v("{text}")]),t._v(" 变量")])]),t._v(" "),s("p",[t._v("调用时应该这样使用：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用chain")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" practice_chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"用户提供的诈骗案例描述文本..."')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("这样就能避免变量缺失的错误了。")]),t._v(" "),s("h3",{attrs:{id:"将agently-框架的写法改为-langchain-写法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#将agently-框架的写法改为-langchain-写法"}},[t._v("#")]),t._v(" 将Agently 框架的写法改为 langchain 写法")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("generate_dialog")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("agent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" agent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_yaml_prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        yaml "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        variables"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"input"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"language"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'chinese'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果遇到markdown格式的json，则尝试用上面封装的remove_markdown_boundary来二次处理.")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("remove_markdown_boundary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'result'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nprint_json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("generate_dialog"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("agent_4o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("column_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 可以将这个使用Agently 框架的代码，改造为使用  langchain agent 吗")]),t._v("\n")])])]),s("p",[t._v("当然可以。以下是使用 LangChain 框架改造后的代码：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agents "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AgentExecutor\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prompts "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" PromptTemplate\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schema "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BaseOutputParser\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" re\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JSONOutputParser")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""自定义JSON输出解析器"""')]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 清理可能的markdown标记")]),t._v("\n        cleaned_text "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sub"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'```json\\s*|\\s*```'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cleaned_text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("remove_markdown_boundary")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""移除markdown边界标记"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sub"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'```json\\s*|\\s*```'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("generate_dialog_langchain")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("agent_executor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" AgentExecutor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""使用LangChain生成对话"""')]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建提示词模板")]),t._v("\n    prompt_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        input_variables"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"input"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"language"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        template"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n严格的格式要求：\n- 输出必须是完全有效的JSON格式\n- 所有字符串使用双引号，不要使用反斜杠转义\n- 内容中的单引号、双引号保持原样，不要转义\n- 确保布尔值是 true 或 false（小写）\n- JSON格式必须能够直接被 json.loads() 解析\n\n请按照以下JSON格式输出结果：\n{{\n    "result": [\n        {{\n            "speaker": "发言人的中文姓名(不含角色)",\n            "content": "发言的详细内容", \n            "is_fraud": "true或false，表示该发言中是否存在明确具体、不可辩驳的欺诈内容"\n        }}\n    ]\n}}\n\n输入文本：\n{input}\n\n语言：{language}"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 填充模板")]),t._v("\n    formatted_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        language"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'chinese'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行agent")]),t._v("\n    response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" agent_executor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("formatted_prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 处理响应")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            cleaned_response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" remove_markdown_boundary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            parsed_response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cleaned_response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" parsed_response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'result'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JSONDecodeError"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果解析失败，尝试直接返回原始响应")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"JSON解析失败，返回原始响应"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" response\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" response\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用示例")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 假设你已经配置好了LangChain agent")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# agent_executor = initialize_your_agent()  # 你需要根据实际情况初始化agent")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从dataset中获取数据")]),t._v("\n    text_content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("column_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 生成对话")]),t._v("\n    result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" generate_dialog_langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("agent_executor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 打印结果")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indent"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    main"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("主要改动说明：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("使用 LangChain 的 PromptTemplate")]),t._v("：替代了原来的 YAML 模板加载方式")]),t._v(" "),s("li",[s("strong",[t._v("自定义 JSONOutputParser")]),t._v("：用于处理可能的 markdown 格式和 JSON 解析")]),t._v(" "),s("li",[s("strong",[t._v("AgentExecutor")]),t._v("：使用 LangChain 的标准 agent 执行器")]),t._v(" "),s("li",[s("strong",[t._v("保持相同的业务逻辑")]),t._v("：包括 markdown 清理、JSON 解析等核心功能")])]),t._v(" "),s("p",[t._v("注意事项：")]),t._v(" "),s("ul",[s("li",[t._v("你需要根据实际情况初始化 "),s("code",[t._v("agent_executor")]),t._v("，这取决于你使用的具体 LLM 和工具配置")]),t._v(" "),s("li",[s("code",[t._v("remove_markdown_boundary")]),t._v(" 函数保持不变，用于清理响应中的 markdown 标记")]),t._v(" "),s("li",[t._v("错误处理机制得以保留，确保代码的健壮性")])]),t._v(" "),s("p",[t._v("使用前请确保已安装 LangChain：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[t._v("pip "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" langchain\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);