(window.webpackJsonp=window.webpackJsonp||[]).push([[105],{549:function(t,_,v){"use strict";v.r(_);var a=v(3),n=Object(a.a)({},(function(){var t=this,_=t._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h2",{attrs:{id:"模型量化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#模型量化"}},[t._v("#")]),t._v(" 模型量化")]),t._v(" "),_("h3",{attrs:{id:"weight-only-量化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#weight-only-量化"}},[t._v("#")]),t._v(" weight-only 量化")]),t._v(" "),_("blockquote",[_("p",[t._v("weight-only 量化是 LLM 轻量化部署的核心技术")])]),t._v(" "),_("h3",{attrs:{id:"一、先理清核心-weight-only量化是什么"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#一、先理清核心-weight-only量化是什么"}},[t._v("#")]),t._v(" 一、先理清核心：weight-only量化是什么？")]),t._v(" "),_("p",[t._v("weight-only量化的核心是"),_("strong",[t._v("只对模型的“权重（Weight）”做低比特量化（如INT4/INT8），而“激活值（Activation）”仍保留FP16/BF16等高精度")]),t._v("。")]),t._v(" "),_("p",[t._v("这么做的原因是："),_("code",[t._v("LLM的权重占模型体积的90%以上")]),t._v("（如7B模型权重约14GB FP16），仅量化权重就能大幅压缩体积；而激活值对精度更敏感，保留高精度可减少推理误差。")]),t._v(" "),_("blockquote",[_("p",[t._v("不同数据类型的 “精度（尾数位）” 和 “动态范围（指数位）”，直接决定了量化时的压缩比和精度损失。")])]),t._v(" "),_("table",[_("thead",[_("tr",[_("th",[t._v("数据类型")]),t._v(" "),_("th",[t._v("符号位")]),t._v(" "),_("th",[t._v("指数位")]),t._v(" "),_("th",[t._v("尾数位")]),t._v(" "),_("th",[t._v("动态范围（近似）")]),t._v(" "),_("th",[t._v("精度特点")]),t._v(" "),_("th",[t._v("适用场景")])])]),t._v(" "),_("tbody",[_("tr",[_("td",[t._v("FP32")]),t._v(" "),_("td",[t._v("1")]),t._v(" "),_("td",[t._v("8")]),t._v(" "),_("td",[t._v("23")]),t._v(" "),_("td",[t._v("±10^-38 ~ ±10^38")]),t._v(" "),_("td",[t._v("精度最高，范围最大")]),t._v(" "),_("td",[t._v("模型训练、高精度计算")])]),t._v(" "),_("tr",[_("td",[t._v("FP16")]),t._v(" "),_("td",[t._v("1")]),t._v(" "),_("td",[t._v("5")]),t._v(" "),_("td",[t._v("10")]),t._v(" "),_("td",[t._v("±6×10^-8 ~ ±6×10^4")]),t._v(" "),_("td",[t._v("精度中等，范围较小")]),t._v(" "),_("td",[t._v("中小模型训练/推理、混合精度")])]),t._v(" "),_("tr",[_("td",[t._v("BF16")]),t._v(" "),_("td",[t._v("1")]),t._v(" "),_("td",[t._v("8")]),t._v(" "),_("td",[t._v("7")]),t._v(" "),_("td",[t._v("±10^-38 ~ ±10^38")]),t._v(" "),_("td",[t._v("精度低，范围与FP32一致")]),t._v(" "),_("td",[t._v("大模型训练（避免溢出）")])]),t._v(" "),_("tr",[_("td",[t._v("FP8")]),t._v(" "),_("td",[t._v("1")]),t._v(" "),_("td",[t._v("5/4")]),t._v(" "),_("td",[t._v("2/3")]),t._v(" "),_("td",[t._v("±10^-15 ~ ±10^19")]),t._v(" "),_("td",[t._v("精度极低，范围较大")]),t._v(" "),_("td",[t._v("超大规模模型轻量化训练")])]),t._v(" "),_("tr",[_("td",[t._v("INT8")]),t._v(" "),_("td",[t._v("1")]),t._v(" "),_("td",[t._v("-")]),t._v(" "),_("td",[t._v("7")]),t._v(" "),_("td",[t._v("-128 ~ 127")]),t._v(" "),_("td",[t._v("无指数位，范围固定、精度低")]),t._v(" "),_("td",[t._v("权重/激活量化（通用）")])]),t._v(" "),_("tr",[_("td",[t._v("INT4")]),t._v(" "),_("td",[t._v("1")]),t._v(" "),_("td",[t._v("-")]),t._v(" "),_("td",[t._v("3")]),t._v(" "),_("td",[t._v("-8 ~ 7")]),t._v(" "),_("td",[t._v("范围极小，精度极低")]),t._v(" "),_("td",[t._v("仅适合权重量化（需算法优化）")])])])]),t._v(" "),_("h3",{attrs:{id:"量化的常用技术"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#量化的常用技术"}},[t._v("#")]),t._v(" 量化的常用技术")]),t._v(" "),_("h4",{attrs:{id:"训练后量化-ptq"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#训练后量化-ptq"}},[t._v("#")]),t._v(" 训练后量化(PTQ)")]),t._v(" "),_("h4",{attrs:{id:"量化感知训练-qat"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#量化感知训练-qat"}},[t._v("#")]),t._v(" 量化感知训练（QAT）")]),t._v(" "),_("h4",{attrs:{id:"混合精度量化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#混合精度量化"}},[t._v("#")]),t._v(" 混合精度量化")]),t._v(" "),_("h3",{attrs:{id:"int8-量化的工作流程"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#int8-量化的工作流程"}},[t._v("#")]),t._v(" Int8 量化的工作流程")]),t._v(" "),_("blockquote",[_("p",[t._v("前置知识： 量化就是一种降低数据表示的精度的过程")])]),t._v(" "),_("p",[t._v("整个量化过程都和数据类型的转换息息相关。")]),t._v(" "),_("p",[t._v("量化的目标，就是将 FP32 的权重和激活值，映射到 "),_("strong",[t._v("低精度")]),t._v(" 的表示上，例如 "),_("strong",[t._v("8位整数（Int8）")]),t._v(" 甚至 "),_("strong",[t._v("4位整数（Int4）")]),t._v("。")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("Int8")]),t._v("：每个参数仅占1个字节，模型体积减小约75%，同时硬件对整型计算有大量优化。")]),t._v(" "),_("li",[_("strong",[t._v("Int4")]),t._v("：每个参数仅占0.5个字节，模型体积减小约87.5%，是极致的压缩。")])]),t._v(" "),_("h4",{attrs:{id:"步骤一-量化-前向推理时"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#步骤一-量化-前向推理时"}},[t._v("#")]),t._v(" 步骤一：量化（前向推理时）")]),t._v(" "),_("p",[t._v("将FP32的权重和激活值，通过上述公式转换为Int8。")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("量化公式")]),t._v("：\n[\n\\text{Int8 Value} = \\text{clamp}(\\text{round}(\\frac{\\text{Float32 Value}}{\\text{Scale}}) + \\text{Zero Point}, -128, 127)\n]\n"),_("code",[t._v("round()")]),t._v(" 是取整函数，"),_("code",[t._v("clamp()")]),t._v(" 是钳位函数，确保结果在Int8范围内。")])]),t._v(" "),_("h4",{attrs:{id:"步骤二-反量化-可选-用于特定计算或输出"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#步骤二-反量化-可选-用于特定计算或输出"}},[t._v("#")]),t._v(" 步骤二：反量化（可选，用于特定计算或输出）")]),t._v(" "),_("p",[t._v("将Int8的计算结果转换回FP32，以便进行后续的浮点计算或作为最终输出。")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("反量化公式")]),t._v("：\n[\n\\text{Dequantized Float32} = \\text{Scale} \\times (\\text{Int8 Value} - \\text{Zero Point})\n]")])]),t._v(" "),_("h4",{attrs:{id:"步骤三-int8矩阵乘法-核心加速点"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#步骤三-int8矩阵乘法-核心加速点"}},[t._v("#")]),t._v(" 步骤三：Int8矩阵乘法（核心加速点）")]),t._v(" "),_("p",[t._v("这是量化技术能加速的关键。我们想计算：\n[\nY = X \\cdot W\n]\n其中 (X) 是输入，(W) 是权重。")]),t._v(" "),_("ol",[_("li",[t._v("将 FP32 的 (X) 和 (W) 分别量化为 Int8 的 (X_q) 和 (W_q)。")]),t._v(" "),_("li",[t._v("在整数计算单元上执行高效的 "),_("strong",[t._v("Int8 矩阵乘法")]),t._v("，得到 Int32 的结果 (Y_{q}^{int32})。\n[\nY_{q}^{int32} = X_q \\cdot W_q\n\\]\n"),_("em",[t._v("为什么是Int32？")]),t._v(" 因为两个Int8数相乘，结果的范围是Int16，但多个结果累加后很容易超出Int16的范围，所以用Int32来存放累加结果。")]),t._v(" "),_("li",[t._v("将 Int32 的结果反量化为 FP32。\n[\nY_{fp32} = S_x \\cdot S_w \\cdot Y_{q}^{int32} + \\text{Bias Terms (涉及Zero Point)}\n]\n这里的公式会比基本的反量化复杂一些，因为它融合了输入和权重的Scale和Zero Point。")])]),t._v(" "),_("p",[t._v("现代的CPU/GPU/DSP/NPU通常都有专门优化的 "),_("strong",[t._v("Int8/Int4 向量指令（如Intel VNNI, ARM DOT）")]),t._v("，执行 "),_("code",[t._v("X_q · W_q")]),t._v(" 的速度远超 "),_("code",[t._v("X_fp32 · W_fp32")]),t._v("，从而实现了巨大的加速。")]),t._v(" "),_("hr")])}),[],!1,null,null,null);_.default=n.exports}}]);