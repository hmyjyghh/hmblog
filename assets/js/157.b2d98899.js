(window.webpackJsonp=window.webpackJsonp||[]).push([[157],{600:function(t,s,a){"use strict";a.r(s);var e=a(3),n=Object(e.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AutoModelForCausalLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AutoModelForSeq2SeqLM"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" DataCollatorForSeq2Seq"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Seq2SeqTrainingArguments"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Seq2SeqTrainer\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这些是transformers库中的一些常用类和函数，它们可以帮助你加载预训练模型、处理数据、训练模型等。下面是一些简单的示例代码，展示了如何使用这些类和函数：")]),t._v("\n\n加载预训练模型：\n\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpt2"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 问题1： 我怎么知道什么模型该用哪个函数？ 不同NLP任务，用不同的模型，可以问deepseek")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 问题2: 为什么要给随机种子？ 是为了给GLM模型去做对比")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 问题3: 模型的评估，使用 BLEU  或者 Rouge")]),t._v("\n创建评估函数\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" rouge_chinese "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Rouge\nrouge "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Rouge"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("compute_metrics")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("eval_pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  predictions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" eval_pred\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 解码， 数据不止一条，所以用batch_decode 批量解码")]),t._v("\n  decoded_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch_decode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skip_special_tokens"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对 labels 进行处理后，再解码")]),t._v("\n  labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("where"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_token_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  decoded_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch_decode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skip_special_tokens"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# decoded_preds = [pred.strip() for pred in decoded_preds]")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# decoded_labels = [label.strip() for label in decoded_labels]")]),t._v("\n  decoded_preds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pred "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" decoded_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  decoded_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" label "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" decoded_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算ROUGE分数")]),t._v("\n  rouge_scores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rouge"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("decoded_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" decoded_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" avg"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 首先加载预训练模型和tokenizer")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算ROUGE分数")]),t._v("\n  rouge_scores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rouge"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_scores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("decoded_preds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" decoded_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" avg"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ol",[s("li",[t._v("先处理输入部分")]),t._v(" "),s("li",[t._v("再处理输出部分  是那个labels")]),t._v(" "),s("li",[t._v('tokenizer.decode(tokenizer_ds["train"][0]["input_ids"])')])]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 首尾加了标记的输出结果")]),t._v("\ntokenizer_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"你好，我是GLM-130B，请问我有什么问题可以帮您解答？"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pt"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenizer_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"input_ids"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("a",{attrs:{href:"https://space.bilibili.com/21060026/lists/1357748?type=season",target:"_blank",rel:"noopener noreferrer"}},[t._v("实战视频"),s("OutboundLink")],1)]),t._v(" "),s("ul",[s("li",[t._v("看这里面的： 实战演练之文本摘要（T5） 这一章节")]),t._v(" "),s("li",[t._v("基于字来做 or 基于词来做")]),t._v(" "),s("li",[t._v("只要是：能从文本中生成另一段文本的，都可以用这种方式来做")]),t._v(" "),s("li",[t._v("比如文本摘要，机器翻译")])]),t._v(" "),s("h2",{attrs:{id:"dataset"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dataset"}},[t._v("#")]),t._v(" Dataset")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" datasets "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 和")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset\n\n区别\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 和 torch.utils.data.Dataset 类似，但是这个类是专门为transformers库设计的，可以更方便地处理文本数据。")]),t._v("\n")])])]),s("p",[t._v("是的，你说得对。当你使用 "),s("code",[t._v("datasets")]),t._v(" 库的 "),s("code",[t._v("load_from_disk")]),t._v(" 函数时，目标目录（在你的例子中是 "),s("code",[t._v("./nlpcc")]),t._v("）"),s("strong",[t._v("必须")]),t._v("包含由 "),s("code",[t._v("save_to_disk")]),t._v(" 方法创建的那些特定文件，其中最重要的两个就是：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("dataset_info.json")])]),t._v(" "),s("li",[s("code",[t._v("state.json")])])]),t._v(" "),s("h2",{attrs:{id:"_2-torch-utils-data-dataset-需要配合dataloader-使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-torch-utils-data-dataset-需要配合dataloader-使用"}},[t._v("#")]),t._v(" 2. torch.utils.data.Dataset 需要配合"),s("code",[t._v("DataLoader")]),t._v(" 使用")])])}),[],!1,null,null,null);s.default=n.exports}}]);