(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{553:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"辅助理解transformer重要核心知识"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#辅助理解transformer重要核心知识"}},[t._v("#")]),t._v(" 辅助理解"),s("code",[t._v("Transformer")]),t._v("重要核心知识")]),t._v(" "),s("h2",{attrs:{id:"_1-多头注意力机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-多头注意力机制"}},[t._v("#")]),t._v(" 1. 多头注意力机制")]),t._v(" "),s("p",[s("strong",[t._v("多头注意力机制、自注意力机制、交叉注意力机制是相互正交的概念，它们之间是组合关系，而不是包含关系。")])]),t._v(" "),s("p",[t._v("我们可以从两个维度来理解它们：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("结构维度（怎么算）")]),t._v("："),s("strong",[t._v("单头 vs. 多头")])])]),t._v(" "),s("ul",[s("li",[t._v("这关心的是"),s("strong",[t._v("模型容量")]),t._v("，是用一组还是多组权重来计算注意力。")])]),t._v(" "),s("ol",{attrs:{start:"2"}},[s("li",[s("strong",[t._v("功能维度（算什么）")]),t._v("："),s("strong",[t._v("自注意力 vs. 交叉注意力")])])]),t._v(" "),s("ul",[s("li",[t._v("这关心的是"),s("strong",[t._v("数据来源")]),t._v("，是计算序列内部的关系还是两个序列之间的关系。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"图解它们的关系"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#图解它们的关系"}},[t._v("#")]),t._v(" 图解它们的关系")]),t._v(" "),s("p",[t._v("它们之间的关系更像一个组合矩阵，而不是层级结构：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}}),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("自注意力 (Self-Attention)")]),t._v(" "),s("br"),t._v(" (Q, K, V 来自"),s("strong",[t._v("同一个")]),t._v("序列)")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("交叉注意力 (Cross-Attention)")]),t._v(" "),s("br"),t._v(" (Q 来自序列 A, K, V 来自序列 B)")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("单头 (Single-Head)")]),t._v(" "),s("br"),t._v(" (一套QKV投影)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("单头自注意力")]),t._v(" "),s("br"),t._v(" (e.g., 原始Transformer论文中的基础形式)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("单头交叉注意力")]),t._v(" "),s("br"),t._v(" (e.g., 解码器层中连接编码器输出)")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("多头 (Multi-Head)")]),t._v(" "),s("br"),t._v(" (多套QKV投影)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("多头自注意力")]),t._v(" "),s("br"),t._v(" (现代模型的标准配置，如BERT, GPT)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("多头交叉注意力")]),t._v(" "),s("br"),t._v(" (现代编码器-解码器的标准配置，如T5)")])])])]),t._v(" "),s("p",[s("strong",[t._v("结论：")])]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("多头注意力")]),t._v("是一种"),s("strong",[t._v("结构")]),t._v("，它可以被应用到"),s("strong",[t._v("自注意力")]),t._v("或"),s("strong",[t._v("交叉注意力")]),t._v("的"),s("strong",[t._v("功能")]),t._v("上。")]),t._v(" "),s("li",[t._v("同样，"),s("strong",[t._v("自注意力")]),t._v("是一种"),s("strong",[t._v("功能")]),t._v("，它可以用"),s("strong",[t._v("单头")]),t._v("或"),s("strong",[t._v("多头")]),t._v("的"),s("strong",[t._v("结构")]),t._v("来实现。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"代码对比-一目了然"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#代码对比-一目了然"}},[t._v("#")]),t._v(" 代码对比：一目了然")]),t._v(" "),s("p",[t._v("让我们看代码如何体现这种组合关系。")]),t._v(" "),s("h4",{attrs:{id:"_1-多头自注意力-multi-head-self-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-多头自注意力-multi-head-self-attention"}},[t._v("#")]),t._v(" 1. 多头自注意力 (Multi-Head Self-Attention)")]),t._v(" "),s("p",[t._v("这是Transformer"),s("strong",[t._v("编码器")]),t._v("的核心。Q, K, V 都来自同一输入 "),s("code",[t._v("x")]),t._v("（比如一个句子）。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 伪代码")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadSelfAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Q, K, V 都来自同一个输入x")]),t._v("\n        output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output\n")])])]),s("h4",{attrs:{id:"_2-多头交叉注意力-multi-head-cross-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-多头交叉注意力-multi-head-cross-attention"}},[t._v("#")]),t._v(" 2. 多头交叉注意力 (Multi-Head Cross-Attention)")]),t._v(" "),s("p",[t._v("这是Transformer"),s("strong",[t._v("解码器")]),t._v("的核心。Q 来自解码器自身，而 K, V 来自编码器的输出。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 伪代码")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MultiHeadCrossAttention")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MultiHeadAttention"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embed_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_heads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" decoder_hidden"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoder_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Q 来自解码器，K, V 来自编码器")]),t._v("\n        output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multihead_attn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            query"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("decoder_hidden"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Q: 来自解码器")]),t._v("\n            key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("encoder_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("     "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# K: 来自编码器")]),t._v("\n            value"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("encoder_outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# V: 来自编码器")]),t._v("\n            mask"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mask\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output\n")])])]),s("p",[t._v("请注意，这两个类都"),s("strong",[t._v("使用了同一个底层模块 "),s("code",[t._v("MultiHeadAttention")])]),t._v("。这个模块内部包含了多套线性层来生成多组的Q, K, V。功能的区别仅仅在于调用时传入的 "),s("code",[t._v("query")]),t._v(", "),s("code",[t._v("key")]),t._v(", "),s("code",[t._v("value")]),t._v(" 参数是否来自同一个源。")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"总结与类比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#总结与类比"}},[t._v("#")]),t._v(" 总结与类比")]),t._v(" "),s("p",[t._v("用一个简单的类比来理解：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("注意力机制")]),t._v("就像"),s("strong",[t._v("做饭")]),t._v("。")]),t._v(" "),s("li",[s("strong",[t._v("单头/多头")]),t._v("：就像是用"),s("strong",[t._v("一口锅")]),t._v("做饭还是用"),s("strong",[t._v("多口不同的锅")]),t._v("（炒锅、炖锅、蒸锅）同时做饭。多头（多口锅）可以同时处理不同的风味，能力更强。")]),t._v(" "),s("li",[s("strong",[t._v("自注意力/交叉注意力")]),t._v("：就像是"),s("strong",[t._v("炒一个菜内部的食材融合")]),t._v("（自注意力） vs. "),s("strong",[t._v("用一个菜的汤汁去炖另一个菜")]),t._v("（交叉注意力）。这是两种完全不同的烹饪方法。")])]),t._v(" "),s("p",[t._v("所以：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("多头自注意力")]),t._v(" = 用多口锅来炒一个菜（让食材内部更充分地融合）。")]),t._v(" "),s("li",[s("strong",[t._v("多头交叉注意力")]),t._v(" = 用多口锅，一口锅做汤底，另一口锅用这个汤底来炖别的菜。")])]),t._v(" "),s("p",[s("strong",[t._v("自注意力功能和交叉注意力功能都可以通过多头结构来实现")]),t._v("。")])])}),[],!1,null,null,null);s.default=e.exports}}]);