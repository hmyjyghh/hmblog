(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{544:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"rag"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#rag"}},[t._v("#")]),t._v(" RAG")]),t._v(" "),s("h3",{attrs:{id:"_1-因为llm的局限性-所以需要rag来增强模型的能力"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-因为llm的局限性-所以需要rag来增强模型的能力"}},[t._v("#")]),t._v(" 1. 因为LLM的局限性，所以需要RAG来增强模型的能力")]),t._v(" "),s("blockquote",[s("p",[t._v("将⼤模型应⽤于实际业务场景时会发现，通⽤的基础⼤模型基本⽆法满⾜我们的实际业务需求，主要有以下⼏⽅⾯原因:")])]),t._v(" "),s("ul",[s("li",[t._v("LLM的知识不是实时的，不具备知识更新")]),t._v(" "),s("li",[t._v("LLM可能不知道你私有的领域/业务知识")]),t._v(" "),s("li",[t._v("LLM有时会在回答中⽣成看似合理但实际上是错误的信息")])]),t._v(" "),s("p",[t._v("RAG(Retrieval Augmented Generation)-检索增强⽣成")]),t._v(" "),s("h3",{attrs:{id:"_2-为什么会用到rag"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-为什么会用到rag"}},[t._v("#")]),t._v(" 2. 为什么会⽤到RAG?")]),t._v(" "),s("ol",[s("li",[t._v("提⾼准确性: "),s("code",[t._v("通过检索相关的信息")]),t._v("，RAG可以提⾼⽣成⽂本的准确性。")]),t._v(" "),s("li",[t._v("减少训练成本：与需要⼤量数据来训练的⼤型⽣成模型相⽐，"),s("code",[t._v("RAG可以通过检索机制")]),t._v("来减少所需的训练数据量，从⽽降低训练成本。")]),t._v(" "),s("li",[t._v("适应性强：RAG模型可以适应新的或不断变化的数据。"),s("code",[t._v("由于它们能够检索最新的信息")]),t._v("，因此在新数据和事件出现时，它们能够快速适应并⽣成相关的⽂本。")])]),t._v(" "),s("h3",{attrs:{id:"_3-rag-和-fine-tuning-对比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-rag-和-fine-tuning-对比"}},[t._v("#")]),t._v(" 3. RAG 和 Fine-tuning 对比")]),t._v(" "),s("ol",[s("li",[t._v("RAG（检索增强⽣成）是把内部的⽂档数据先进⾏embedding，"),s("strong",[t._v("借助检索先获得⼤致的知识范围答案，再结合prompt给到LLM")]),t._v("，让LLM⽣成最终的答案")]),t._v(" "),s("li",[t._v("Fine-tuning（微调）"),s("strong",[t._v("是⽤⼀定量的数据集")]),t._v("对LLM进⾏局部参数的调整，以期望LLM更加理解我们的业务逻辑，有更好的"),s("code",[t._v("zero-shot")]),t._v("能⼒。")])]),t._v(" "),s("h2",{attrs:{id:"rag概念"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#rag概念"}},[t._v("#")]),t._v(" RAG概念")]),t._v(" "),s("blockquote",[s("p",[t._v("RAG（Retrieval Augmented Generation）顾名思义，通过检索外部数据，增强⼤模型的⽣成效果。")])]),t._v(" "),s("ul",[s("li",[t._v("RAG即检索增强⽣成，为LLM提供了从某些数据源检索到的信息，并基于此修正⽣成的答案。")]),t._v(" "),s("li",[t._v("RAG 基本上是Search + LLM 提示，可以通过⼤模型回答查询，并将搜索算法所找到的信息作为⼤模型的上下⽂。")]),t._v(" "),s("li",[t._v("查询和检索到的上下⽂都会被注⼊到发送到 LLM 的提示语中。")])]),t._v(" "),s("h3",{attrs:{id:"_1-rag系统工作流程图解"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-rag系统工作流程图解"}},[t._v("#")]),t._v(" 1. RAG系统⼯作流程图解")]),t._v(" "),s("p",[s("img",{attrs:{src:"/hmblog/images/llm/rag/RAG-1.png",alt:"RAG系统⼯作流程图解"}})]),t._v(" "),s("h3",{attrs:{id:"_2-rag系统的搭建流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-rag系统的搭建流程"}},[t._v("#")]),t._v(" 2. RAG系统的搭建流程")]),t._v(" "),s("p",[s("img",{attrs:{src:"/hmblog/images/llm/rag/RAG-2.png",alt:"RAG系统搭建流程"}})]),t._v(" "),s("h3",{attrs:{id:"_3-索引-indexing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-索引-indexing"}},[t._v("#")]),t._v(" 3. 索引（Indexing）")]),t._v(" "),s("ul",[s("li",[t._v("索引⾸先清理和提取各种格式的原始数据，如 PDF、HTML、 Word 和 Markdown，然后将其转换为统⼀的纯⽂本格式。")]),t._v(" "),s("li",[t._v("为了适应语⾔模型的上下⽂限制，⽂本被分割成更⼩的、可消化的块（chunk）。")]),t._v(" "),s("li",[t._v("然后"),s("code",[t._v("使⽤嵌⼊模型")]),t._v("将块编码成向量表示，"),s("code",[t._v("并存储在向量数据库中")]),t._v("。这⼀步对于在随后的检索阶段实现⾼效的相似性搜索⾄关重要。")]),t._v(" "),s("li",[t._v("知识库分割成 chunks，并将 chunks 向量化⾄向量库中")])]),t._v(" "),s("h3",{attrs:{id:"_4-检索-retrieval"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-检索-retrieval"}},[t._v("#")]),t._v(" 4. 检索（Retrieval）")]),t._v(" "),s("ul",[s("li",[t._v("在收到⽤户查询（Query）后，RAG 系统采⽤与索引阶段相同的编码模型将查询转换为向量表示，")]),t._v(" "),s("li",[t._v("然后计算索引语料库中查询向量与块向量的相似性得分。该系统优先级和检索最⾼ k （Top-K）块，显示最⼤的相似性查询。")]),t._v(" "),s("li",[t._v("这些块随后被⽤作 "),s("code",[t._v("prompt")]),t._v(" 中的扩展上下⽂。Query 向量化，匹配向量空间中相近的 chunks")])]),t._v(" "),s("h3",{attrs:{id:"_5-rag具体实现流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-rag具体实现流程"}},[t._v("#")]),t._v(" 5. RAG具体实现流程")]),t._v(" "),s("p",[t._v("加载⽂件 => 读取⽂本 => ⽂本分割 =>⽂本向量化 =>输⼊问题向量化 =>在⽂本向量中匹配出与问题向量最相似的 top k 个 => 匹配出的⽂本作为上下⽂和问题⼀起添加到 "),s("code",[t._v("prompt")]),t._v(" 中 =>提交给 LLM ⽣成回答")]),t._v(" "),s("h3",{attrs:{id:"_6-向量与embeddings的定义"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-向量与embeddings的定义"}},[t._v("#")]),t._v(" 6. 向量与Embeddings的定义")]),t._v(" "),s("ol",[s("li",[t._v("将⽂本转成⼀组浮点数：每个下标 i ，对应⼀个维度")]),t._v(" "),s("li",[t._v("整个数组对应⼀个 n 维空间的⼀个点，即"),s("code",[t._v("⽂本向量")]),t._v("⼜叫 Embeddings")]),t._v(" "),s("li",[t._v("向量之间可以计算距离，距离远近对应"),s("code",[t._v("语义相似度")]),t._v("⼤⼩")])]),t._v(" "),s("h3",{attrs:{id:"_7-文档的加载与切割-基于文档的llm回复系统搭建"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-文档的加载与切割-基于文档的llm回复系统搭建"}},[t._v("#")]),t._v(" 7. ⽂档的加载与切割（基于⽂档的LLM回复系统搭建）")]),t._v(" "),s("p",[s("img",{attrs:{src:"/hmblog/images/llm/rag/RAG-3.png",alt:"⽂档的加载与切割"}})]),t._v(" "),s("h2",{attrs:{id:"rag-框架或技术点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#rag-框架或技术点"}},[t._v("#")]),t._v(" RAG 框架或技术点")]),t._v(" "),s("h3",{attrs:{id:"_1-react"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-react"}},[t._v("#")]),t._v(" 1. ReAct")]),t._v(" "),s("p",[s("strong",[t._v("推理与行动")])]),t._v(" "),s("p",[t._v("在 RAG（检索增强生成）中，ReAct 是一种用于构建基于大语言模型（LLM）的智能体的框架，其核心理念是将链式思维提示的优势整合到 Agent 框架中 。")]),t._v(" "),s("p",[t._v("ReAct 最早在论文《ReAct: Synergizing Reasoning and Acting in Language Models》中提出，它的含义是直接利用 LLM 进行 Reasoning+Action，即推理与行动 。具体来说，ReAct 让 LLM 通过一系列 “思考 - 行动 - 观察”（Thought-Action-Observation）的循环来解决问题，而不是一次性直接生成最终答案 。其中，推理（Thought）阶段利用 LLM 生成分析步骤，解释任务上下文或状态，为下一步行动提供逻辑依据；行动（Action）则依据推理结果生成工具调用请求，像查搜索引擎、调用 API、数据库检索等；观察（Observation）就是模型接收来自环境的反馈，并据此更新高层计划 。")]),t._v(" "),s("p",[t._v("通过这样交替执行推理和操作步骤，**ReAct 框架允许模型动态生成推理路径，在与环境交互的同时不断调整计划，从而实现迭代和增量式的任务解决，其检索并非被动，而是一个主动的、由决策驱动的过程 。**该模式可以增强 LLM 的决策和解决问题的能力，让其处理复杂任务时更具可解释性、可诊断性和稳健性 。")]),t._v(" "),s("h3",{attrs:{id:"_2-fusion-rag"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-fusion-rag"}},[t._v("#")]),t._v(" 2. Fusion RAG")]),t._v(" "),s("p",[t._v("RAG Fusion（RAG 融合）")]),t._v(" "),s("p",[t._v("包含 多查询  +  RRF")]),t._v(" "),s("h3",{attrs:{id:"_3-decompositon"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-decompositon"}},[t._v("#")]),t._v(" 3. Decompositon")]),t._v(" "),s("h3",{attrs:{id:"_4-hyde"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-hyde"}},[t._v("#")]),t._v(" 4. HyDE")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step1 首先就是定义一个生成假设文档的提示词")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# HyDE document generation")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 您可以使用这个文档生成提示,您可以根据您感兴趣的领域任意调整它。")]),t._v("\ntemplate "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Please write a scientific paper passage to answer the question\nQuestion: {question}\nPassage:"""')]),t._v("\nprompt_hyde "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step2 调用大模型, 得到与我们的问题相关的假设文档部分。")]),t._v("\ngenerate_docs_for_retrieval "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  prompt_hyde "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" ChatOpenAI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temperature"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" StrOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Run")]),t._v("\nquestion "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"What is task decomposition for LLM agents?"')]),t._v("\ngenerate_docs_for_retrieval"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"question"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step3 接下来，就是用那个假设性文档，构成链 去使用, 运行检索")]),t._v("\nretrieval_chain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" generate_docs_for_retrieval "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" retriever\nretrieved_docs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" retrieval_chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"question"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nretrieved_docs\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# retriever, 我们将从索引中获取与`已嵌入的假设文档`相关的文档。")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 得到了一些与该假设文档相关的检索块。")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step4 最后，我们在这里获取我们定义的检索到的文档以及我们的问题。")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 也就是将，查询到相关片段(eg: 这里的retrieved_docs)作为上下文，再次传送给大模型去检索，答案")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在这里定义我们检索到的文档  和  要询问的问题")]),t._v("\ntemplate "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n"""')]),t._v("\n\nprompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfinal_rag_chain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  prompt\n  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" llm\n  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" StrOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfinal_rag_chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"context"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("retrieved_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"question"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("h1",{attrs:{id:"检索过程-可以在langsmith-平台去查看"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#检索过程-可以在langsmith-平台去查看"}},[t._v("#")]),t._v(" 检索过程，可以在LangSmith 平台去查看")]),t._v(" "),s("h3",{attrs:{id:"_5-routing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-routing"}},[t._v("#")]),t._v(" 5. Routing")]),t._v(" "),s("p",[t._v("个人理解：根据不同的问题，导航到不同的向量数据库去查询，以便更精准地进行查询。")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step1 ")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 假设我们有三个不同的文档,例如Python文档、JS文档、Golang文档")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Data model")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("RouteQuery")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseModel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Route a user query to the most relevant datasource."""')]),t._v("\n\n  datasource"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Literal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"python_docs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"js_docs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"golang_docs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    description"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Given a user question choose which datasource would be most relevant for answering their question"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LLM with function call ")]),t._v("\nllm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatOpenAI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpt-3.5-turbo-0125"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" temperature"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstructured_llm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("with_structured_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("RouteQuery"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 拥有3个文档源，然后与LLM进行绑定")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 因此, 您可以看到我们对结构化输出所做的工作,基本上是在幕后。即采用对象定义,转换为函数模式并将该函数模式绑定到我们的LLM。")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step2 然后我们调用 Prompt")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Prompt ")]),t._v("\nsystem "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""You are an expert at routing a user question to the appropriate data source.\n\nBased on the programming language the question is referring to, route it to the relevant data source."""')]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提示词翻译")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""您是将用户问题路由到适当数据源的专家。\n\n根据问题所指的编程语言，将其路由到相关数据源。"""')]),t._v("\nprompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"system"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" system"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{question}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define router ")]),t._v("\nrouter "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" structured_llm\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 因此, 您基本上可以将这个问题与不同的链挂钩,例如,Python的检索器链一, JS的检索器链二, 等等。")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step3 ")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("choose_route")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"python_docs"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasource"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lower"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("### Logic here ")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"chain for python_docs"')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"js_docs"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasource"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lower"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("### Logic here ")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"chain for js_docs"')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("### Logic here ")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"golang_docs"')]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 所有这有点像路由机制")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 但这实际上是在承担输入问题并将其转换为结构化对象的繁重工作,该结构化对象将输出限制为我们在路由问题中关心的几种输出类型之一。")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 所以,这确实是这一切结合在一起的方式。")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# step4")]),t._v("\n")])])]),s("hr"),t._v(" "),s("p",[t._v("语义路由貌似更简单")]),t._v(" "),s("ol",{attrs:{start:"4"}},[s("li",[t._v("COT 思维链\n已成为增强复杂任务上模型性能的标准提示技术。该模型被指示“一步一步思考”，以利用更多的测试时间计算将困难的任务分解为更小、更简单的步骤。CoT 将大任务转化为多个可管理的任务，并阐明对模型思维过程的解释。")])]),t._v(" "),s("p",[t._v("在 RAG（检索增强生成）中，ReAct 是一种用于构建基于大语言模型（LLM）的智能体的框架，其核心理念是将链式思维提示的优势整合到 Agent 框架中 。")]),t._v(" "),s("p",[t._v("论文：《ReAct: Synergizing Reasoning and Acting in Language Models》")]),t._v(" "),s("ul",[s("li",[t._v("ReAct 让 LLM 通过一系列 “思考 - 行动 - 观察”（Thought-Action-Observation）的循环来解决问题，而不是一次性直接生成最终答案 。")]),t._v(" "),s("li",[t._v("其中，推理（Thought）阶段利用 LLM 生成分析步骤，解释任务上下文或状态，为下一步行动提供逻辑依据；")]),t._v(" "),s("li",[t._v("行动（Action）则依据推理结果生成工具调用请求，像查搜索引擎、调用 API、数据库检索等；")]),t._v(" "),s("li",[t._v("观察（Observation）就是模型接收来自环境的反馈，并据此更新高层计划 。")])]),t._v(" "),s("p",[t._v("Task decomposition：任务分解")]),t._v(" "),s("p",[t._v("任务分解是将复杂任务分解为更小、更易于管理的步骤或子目标的过程。\n这可以通过思维链提示等技术来实现，它指示模型逐步思考，或者使用特定任务的指令和人工输入。\n目标是通过规划和执行每个组件步骤，使大问题更容易解决。")]),t._v(" "),s("p",[t._v("Task decomposition 可以通过以下方式实现：")]),t._v(" "),s("ul",[s("li",[t._v("提示词工程：设计特定的提示词，引导模型生成结构化的思考步骤。")]),t._v(" "),s("li",[t._v("人工输入：在任务执行过程中，人类提供额外的输入，帮助模型更好地理解任务需求。")]),t._v(" "),s("li",[t._v("自动分解：利用模型的推理能力，自动将复杂任务分解为多个子任务。")])]),t._v(" "),s("p",[t._v("思维链（CoT）已成为增强复杂任务上模型性能的标准提示技术。该模型被指示“一步一步思考”，以利用更多的测试时间计算将困难的任务分解为更小、更简单的步骤。CoT 将大任务转化为多个可管理的任务，并阐明对模型思维过程的解释。\nTree of Thoughts 通过探索每个步骤的多种推理可能性来扩展 CoT。")]),t._v(" "),s("ul",[s("li",[t._v("它首先将问题分解为多个思维步骤，每个步骤生成多个想法，从而创建树结构。搜索过程可以是 BFS（广度优先搜索）或 DFS（深度优先搜索），每个状态都由分类器（通过提示）或多数票评估。")])]),t._v(" "),s("h2",{attrs:{id:"_6-多查询-multi-query-的优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-多查询-multi-query-的优化"}},[t._v("#")]),t._v(" 6. 多查询（Multi-Query）的优化")]),t._v(" "),s("ul",[s("li",[s("p",[s("strong",[t._v("查询生成 / 改写")]),t._v("：使用 LLM 对用户的初始查询进行改写，生成多个语义相似但表述不同的查询变体。这些变体从不同角度诠释原始问题，扩大检索覆盖范围。例如，原始查询是 “气候变化的影响”，生成的查询可能包括 “气候变化的经济后果”, “气候变化与公共健康” 等。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("逆向排名融合（RRF）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("对每个生成的查询进行基于向量的搜索，形成多路搜索召回。")]),t._v(" "),s("li",[t._v("然后应用逆向排名融合算法，根据文档在多个查询中的相关性重新排列文档。")]),t._v(" "),s("li",[t._v("RRF 通过汇总不同搜索请求的排名，为每个排名列表中的结果分配倒数排名分数，分数按 1/(rank + k) 计算，其中 rank 是文档在列表中的位置，k 是一个常量，通常设置为 60 效果最佳。")]),t._v(" "),s("li",[t._v("最后将从每个搜索系统中获得的倒数排名分数相加，为每个文档生成合并分数，并根据合并分数对文档进行排名和排序，提高最相关文档在结果列表顶部出现的可能性。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("提示调整")]),t._v("：在提示中要求 LLM 更重视原始查询，以缓解多查询可能稀释用户原始意图的问题。")])])]),t._v(" "),s("h2",{attrs:{id:"_7-任务分解-task-decomposition-的优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-任务分解-task-decomposition-的优化"}},[t._v("#")]),t._v(" 7. 任务分解（Task Decomposition）的优化")]),t._v(" "),s("ul",[s("li",[s("p",[s("strong",[t._v("确定分解模式")]),t._v("：根据问题的特点确定使用串行模式或并行模式。串行模式适用于逻辑依赖强的问题分解，确保步骤的连贯性，如对于 “RAG 都有哪些阶段？” 这样的问题，需要先找到都有哪些阶段，然后再询问各个阶段该做什么事情。并行模式适用于独立子任务的高效处理，能提升响应速度，如对于 “如何规划北京到上海的 5 天旅游行程？” 的问题，可以分解成交通、住宿、景点三个子问题，分别完成。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("构建提示模板")]),t._v("：使用特定的提示模板引导 LLM 进行问题分解。")]),t._v(" "),s("ul",[s("li",[t._v("例如，“你的任务是针对输入的问题生成多个相关的子问题或子查询，将输入问题分解成一组可以独立回答的子问题或子任务。")]),t._v(" "),s("li",[t._v("以下是输入的问题："),s("question",[t._v(t._s(t.question))]),t._v("请生成 3-5 个与该问题相关的搜索查询，并使用换行符进行分割。")],1),t._v(" "),s("li",[t._v("生成的子问题 / 子查询应具有明确的主题和可独立回答的特点。请在 <子问题> 标签内写下生成的子问题 / 子查询。”")])])]),t._v(" "),s("li",[s("p",[t._v("子任务处理与结果整合：")]),t._v(" "),s("ul",[s("li",[t._v("针对每个子任务单独执行检索和生成，最后将子结果汇总整合，形成最终答案。")]),t._v(" "),s("li",[t._v("在整合过程中，需要确保子结果之间的逻辑连贯性和一致性。")])])])])])}),[],!1,null,null,null);s.default=e.exports}}]);