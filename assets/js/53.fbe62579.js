(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{537:function(t,a,e){"use strict";e.r(a);var n=e(3),s=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"模型微调需要的库或者方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型微调需要的库或者方法"}},[t._v("#")]),t._v(" 模型微调需要的库或者方法")]),t._v(" "),a("h3",{attrs:{id:"_1-create-and-prepare-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-create-and-prepare-model"}},[t._v("#")]),t._v(" 1. create_and_prepare_model")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" utils "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" create_and_prepare_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" create_datasets\n")])])]),a("p",[a("code",[t._v("create_and_prepare_model")]),t._v(" 是一个从 "),a("code",[t._v("utils")]),t._v(" 模块导入的工具函数，从函数名推测，它的主要作用是"),a("strong",[t._v("创建并预处理模型")]),t._v("，通常用于机器学习或深度学习任务中。")]),t._v(" "),a("p",[t._v("具体来说，这个函数可能包含以下功能：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("模型创建")]),t._v("：根据任务需求（如分类、回归、生成等）初始化一个模型结构（可能是自定义模型或基于现有框架的预训练模型）")]),t._v(" "),a("li",[a("strong",[t._v("模型配置")]),t._v("：设置模型的超参数（如学习率、优化器、损失函数等）")]),t._v(" "),a("li",[a("strong",[t._v("预处理操作")]),t._v("：\n"),a("ul",[a("li",[t._v("可能包含模型权重的初始化或加载")]),t._v(" "),a("li",[t._v("对模型进行必要的调整（如冻结部分层、修改输出层适应新任务等）")]),t._v(" "),a("li",[t._v("可能还会将模型移动到指定设备（如GPU）")])])])]),t._v(" "),a("p",[t._v("这类函数通常与数据处理函数（如你提到的 "),a("code",[t._v("create_datasets")]),t._v('）配合使用，形成"数据准备-模型创建-训练评估"的完整流程。')]),t._v(" "),a("p",[t._v("如果没有具体的实现代码，以上是基于函数命名的合理推测。实际功能可能会根据具体项目的需求有所不同。")]),t._v(" "),a("h3",{attrs:{id:"_2-trainer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-trainer"}},[t._v("#")]),t._v(" 2. Trainer")]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Trainer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainingArguments\n")])])]),a("h3",{attrs:{id:"_3-练习-peft-微调-mathstral-7b-v0-1-模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-练习-peft-微调-mathstral-7b-v0-1-模型"}},[t._v("#")]),t._v(" 3. 练习 peft 微调 Mathstral-7B-v0.1 模型")]),t._v(" "),a("h4",{attrs:{id:"_3-1-克隆-peft-仓库-安装根目录下相关库"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-克隆-peft-仓库-安装根目录下相关库"}},[t._v("#")]),t._v(" 3.1 克隆 peft 仓库，安装根目录下相关库，")]),t._v(" "),a("ul",[a("li",[t._v("安装 "),a("code",[t._v("modelscope")]),t._v(" 库，用于下载 Mathstral-7B-v0.1 模型")]),t._v(" "),a("li",[t._v("运行 "),a("code",[t._v("run_peft.sh")]),t._v(" 脚本，微调 Mathstral-7B-v0.1 模型")])]),t._v(" "),a("div",{staticClass:"language-Text extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("git clone https://github.com/huggingface/peft.git\n\ncd peft\npip install -e .\n\n运行 SFT 示例前的准备：\n\ncd examples/sft\npip install -r requirements.txt\n\n\n然后就可以运行脚本了：\n\nbash run_peft.sh\n\n- 下载模型到本地\n\n\n- 安装 `modelscope` 库，用于下载 Mathstral-7B-v0.1 模型\n- 运行 `run_peft.sh` 脚本，微调 Mathstral-7B-v0.1 模型\n\npip install modelscope\nmodelscope download --model mistralai/Mathstral-7B-v0.1\n\n\n# 推荐使用 huggingface-cli 下载，保持一致性\npip install huggingface-hub\nhuggingface-cli download --resume-download mistralai/Mathstral-7B-v0.1 --local-dir ./mathstral-7b-model\n\n- 数据集\n\nsmangrul/ultrachat-10k-chatml 是一个用于对话微调的数据集，基于 UltraChat 数据，使用 ChatML 格式进行处理。\n\n\n# 设置镜像环境变量\nexport HF_ENDPOINT=https://hf-mirror.com\n\n# 使用 huggingface-cli 下载\npip install huggingface-hub\nhuggingface-cli download --repo-type dataset smangrul/ultrachat-10k-chatml --local-dir ./ultrachat-10k-chatml\n\n\n\n\n\n/mnt/workspace/.cache/modelscope/models/mistralai/Mathstral-7B-v0.1\n./ultrachat-10k-chatml\n\n\n")])])]),a("h4",{attrs:{id:"_3-2-完整的修正步骤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-完整的修正步骤"}},[t._v("#")]),t._v(" 3.2 完整的修正步骤")]),t._v(" "),a("h5",{attrs:{id:"_1-克隆仓库"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-克隆仓库"}},[t._v("#")]),t._v(" 1. 克隆仓库")]),t._v(" "),a("p",[t._v("git clone https://github.com/huggingface/peft.git\ncd peft")]),t._v(" "),a("h5",{attrs:{id:"_2-安装-peft"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-安装-peft"}},[t._v("#")]),t._v(" 2. 安装 PEFT")]),t._v(" "),a("p",[t._v("pip install -e .")]),t._v(" "),a("h5",{attrs:{id:"_3-进入示例目录并安装依赖"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-进入示例目录并安装依赖"}},[t._v("#")]),t._v(" 3. 进入示例目录并安装依赖")]),t._v(" "),a("p",[t._v("cd examples/sft\npip install -r requirements.txt")]),t._v(" "),a("h5",{attrs:{id:"_4-设置镜像-如果国内网络需要"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-设置镜像-如果国内网络需要"}},[t._v("#")]),t._v(" 4. 设置镜像（如果国内网络需要）")]),t._v(" "),a("p",[t._v("export HF_ENDPOINT=https://hf-mirror.com")]),t._v(" "),a("h5",{attrs:{id:"_5-下载模型-推荐方式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-下载模型-推荐方式"}},[t._v("#")]),t._v(" 5. 下载模型（推荐方式）")]),t._v(" "),a("p",[t._v("pip install huggingface-hub\nhuggingface-cli download --resume-download mistralai/Mathstral-7B-v0.1 --local-dir ./mathstral-7b-model")]),t._v(" "),a("h5",{attrs:{id:"_6-下载数据集"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-下载数据集"}},[t._v("#")]),t._v(" 6. 下载数据集")]),t._v(" "),a("p",[t._v("huggingface-cli download --repo-type dataset smangrul/ultrachat-10k-chatml --local-dir ./ultrachat-10k-chatml")]),t._v(" "),a("h5",{attrs:{id:"_7-修改脚本或使用参数指定路径-重要"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-修改脚本或使用参数指定路径-重要"}},[t._v("#")]),t._v(" 7. 修改脚本或使用参数指定路径（重要！）")]),t._v(" "),a("h5",{attrs:{id:"确保-run-peft-sh-脚本中指定了正确的模型路径和数据集路径"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#确保-run-peft-sh-脚本中指定了正确的模型路径和数据集路径"}},[t._v("#")]),t._v(" 确保 run_peft.sh 脚本中指定了正确的模型路径和数据集路径")]),t._v(" "),a("h5",{attrs:{id:"例如"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#例如"}},[t._v("#")]),t._v(" 例如：")]),t._v(" "),a("h5",{attrs:{id:"model-name-or-path-mathstral-7b-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#model-name-or-path-mathstral-7b-model"}},[t._v("#")]),t._v(" --model_name_or_path ./mathstral-7b-model")]),t._v(" "),a("h5",{attrs:{id:"dataset-name-ultrachat-10k-chatml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dataset-name-ultrachat-10k-chatml"}},[t._v("#")]),t._v(" --dataset_name ./ultrachat-10k-chatml")]),t._v(" "),a("h4",{attrs:{id:"autodl-4090-环境注意事项"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#autodl-4090-环境注意事项"}},[t._v("#")]),t._v(" AutoDL 4090 环境注意事项")]),t._v(" "),a("h5",{attrs:{id:"_1-显存优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-显存优化"}},[t._v("#")]),t._v(" 1. 显存优化")]),t._v(" "),a("p",[t._v("4090 有 24GB 显存，但对于 7B 模型还是有些紧张，建议：")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 减小批次大小，如果出现 OOM 错误")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--per_device_train_batch_size")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--gradient_accumulation_steps")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 启用梯度检查点")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--gradient_checkpointing")]),t._v(" True\n")])])]),a("h5",{attrs:{id:"_2-环境检查"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-环境检查"}},[t._v("#")]),t._v(" 2. 环境检查")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查 GPU 状态")]),t._v("\nnvidia-smi\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查 CUDA 和 PyTorch")]),t._v("\npython "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-c")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())"')]),t._v("\n")])])]),a("h5",{attrs:{id:"_3-完整的验证流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-完整的验证流程"}},[t._v("#")]),t._v(" 3. 完整的验证流程")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 环境验证")]),t._v("\npython "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-c")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"\nimport torch\nprint(f'CUDA available: {torch.cuda.is_available()}')\nprint(f'GPU count: {torch.cuda.device_count()}')\nprint(f'Current GPU: {torch.cuda.current_device()}')\nprint(f'GPU name: {torch.cuda.get_device_name()}')\nprint(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n\"")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 测试模型加载")]),t._v("\npython "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-c")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained('./mathstral-7b-model', torch_dtype=torch.bfloat16)\nprint('模型加载成功！')\n\"")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 运行训练（先小规模测试）")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加 --max_steps 10 进行快速测试")]),t._v("\n")])])]),a("h2",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),a("p",[t._v("你的基本步骤是正确的，主要需要：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("统一模型下载方式")]),t._v("（建议都用 huggingface-cli）")]),t._v(" "),a("li",[a("strong",[t._v("正确指定本地路径")])]),t._v(" "),a("li",[a("strong",[t._v("根据 4090 的显存适当调整批次大小")])]),t._v(" "),a("li",[a("strong",[t._v("先进行小规模测试验证环境")])])]),t._v(" "),a("p",[t._v("按照这个修正后的流程，你应该能在 AutoDL 的 4090 上成功运行 PEFT SFT 示例。")]),t._v(" "),a("blockquote",[a("p",[t._v("只对模型下载使用镜像，其他操作使用官方源：")])]),t._v(" "),a("h1",{attrs:{id:"下载模型和数据时使用镜像"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#下载模型和数据时使用镜像"}},[t._v("#")]),t._v(" 下载模型和数据时使用镜像")]),t._v(" "),a("p",[t._v("export HF_ENDPOINT=https://hf-mirror.com\nhuggingface-cli download --resume-download mistralai/Mathstral-7B-v0.1 --local-dir ./mathstral-7b-model\nhuggingface-cli download --repo-type dataset smangrul/ultrachat-10k-chatml --local-dir ./ultrachat-10k-chatml")]),t._v(" "),a("h1",{attrs:{id:"训练时使用官方源"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#训练时使用官方源"}},[t._v("#")]),t._v(" 训练时使用官方源")]),t._v(" "),a("p",[t._v("unset HF_ENDPOINT")]),t._v(" "),a("h1",{attrs:{id:"_1-确保已经下载了模型和数据集-使用镜像"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-确保已经下载了模型和数据集-使用镜像"}},[t._v("#")]),t._v(" 1. 确保已经下载了模型和数据集（使用镜像）")]),t._v(" "),a("p",[t._v("export HF_ENDPOINT=https://hf-mirror.com\npip install huggingface-hub\nhuggingface-cli download --resume-download mistralai/Mathstral-7B-v0.1 --local-dir ./mathstral-7b-model\nhuggingface-cli download --repo-type dataset smangrul/ultrachat-10k-chatml --local-dir ./ultrachat-10k-chatml")]),t._v(" "),a("h1",{attrs:{id:"_2-取消镜像设置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-取消镜像设置"}},[t._v("#")]),t._v(" 2. 取消镜像设置")]),t._v(" "),a("p",[t._v("unset HF_ENDPOINT")]),t._v(" "),a("h1",{attrs:{id:"_3-创建修正版脚本"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-创建修正版脚本"}},[t._v("#")]),t._v(" 3. 创建修正版脚本")]),t._v(" "),a("p",[t._v("cat > run_peft_fixed.sh << 'EOF'\n#!/bin/bash")]),t._v(" "),a("p",[t._v("accelerate launch "),a("br"),t._v("\n--config_file ../configs/accelerate_configs/multi_gpu.yaml "),a("br"),t._v("\n../sft.py "),a("br"),t._v("\n--model_name_or_path ./mathstral-7b-model "),a("br"),t._v("\n--dataset_name ./ultrachat-10k-chatml "),a("br"),t._v("\n--output_dir ./mathstral-7b-sft-lora "),a("br"),t._v("\n--log_with none "),a("br"),t._v("\n--use_peft "),a("br"),t._v("\n--peft_method lora "),a("br"),t._v("\n--lora_r 16 "),a("br"),t._v("\n--lora_alpha 32 "),a("br"),t._v("\n--lora_target_modules q_proj k_proj v_proj o_proj "),a("br"),t._v("\n--per_device_train_batch_size 1 "),a("br"),t._v("\n--gradient_accumulation_steps 4 "),a("br"),t._v("\n--learning_rate 2e-4 "),a("br"),t._v("\n--lr_scheduler_type constant "),a("br"),t._v("\n--warmup_ratio 0.03 "),a("br"),t._v("\n--weight_decay 0.0 "),a("br"),t._v("\n--num_train_epochs 1 "),a("br"),t._v("\n--max_steps 50 "),a("br"),t._v("\n--logging_steps 5 "),a("br"),t._v("\n--save_strategy no "),a("br"),t._v("\n--bf16 "),a("br"),t._v("\n--tf32 True "),a("br"),t._v("\n--packing True "),a("br"),t._v("\n--gradient_checkpointing True\nEOF")]),t._v(" "),a("p",[t._v("chmod +x run_peft_fixed.sh")]),t._v(" "),a("h1",{attrs:{id:"_4-运行测试-先小规模"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-运行测试-先小规模"}},[t._v("#")]),t._v(" 4. 运行测试（先小规模）")]),t._v(" "),a("p",[t._v("./run_peft_fixed.sh")]),t._v(" "),a("h3",{attrs:{id:"_4-错误收集"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-错误收集"}},[t._v("#")]),t._v(" 4. 错误收集")]),t._v(" "),a("ul",[a("li",[t._v("ConnectionError: Couldn't reach 'smangrul/ultrachat-10k-chatml' on the Hub (LocalEntryNotFoundError)")])]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" huggingface_hub\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Check Hub connectivity")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hub status:"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" huggingface_hub"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hf_hub_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"smangrul/ultrachat-10k-chatml"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Try with different parameters")]),t._v("\ndataset "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"smangrul/ultrachat-10k-chatml"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trust_remote_code"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    use_auth_token"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# if needed")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"peft"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#peft"}},[t._v("#")]),t._v(" PEFT")]),t._v(" "),a("p",[t._v("Hugging Face 的 "),a("strong",[t._v("PEFT")]),t._v(" 代码仓库。")]),t._v(" "),a("h3",{attrs:{id:"_1-项目概览"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-项目概览"}},[t._v("#")]),t._v(" 1. 项目概览")]),t._v(" "),a("p",[a("strong",[t._v("PEFT")]),t._v(" 的全称是 "),a("strong",[t._v("Parameter-Efficient Fine-Tuning")]),t._v("。这是一个非常流行且重要的开源库，它的核心目标是让开发者能够"),a("strong",[t._v("高效地")]),t._v("对大型预训练语言模型进行微调。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("核心问题")]),t._v("：直接微调像 Llama、GPT、T5 这样拥有数十亿甚至数百亿参数的大模型，成本极高。它需要庞大的 GPU 显存来存储模型权重、优化器状态、梯度等，这对于大多数研究者和公司来说是不现实的。")]),t._v(" "),a("li",[a("strong",[t._v("PEFT 的解决方案")]),t._v("：PEFT 提供了一系列技术，"),a("strong",[t._v("只微调模型的一小部分参数")]),t._v("（比如添加一些额外的层或参数），同时保持原始预训练模型的绝大部分参数冻结不变。这种方法能以极小的计算和存储成本，达到接近全参数微调的性能。")])]),t._v(" "),a("p",[a("strong",[t._v("一句话总结：PEFT 让普通开发者也能在自己的硬件上（例如单个消费级 GPU）微调大模型。")])]),t._v(" "),a("h3",{attrs:{id:"_2-核心方法与技术"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-核心方法与技术"}},[t._v("#")]),t._v(" 2. 核心方法与技术")]),t._v(" "),a("p",[t._v("PEFT 库实现了多种主流的参数高效微调方法，以下是其中最关键的几种：")]),t._v(" "),a("ol",[a("li",[a("p",[a("strong",[t._v("LoRA")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("思想")]),t._v("：假设模型在微调过程中的权重变化是低秩的。LoRA 不在原始权重上直接更新，而是为模型中的特定层（通常是注意力层的 Q, K, V, O 矩阵）注入一对小的、可训练的"),a("strong",[t._v("低秩矩阵")]),t._v("。在推理时，这些小矩阵可以与原始权重合并，因此"),a("strong",[t._v("不会引入任何延迟")]),t._v("。")]),t._v(" "),a("li",[a("strong",[t._v("优势")]),t._v("：广泛适用，效果出色，是目前最流行的方法之一。")])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("QLoRA")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("思想")]),t._v("：LoRA 的进一步优化。它首先将原始模型权重量化为 4-bit（大大减少显存占用），然后在此基础上进行 LoRA 微调。它还引入了分页优化器等技巧来防止显存溢出。")]),t._v(" "),a("li",[a("strong",[t._v("优势")]),t._v("："),a("strong",[t._v("显存需求极低")]),t._v("！使得在单张 24GB 的 GPU 上微调 70B 参数的模型成为可能。")])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("Prompt Tuning / Prefix Tuning")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("思想")]),t._v("：不修改模型本身，而是在输入序列前添加一些可训练的"),a("strong",[t._v("软提示向量")]),t._v("。模型通过这些提示向量来适应下游任务。")]),t._v(" "),a("li",[a("strong",[t._v("区别")]),t._v("：Prompt Tuning 直接训练这些向量；Prefix Tuning 通过一个小型的前馈网络来生成这些向量，训练的是这个网络的参数。")])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("AdaLoRA")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("思想")]),t._v("：LoRA 的自适应版本。它不是为所有选定的层分配固定的秩，而是根据重要性评分动态地调整每个 LoRA 模块的秩（参数预算），将更多的参数分配给更重要的模块。")]),t._v(" "),a("li",[a("strong",[t._v("优势")]),t._v("：在相同的参数预算下，通常能获得比标准 LoRA 更好的性能。")])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("IA³")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("思想")]),t._v("：通过引入可训练的"),a("strong",[t._v("缩放向量")]),t._v("来放大模型内部激活值的重要部分。这些向量被添加到注意力机制和前馈网络中的特定位置。")]),t._v(" "),a("li",[a("strong",[t._v("优势")]),t._v("：需要训练的参数比 LoRA 更少，速度更快。")])])])]),t._v(" "),a("h3",{attrs:{id:"_3-项目结构与代码组织"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-项目结构与代码组织"}},[t._v("#")]),t._v(" 3. 项目结构与代码组织")]),t._v(" "),a("p",[t._v("仓库的结构非常清晰，主要部分在 "),a("code",[t._v("src/peft")]),t._v(" 目录下：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("src/peft/\n├── __init__.py                         # 导出主要类和函数\n├── config.py                           # 所有 PEFT 方法的配置类\n├── mapping.py                          # 模型名称与自动 PEFT 配置的映射\n├── tuners/                             # **核心目录：各种微调方法的实现**\n│   ├── __init__.py\n│   ├── lora.py                         # LoRA, QLoRA 的实现\n│   ├── prefix_tuning.py               # Prefix Tuning 的实现\n│   ├── prompt_tuning.py               # Prompt Tuning 的实现\n│   ├── adalora.py                     # AdaLoRA 的实现\n│   └── ... (其他方法)\n├── trainer.py                          # 与 Hugging Face Trainer 的集成\n├── utils/                              # 工具函数\n│   ├── constants.py                    # 常量定义\n│   ├── save_and_load.py               # 模型保存和加载逻辑\n│   └── ...\n└── import_utils.py                     # 动态导入辅助函数\n")])])]),a("p",[a("strong",[t._v("关键设计模式：")]),t._v("\nPEFT 库采用了"),a("strong",[t._v("配置类")]),t._v(" + "),a("strong",[t._v("模型包装器")]),t._v("的设计模式。")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("配置类")]),t._v("（如 "),a("code",[t._v("LoraConfig")]),t._v("）：你首先创建一个配置对象，指定要使用的方法（如 "),a("code",[t._v('method="lora"')]),t._v("）及其超参数（如 "),a("code",[t._v("r=8")]),t._v(", "),a("code",[t._v("lora_alpha=16")]),t._v("）。")]),t._v(" "),a("li",[a("strong",[t._v("模型包装器")]),t._v("：通过 "),a("code",[t._v("get_peft_model(model, config)")]),t._v(" 函数，该函数会根据配置，将原始模型包装成一个 "),a("code",[t._v("PeftModel")]),t._v("。这个 "),a("code",[t._v("PeftModel")]),t._v(" 内部会冻结原始模型的参数，并注入可训练的参数（如 LoRA 矩阵）。")])]),t._v(" "),a("h3",{attrs:{id:"_4-主要特性与优势"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-主要特性与优势"}},[t._v("#")]),t._v(" 4. 主要特性与优势")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("与 Transformers 无缝集成")]),t._v("：这是 PEFT 成功的关键。它可以轻松地与 Hugging Face "),a("code",[t._v("transformers")]),t._v(" 库中的任何模型结合使用，API 设计非常友好。")]),t._v(" "),a("li",[a("strong",[t._v("支持多种流行方法")]),t._v("：在一个库中集成了多种 SOTA 方法，方便用户比较和选择。")]),t._v(" "),a("li",[a("strong",[t._v("易于使用")]),t._v("：通常只需要几行代码就可以将一个大模型转换为可高效微调的 PEFT 模型。")]),t._v(" "),a("li",[a("strong",[t._v("实用性极强")]),t._v("：\n"),a("ul",[a("li",[a("strong",[t._v("节省显存")]),t._v("：大幅降低硬件门槛。")]),t._v(" "),a("li",[a("strong",[t._v("节省时间")]),t._v("：训练参数少，收敛更快。")]),t._v(" "),a("li",[a("strong",[t._v("便于部署")]),t._v("：多个任务可以共享同一个基础模型，只需加载不同的、体积很小的适配器权重（通常只有几 MB 到几十 MB），大大简化了模型管理。")])])]),t._v(" "),a("li",[a("strong",[t._v("支持保存和加载适配器")]),t._v("：可以单独保存训练好的“小权重”（适配器），并轻松地加载到基础模型上，实现灵活的任务切换。")])]),t._v(" "),a("h3",{attrs:{id:"_5-示例代码"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-示例代码"}},[t._v("#")]),t._v(" 5. 示例代码")]),t._v(" "),a("p",[t._v("以下是一个典型的使用 LoRA 微调模型的代码片段：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoModelForCausalLM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AutoTokenizer\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LoraConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 加载预训练模型和分词器")]),t._v("\nmodel_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bigscience/bloomz-560m"')]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModelForCausalLM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 定义 LoRA 配置")]),t._v("\nlora_config "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LoraConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  r"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("           "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LoRA 的秩")]),t._v("\n  lora_alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 缩放参数")]),t._v("\n  target_modules"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"query_key_value"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 针对 BLOOM 模型的注意力层")]),t._v("\n  lora_dropout"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 包装模型，得到 PeftModel")]),t._v("\npeft_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_peft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lora_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npeft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_trainable_parameters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 打印可训练参数数量，会发现只占原模型的一小部分")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 接下来就可以像往常一样使用 Hugging Face Trainer 进行训练了")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ... (准备数据，设置 TrainingArguments)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# trainer = Trainer(model=peft_model, ...)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# trainer.train()")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5. 保存适配器（只保存 LoRA 权重）")]),t._v("\npeft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./my_lora_model"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6. 加载适配器进行推理")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# from peft import PeftModel")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# model = AutoModelForCausalLM.from_pretrained(model_name)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# model = PeftModel.from_pretrained(model, "./my_lora_model")')]),t._v("\n")])])]),a("h3",{attrs:{id:"_6-总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-总结"}},[t._v("#")]),t._v(" 6. 总结")]),t._v(" "),a("p",[a("strong",[t._v("PEFT 仓库是一个设计精良、功能强大且极其实用的工具库。")]),t._v(" 它深刻地把握住了大模型时代的关键痛点——微调成本，并通过提供一套统一、易用的 API 来解决这个问题。它的成功不仅在于技术的先进性，更在于其出色的工程实现和与 Hugging Face 生态系统的深度集成，极大地推动了大模型技术的民主化。")]),t._v(" "),a("p",[a("strong",[t._v("如果你打算在自己的项目中对大模型进行微调，PEFT 绝对是你的首选工具之一。")])]),t._v(" "),a("h2",{attrs:{id:"peft-bash-run-peft-sh"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#peft-bash-run-peft-sh"}},[t._v("#")]),t._v(" PEFT bash run_peft.sh")]),t._v(" "),a("p",[t._v("执行输出")]),t._v(" "),a("div",{staticClass:"language-Text extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [05:14&lt;00:00, 52.38s/it]\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nSize of the train set: 10000. Size of the validation set: 2000\nA sample of train dataset: {'content': \"&lt;|im_start|>user\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages &amp; Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages &amp; Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?&lt;|im_end|>\\n&lt;|im_start|>assistant\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.&lt;|im_end|>\\n&lt;|im_start|>user\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?&lt;|im_end|>\\n&lt;|im_start|>assistant\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.&lt;|im_end|>\\n&lt;|im_start|>user\\nCan you provide me with a link to the documentation for my theme?&lt;|im_end|>\\n&lt;|im_start|>assistant\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.&lt;|im_end|>\\n&lt;|im_start|>user\\nCan you confirm if this feature also works for the Quick Shop section of my theme?&lt;|im_end|>\\n&lt;|im_start|>assistant\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.&lt;|im_end|>\\n\"}\n[2025-09-26 18:18:14,200] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-09-26 18:18:16,092] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nUsing auto half precision backend\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32776, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): MistralRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32776, bias=False)\n    )\n  )\n)\ntrainable params: 20,971,520 || all params: 7,269,060,608 || trainable%: 0.2885\nThe following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: content. If content are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 10,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed &amp; accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 2,500\n  Number of trainable parameters = 20,971,520\n")])])]),a("hr"),t._v(" "),a("h2",{attrs:{id:"lora"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lora"}},[t._v("#")]),t._v(" LoRA")]),t._v(" "),a("p",[a("img",{attrs:{src:"/hmblog//images/fine-tuning/Lora1.png",alt:"Lora 思想"}})]),t._v(" "),a("ul",[a("li",[a("p",[t._v("具体做法：在矩阵计算中增加一个旁系分支，旁系分支由两个低秩矩阵A和B组成。")])]),t._v(" "),a("li",[a("p",[t._v("做法：对指定参数增加额外的低秩矩阵，也就是在原始的PLM旁边，增加一个旁路(智泊的说法)")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"/hmblog//images/fine-tuning/Lora2.png",alt:"Lora 训练中 和 训练完成发生了什么"}})]),t._v(" "),a("ul",[a("li",[a("p",[t._v("在优化的过程中，我们是不会对原始权重进行优化的，这也符合参数高效微调的特点：只更新部分参数")])]),t._v(" "),a("li",[a("p",[t._v("更新的这部分参数，就是"),a("code",[t._v("低秩矩阵A")]),t._v(" 和 "),a("code",[t._v("低秩矩阵B")])])])]),t._v(" "),a("blockquote",[a("p",[t._v("训练完成后，是把这2个低秩矩阵和原始模型中的权重，进行合并")])]),t._v(" "),a("ul",[a("li",[t._v("合并步骤： 将A*B 相乘的，得到的值和 W 是一样的，新的模型，就是合并之后的模型，看起来和 原始模型没有差别了")]),t._v(" "),a("li",[t._v("这样做的好处，相较于之前的 prompt-tuning, "),a("code",[t._v("不需要额外的计算量")])]),t._v(" "),a("li",[t._v("直白点说，一个模型，加了"),a("code",[t._v("LoRA")]),t._v(" 合并之后，和 没加LoRA 之前，其实是一个模型")]),t._v(" "),a("li",[t._v("而 用prompt-tuning  or p-tuning or prefix-tuning 的话，终究是有一些额外的计算量")])]),t._v(" "),a("blockquote",[a("p",[t._v("LoRA 核心，一句话总结:")])]),t._v(" "),a("ul",[a("li",[t._v("在每一个要计算的大的矩阵(权重)旁边，新起一条分支，")]),t._v(" "),a("li",[t._v("这个分支的话，是由两个小矩阵组成")]),t._v(" "),a("li",[t._v("那我更新的时候只更新这两个小矩阵")]),t._v(" "),a("li",[t._v("训练完成之后，再把它合并回去，这就是LoRA")])]),t._v(" "),a("h3",{attrs:{id:"实战部分"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#实战部分"}},[t._v("#")]),t._v(" 实战部分")]),t._v(" "),a("blockquote",[a("p",[t._v("所需环境或者依赖: transformer、peft、accelerate")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LoraConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TaskType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n\nconfig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LoraConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("task_type"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("TaskType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CAUSAL_LM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LoraConfig 参数配置， 比如 r：多少秩 ？ ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 为模型引入lora 配置，并打印model")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_peft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 打印这个model，随便看一层，能看到有 input_layernorm, self_attention(里面有query_key_value)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# query_key_value: 是一个 Linear(也就是全连接层) 里面有lora_A  和  lora_B")]),t._v("\n")])])]),a("h3",{attrs:{id:"原理部分"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#原理部分"}},[t._v("#")]),t._v(" 原理部分")]),t._v(" "),a("h4",{attrs:{id:"_1-loraconfig-这个类里"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-loraconfig-这个类里"}},[t._v("#")]),t._v(" 1. LoraConfig 这个类里，")]),t._v(" "),a("ul",[a("li",[t._v("能指定的参数有："),a("code",[t._v("r")]),t._v(", "),a("code",[t._v("target_modules")]),t._v(", "),a("code",[t._v("lora_alpha")]),t._v(", "),a("code",[t._v("lora_dropout")]),t._v(", "),a("code",[t._v("bias")]),t._v(", "),a("code",[t._v("modules_to_save")]),t._v(" 等\n"),a("ul",[a("li",[a("code",[t._v("target_modules")]),t._v(" 是个数组，里面的值可以从导的模块里："),a("code",[t._v("TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING")]),t._v("，点进去能看到好多，比如："),a("code",[t._v("bloom")])]),t._v(" "),a("li",[a("code",[t._v("target_modules")]),t._v(' 还支持指定正则表达式，config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[".*.1.*query_key_value"])')])])]),t._v(" "),a("li",[a("code",[t._v("lora_alpha")]),t._v(" 是对lora 这部分的权重，进行缩放的一个控制，lora的更新占的比重更大一些，就可以调整这个值")]),t._v(" "),a("li",[a("code",[t._v("modules_to_save")]),t._v(" 可以把你想训练的那部分存进去，它就会进行训练，而且会把这部分的权重保留下来，")]),t._v(" "),a("li",[a("code",[t._v("modules_to_save")]),t._v(' 你除了lora 部分以外，你还要训练哪些参数config = LoraConfig(..., modules_to_save=["word_embeddings"])')])]),t._v(" "),a("h4",{attrs:{id:"_2-训练完之后-怎么进行合并的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-训练完之后-怎么进行合并的"}},[t._v("#")]),t._v(" 2. 训练完之后，怎么进行合并的")]),t._v(" "),a("ul",[a("li",[t._v("权重合并")])]),t._v(" "),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[t._v("merge_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" peft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("merge_and_unload"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 得到merge 之后的model, 和之前是一样的")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用一个Question 去验证(或者叫推理) ，看结果是否一样")]),t._v("\n")])])]),a("hr"),t._v(" "),a("h2",{attrs:{id:"ia3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ia3"}},[t._v("#")]),t._v(" IA3")]),t._v(" "),a("ul",[a("li",[t._v("通过可学习的向量对激活值进行抑制或放大。")]),t._v(" "),a("li",[t._v("具体来说，会对"),a("code",[t._v("K、V、FFN 三部分")]),t._v("的值进行调整")]),t._v(" "),a("li",[t._v("训练过程中同样冻结原始模型的权重，只更新可学习的向量部分。")]),t._v(" "),a("li",[t._v("训练完成后，与LoRA 类似，也可以将学习部分的参数与原始权重合并，没有额外的推理开销。")])]),t._v(" "),a("blockquote",[a("p",[t._v("一句话描述： 通过一个"),a("code",[t._v("可学习的向量")]),t._v(", 然后对计算过程中的激活值, 进行抑制或放大，从而达到对整个模型进行调整的一个效果。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/hmblog//images/fine-tuning/IA3-1.png",alt:"Lora"}})]),t._v(" "),a("ol",[a("li",[t._v("图中的lv lk  lff  就是对应上述提到的，会对"),a("code",[t._v("K、V、FFN 三部分")]),t._v("的值进行调整")])]),t._v(" "),a("ul",[a("li",[t._v("优势： 真的调了非常非常少的参数，就可以让模型的效果非常好")]),t._v(" "),a("li",[t._v("具体使用哪种方案，看任务需求")])]),t._v(" "),a("h3",{attrs:{id:"实战部分-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#实战部分-2"}},[t._v("#")]),t._v(" 实战部分")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" IA3Config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TaskType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n\nconfig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" IA3Config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("task_type"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("TaskType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CAUSAL_LM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注意里面的 feedforward_modules=")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 还有target_modules")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 配置训练参数，重点注意：学习率:`learning_rate`, 官方推荐的是:3e-3")]),t._v("\n")])])]),a("h3",{attrs:{id:"原理部分-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#原理部分-2"}},[t._v("#")]),t._v(" 原理部分")]),t._v(" "),a("h4",{attrs:{id:"_1-ia3config-这个类里"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-ia3config-这个类里"}},[t._v("#")]),t._v(" 1. IA3Config 这个类里，")]),t._v(" "),a("ul",[a("li",[t._v("能指定的参数有："),a("code",[t._v("target_modules")]),t._v(", "),a("code",[t._v("feedforward_modules")]),t._v(", "),a("code",[t._v("modules_to_save")]),t._v(" 等")]),t._v(" "),a("li",[t._v("这些参数来自： "),a("code",[t._v("TRANSFORMERS_MODELS_TO_IA3_FEEDFORWARD_MODULES_MAPPIING")]),t._v(" 和 "),a("code",[t._v("TRANSFORMERS_MODELS_TO_IA3_TARGET_MODULES_MAPPING")])]),t._v(" "),a("li",[t._v("这2个MAPPING 就是来指定，我在当前这个模型里面，哪一部分"),a("code",[t._v("是我要去进行IA3 这个适配的")])])]),t._v(" "),a("h4",{attrs:{id:"_2-看里面的-linear"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-看里面的-linear"}},[t._v("#")]),t._v(" 2. 看里面的 "),a("code",[t._v("Linear")])]),t._v(" "),a("ul",[a("li",[t._v("看它的"),a("code",[t._v("forward")]),t._v("方法， 看 if self.is_feedforward  和 else 部分\n"),a("ul",[a("li",[t._v("不是的话：先把数据经过这个全连接层, "),a("code",[t._v("F.linear(x, )")]),t._v("，再进行这个IA3 的Scale(或者叫 抑制或放大)")]),t._v(" "),a("li",[t._v("是的话： 先对x 进行一个调整，然后再经过全连接层"),a("code",[t._v("F.linear()")]),t._v(", 相当于就是把可学习的向量lff， 先进行一个调整")])])])]),t._v(" "),a("h4",{attrs:{id:"_3-调整learning-rate为-3e-3-loss-下降会更明显"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-调整learning-rate为-3e-3-loss-下降会更明显"}},[t._v("#")]),t._v(" 3.  调整"),a("code",[t._v("learning_rate")]),t._v("为: 3e-3, loss 下降会更明显")]),t._v(" "),a("h4",{attrs:{id:"_4-跟lora-很相似"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-跟lora-很相似"}},[t._v("#")]),t._v(" 4. 跟LoRA 很相似")])])}),[],!1,null,null,null);a.default=s.exports}}]);