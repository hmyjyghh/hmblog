(window.webpackJsonp=window.webpackJsonp||[]).push([[135],{582:function(t,v,_){"use strict";_.r(v);var n=_(3),e=Object(n.a)({},(function(){var t=this,v=t._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h3",{attrs:{id:"一句话总结"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#一句话总结"}},[t._v("#")]),t._v(" 一句话总结")]),t._v(" "),v("p",[v("strong",[t._v("vLLM 是一个专为大语言模型设计的高性能、低延迟的推理和服务化引擎。")]),t._v(" 它的核心目标就是："),v("strong",[t._v("用同样大小的显卡，让 LLM 服务同时处理更多用户的请求")]),t._v("，从而极大地降低成本、提高效率。")]),t._v(" "),v("p",[t._v("您可以把它想象成 LLM 世界的 "),v("strong",[t._v("“超级服务器”")]),t._v("。")]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"核心价值-解决-内存墙-问题"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#核心价值-解决-内存墙-问题"}},[t._v("#")]),t._v(" 核心价值：解决“内存墙”问题")]),t._v(" "),v("p",[t._v("在 vLLM 出现之前，像使用 Hugging Face Transformers 这样的库来部署 LLM 服务时，会遇到一个巨大的瓶颈——"),v("strong",[t._v("KV Cache 的内存管理效率极低")]),t._v("。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("什么是 KV Cache？")]),t._v(" LLM 在生成文本（如回答问题）时，是一个字一个字地蹦出来的。每生成下一个字，它都需要参考之前已经生成的所有字的信息。为了避免重复计算，系统会把中间计算结果（称为 Key 和 Value 向量）缓存起来，这个缓存就是 "),v("strong",[t._v("KV Cache")]),t._v("。")]),t._v(" "),v("li",[v("strong",[t._v("问题所在")]),t._v("：传统的系统会为每个用户请求"),v("strong",[t._v("预留一大块连续内存")]),t._v("来存放 KV Cache，以防生成过程很长。但实际上，大部分请求可能很短，这就导致了大量内存被白浪费（内部碎片）。同时，频繁的分配和释放也会产生内存空洞（外部碎片）。")])]),t._v(" "),v("p",[v("strong",[t._v("这就好比：")])]),t._v(" "),v("blockquote",[v("p",[t._v("一个停车场，每当来一辆车，就为它预留 10 个连续车位，因为它“可能”会来一群朋友。结果大部分车都只有司机一人，导致 9 个车位空着，其他车也停不进来。停车场看似满了，但实际利用率很低。")])]),t._v(" "),v("p",[t._v("vLLM 就是为了解决这个“停车场”效率问题而生的。")]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"vllm-的杀手锏-pagedattention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#vllm-的杀手锏-pagedattention"}},[t._v("#")]),t._v(" vLLM 的杀手锏：PagedAttention")]),t._v(" "),v("p",[t._v("vLLM 的核心技术是它创新的内存管理算法——"),v("strong",[t._v("PagedAttention")]),t._v("。这个技术的灵感来自于"),v("strong",[t._v("操作系统的虚拟内存和分页机制")]),t._v("。")]),t._v(" "),v("p",[v("strong",[t._v("它的工作方式如下：")])]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("分块")]),t._v("：vLLM 将 GPU 显存划分为许多个"),v("strong",[t._v("固定大小的内存块")]),t._v("，就像停车场划好了很多个标准车位。")]),t._v(" "),v("li",[v("strong",[t._v("按需分配")]),t._v("：当一个用户请求到来时，系统不再一次性预留大量空间，而是按实际需要，每次分配一个或多个"),v("strong",[t._v("内存块")]),t._v("给它。生成了多少内容，就用多少块。")]),t._v(" "),v("li",[v("strong",[t._v("非连续但逻辑连续")]),t._v("：这些块在物理上可能是不连续的，但 vLLM 内部通过一个“映射表”来管理，让模型在计算时感觉像是在访问一块连续的内存。")])]),t._v(" "),v("p",[v("strong",[t._v("继续用停车场的比喻：")])]),t._v(" "),v("blockquote",[v("p",[t._v("vLLM 管理的停车场不再为每辆车预留固定连续车位。来了一辆车，就给它一个车位。如果它需要接人（生成长文本），再来一辆车，就再给一个车位，这个车位可能在停车场的任何角落，但系统通过一个“智能管理员”（块表）记得所有这些车是属于同一个“车队”的。这样就最大限度地利用了每一个车位。")])]),t._v(" "),v("h3",{attrs:{id:"vllm-的主要特点和优势"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#vllm-的主要特点和优势"}},[t._v("#")]),t._v(" vLLM 的主要特点和优势")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("极高的吞吐量")]),t._v("：通过 PagedAttention，vLLM 的显存利用率接近 100%，几乎没有浪费。这意味着在同样大小的显卡上，它可以同时处理比传统方案（如 Hugging Face）多得多的高并发请求。官方数据显示，吞吐量"),v("strong",[t._v("最高可提升 24 倍")]),t._v("。")]),t._v(" "),v("li",[v("strong",[t._v("高效的内存共享")]),t._v("：对于需要生成多个候选结果（如并行采样、集束搜索）的场景，vLLM 可以让这些候选序列"),v("strong",[t._v("共享")]),t._v("它们共同前缀的 KV Cache，避免了重复存储，进一步节省了显存。")]),t._v(" "),v("li",[v("strong",[t._v("易于使用")]),t._v("：\n"),v("ul",[v("li",[t._v("它与流行的 Hugging Face 模型完全兼容，通常只需要改动几行代码就能将现有服务切换到 vLLM。")]),t._v(" "),v("li",[t._v("它提供了与 "),v("strong",[t._v("OpenAI API 完全兼容的接口")]),t._v("，这意味着任何可以使用 OpenAI API 的客户端应用都可以无缝切换到私有的 vLLM 服务。")])])]),t._v(" "),v("li",[v("strong",[t._v("支持连续批处理")]),t._v("：当一个批次中的某些请求先完成时，vLLM 会立即释放其资源并接入新的等待请求，让 GPU 时刻保持忙碌，进一步提升了计算效率。")])]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"总结"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),v("p",[v("strong",[t._v("vLLM 是什么？")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("角色上")]),t._v("：它是一个 LLM 推理加速器和部署框架。")]),t._v(" "),v("li",[v("strong",[t._v("核心上")]),t._v("：它通过革命性的 "),v("strong",[t._v("PagedAttention")]),t._v(" 算法，极致地优化了 LLM 推理过程中最耗资源的 KV Cache 的内存管理。")]),t._v(" "),v("li",[v("strong",[t._v("效果上")]),t._v("：它极大地提升了 GPU 的利用率和服务的吞吐量，降低了单位请求的成本。")]),t._v(" "),v("li",[v("strong",[t._v("生态上")]),t._v("：它简单易用，兼容性强，是目前业界部署高性能 LLM 服务的"),v("strong",[t._v("事实标准")]),t._v("。")])]),t._v(" "),v("p",[t._v("无论是公司还是研究者，当他们需要将一个大模型（如 LLaMA, ChatGLM, Qwen 等）高效、低成本地提供给大量用户使用时，vLLM 通常是他们的首选方案。")]),t._v(" "),v("h2",{attrs:{id:"_2-它的底层原理-pagedattention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-它的底层原理-pagedattention"}},[t._v("#")]),t._v(" 2. 它的底层原理: PagedAttention")]),t._v(" "),v("p",[t._v("vLLM的核心理念和巨大成功，很大程度上都归功于其创新的底层算法——"),v("strong",[t._v("PagedAttention")]),t._v("。")]),t._v(" "),v("p",[t._v("我们可以把它理解为一个“神来之笔”的类比："),v("strong",[t._v("就像操作系统（OS）管理物理内存一样，vLLM用PagedAttention来管理KV Cache。")])]),t._v(" "),v("p",[t._v("下面我为你详细拆解一下它的原理。")]),t._v(" "),v("h3",{attrs:{id:"_1-背景-llm推理的瓶颈-kv-cache"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-背景-llm推理的瓶颈-kv-cache"}},[t._v("#")]),t._v(" 1. 背景：LLM推理的瓶颈——KV Cache")]),t._v(" "),v("p",[t._v("在详细讲解PagedAttention之前，必须先理解它要解决的核心问题。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("自回归生成")]),t._v("：LLM（大语言模型）生成文本是一个一个token进行的。生成下一个token时，需要基于之前所有已生成的token。")]),t._v(" "),v("li",[v("strong",[t._v("KV Cache")]),t._v("：为了避免在生成每个新token时都重新计算所有之前token的Key和Value向量（Transformer Decoder层的核心），人们引入了KV Cache。它将之前所有token的K和V向量缓存起来，这样在计算下一个token时，只需要计算当前token的K、V，然后与缓存中的K、V一起进行Attention计算即可。")])]),t._v(" "),v("p",[v("strong",[t._v("这带来了巨大的性能提升，但也引入了一个新问题：内存管理。")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("内存浪费")]),t._v("：在传统的服务系统中，每个请求（序列）的KV Cache在内存中都是连续分配的。\n"),v("ul",[v("li",[v("strong",[t._v("内部碎片化")]),t._v("：由于序列生成长度不确定，你不得不为每个序列预留一个“可能的最大长度”的内存。比如，你预设最大生成长度为2048，但很多序列可能只生成了500个token，那么剩下的1548个token位置的内存就被浪费了。")]),t._v(" "),v("li",[v("strong",[t._v("外部碎片化")]),t._v("：序列有生有灭，不断分配和释放这些连续的大内存块，会在内存中产生很多“空洞”，导致即使总空闲内存足够，也无法分配一个新的连续大内存块。")])])])]),t._v(" "),v("p",[v("strong",[t._v("正是这个内存管理问题，严重限制了LLM服务的吞吐量")]),t._v("。")]),t._v(" "),v("h3",{attrs:{id:"_2-pagedattention-灵感来自操作系统的虚拟内存和分页"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-pagedattention-灵感来自操作系统的虚拟内存和分页"}},[t._v("#")]),t._v(" 2. PagedAttention：灵感来自操作系统的虚拟内存和分页")]),t._v(" "),v("p",[t._v("vLLM的开发者从操作系统的虚拟内存和分页机制中获得了灵感，并巧妙地将其应用到了KV Cache的管理上。")]),t._v(" "),v("p",[t._v("它的核心思想是：")]),t._v(" "),v("p",[v("strong",[t._v("将每个序列的KV Cache逻辑上视为一个连续空间，但物理上分割成多个非连续的、固定大小的“块”（Block）。")])]),t._v(" "),v("p",[t._v("我们来对照操作系统的概念来理解：")]),t._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[t._v("概念")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("操作系统")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("vLLM (PagedAttention)")])])]),t._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("虚拟内存")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("进程看到的连续、独立的地址空间")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("一个序列"),v("strong",[t._v("逻辑上")]),t._v("的KV Cache")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("物理内存")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("实际的、不连续的物理内存条")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("GPU的"),v("strong",[t._v("连续显存池")])])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("页")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("固定大小的内存块（如4KB）")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("固定大小的"),v("strong",[t._v("块")]),t._v("，可存储一定数量token的K和V向量（如16个token）")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("页表")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("记录虚拟页到物理页的映射关系")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("块表")]),t._v("，记录序列的逻辑块到物理块的映射")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("缺页中断")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("访问的页不在物理内存中")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("（在vLLM中基本避免，因为管理在显存内）")])])])]),t._v(" "),v("h3",{attrs:{id:"_3-pagedattention-的工作流程"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-pagedattention-的工作流程"}},[t._v("#")]),t._v(" 3. PagedAttention 的工作流程")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("初始化内存池")]),t._v("：\nvLLM启动时，会向GPU显存申请一个大的、连续的内存池，并将其划分为大量"),v("strong",[t._v("大小相等的块")]),t._v("。每个块可以容纳固定数量的token（例如，"),v("code",[t._v("block_size=16")]),t._v("）的K和V向量。")])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("序列生成与块分配")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("当一个新的序列请求到来时，系统会为它创建一个"),v("strong",[t._v("逻辑上的“块表”")]),t._v("。")]),t._v(" "),v("li",[t._v("生成第一个token时，系统从"),v("strong",[t._v("空闲块列表")]),t._v("中分配一个"),v("strong",[t._v("物理块")]),t._v("给它，并将映射关系记录在块表中。这个块可以存储前16个token的KV Cache。")]),t._v(" "),v("li",[t._v("当序列长度超过16时，系统再分配一个新的物理块，并更新块表。这个新块在物理上可能与第一个块不连续，但在逻辑上，序列认为它们是连续的。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("Attention计算")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("当需要进行Attention计算时（例如，生成第17个token），vLLM的PagedAttention内核会根据该序列的"),v("strong",[t._v("块表")]),t._v("，去不同的物理块中 gathering（收集）所需的K和V向量。")]),t._v(" "),v("li",[t._v("然后，将这些收集来的K、V向量与当前token的Q向量一起进行Attention计算。")]),t._v(" "),v("li",[t._v("这个过程对模型来说是透明的，它仍然“感觉”自己在访问一个连续的KV Cache。")])])])]),t._v(" "),v("h3",{attrs:{id:"_4-pagedattention-带来的巨大优势"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-pagedattention-带来的巨大优势"}},[t._v("#")]),t._v(" 4. PagedAttention 带来的巨大优势")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("近乎零内存浪费")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("内存分配单位从“整个序列可能的最大长度”变成了“一个小的块”。")]),t._v(" "),v("li",[t._v("序列需要多长，就分配多少个块，最后一个块没满也没关系，浪费的空间很小（内部碎片化极低）。")]),t._v(" "),v("li",[t._v("所有序列共享同一个物理块池，块可以被高效复用，避免了外部碎片化。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("实现高效的内存共享")]),t._v("：\n这是PagedAttention另一个杀手级特性，尤其对于"),v("strong",[t._v("并行采样")]),t._v("和"),v("strong",[t._v("Beam Search")]),t._v("。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("场景")]),t._v("：多个序列可能共享同一个前缀（例如，同一个问题的多个回答）。")]),t._v(" "),v("li",[v("strong",[t._v("实现")]),t._v("：在PagedAttention中，这些序列可以"),v("strong",[t._v("直接共享存储前缀token的物理块")]),t._v("。系统只需在它们的块表中记录指向"),v("strong",[t._v("同一个物理块")]),t._v("的映射即可。")]),t._v(" "),v("li",[v("strong",[t._v("好处")]),t._v("：避免了重复存储共享前缀的KV Cache，节省了大量显存。这在传统连续分配方案中很难高效实现。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("高吞吐量")]),t._v("：\n正是由于极佳的内存利用率，vLLM可以在相同的GPU显存下，同时处理"),v("strong",[t._v("更多并发请求")]),t._v("，从而极大地提升了"),v("strong",[t._v("吞吐量")]),t._v("。官方数据显示，在某些场景下，吞吐量比HuggingFace Transformers高出24倍。")])])]),t._v(" "),v("h3",{attrs:{id:"总结-2"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结-2"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),v("p",[v("strong",[t._v("PagedAttention是vLLM的灵魂所在。")])]),t._v(" "),v("p",[t._v("它通过将操作系统成熟的内存分页管理思想引入到LLM推理中最耗资源的KV Cache管理中，巧妙地解决了内存碎片化和利用率低下的核心痛点。它不仅减少了浪费，还通过内存共享机制进一步优化，最终使得LLM服务能够在有限的GPU显存下实现前所未有的高吞吐量，成为了LLM部署领域一个里程碑式的技术。")]),t._v(" "),v("h2",{attrs:{id:"vllm、sglang-和-tensorrt"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#vllm、sglang-和-tensorrt"}},[t._v("#")]),t._v(" VLLM、SGLang 和 TensorRT")]),t._v(" "),v("p",[t._v("VLLM、SGLang和TensorRT都是在人工智能领域，尤其是在大语言模型推理优化方面有重要应用的工具，以下为你详细介绍：")]),t._v(" "),v("h3",{attrs:{id:"vllm"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#vllm"}},[t._v("#")]),t._v(" VLLM")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("简介")]),t._v("：VLLM是加州大学伯克利分校开发的一个高性能的大语言模型推理和服务库。它旨在显著加速大语言模型的推理速度，降低推理成本，使得在生产环境中部署大语言模型变得更加高效。")]),t._v(" "),v("li",[v("strong",[t._v("核心特性")]),t._v("：\n"),v("ul",[v("li",[v("strong",[t._v("PagedAttention")]),t._v("：这是VLLM的关键创新技术，它解决了传统注意力机制在处理长序列时内存碎片化和低效的问题。通过类似操作系统内存分页的机制，PagedAttention能够更高效地管理KV缓存，大幅减少内存占用，提升推理速度，尤其是在处理长文本生成任务时优势明显。")]),t._v(" "),v("li",[v("strong",[t._v("并行采样")]),t._v("：支持并行生成多个输出序列，充分利用GPU的并行计算能力，极大提高了推理的吞吐量，适用于需要同时处理多个请求的场景，比如聊天机器人的多用户并发访问。")]),t._v(" "),v("li",[v("strong",[t._v("兼容多种模型")]),t._v("：对常见的大语言模型架构，如GPT系列、Llama及其衍生模型等都有良好的支持，用户可以方便地使用VLLM加速这些模型的推理。")])])])]),t._v(" "),v("h3",{attrs:{id:"sglang"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#sglang"}},[t._v("#")]),t._v(" SGLang")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("简介")]),t._v("：SGLang（Simple GPU Language）是一种用于在GPU上高效执行大语言模型推理的特定领域语言。它专注于简化大语言模型在GPU上的计算过程，通过对计算图的优化和编译，实现高效的推理性能。")]),t._v(" "),v("li",[v("strong",[t._v("核心特性")]),t._v("：\n"),v("ul",[v("li",[v("strong",[t._v("计算图优化")]),t._v("：SGLang能够对大语言模型的计算图进行深度分析和优化，去除冗余计算，合并可并行的操作，从而减少推理时间。")]),t._v(" "),v("li",[v("strong",[t._v("硬件适配性")]),t._v("：针对不同的GPU硬件架构进行定制化优化，充分发挥GPU的计算潜力。它可以根据GPU的核心数量、内存带宽等特性，生成最优的执行代码，提高计算资源的利用率。")]),t._v(" "),v("li",[v("strong",[t._v("易于集成")]),t._v("：设计上考虑了与现有深度学习框架的兼容性，方便开发者将其集成到已有的模型开发和部署流程中，不需要对模型代码进行大规模重写。")])])])]),t._v(" "),v("h3",{attrs:{id:"tensorrt"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#tensorrt"}},[t._v("#")]),t._v(" TensorRT")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("简介")]),t._v("：TensorRT是NVIDIA推出的一款高性能的深度学习推理优化器和运行时库，主要用于在NVIDIA GPU上加速深度学习推理。它支持多种深度学习框架，如TensorFlow、PyTorch等导出的模型，能显著提升模型的推理速度，广泛应用于图像识别、自然语言处理、自动驾驶等多个领域。")]),t._v(" "),v("li",[v("strong",[t._v("核心特性")]),t._v("：\n"),v("ul",[v("li",[v("strong",[t._v("层融合技术")]),t._v("：将多个神经网络层进行合并和优化，减少层与层之间的数据传输和计算开销。例如，将卷积层、偏置层和激活函数层融合成一个计算单元，减少内存访问次数，加快推理速度。")]),t._v(" "),v("li",[v("strong",[t._v("精度校准")]),t._v("：支持模型量化技术，包括INT8、FP16等低精度数据格式，在几乎不损失模型精度的前提下，大幅减少计算量和内存占用，提高推理效率。")]),t._v(" "),v("li",[v("strong",[t._v("动态张量显存管理")]),t._v("：能够根据模型推理过程中的实际需求，动态分配和释放显存，避免显存浪费，提高GPU显存的使用效率，支持在资源有限的环境下运行大型模型。")])])])]),t._v(" "),v("p",[t._v("总体而言，VLLM侧重于大语言模型推理加速和服务，SGLang聚焦于为大语言模型在GPU上提供高效的计算语言支持，而TensorRT则是一个通用的深度学习推理优化工具，对多种深度学习任务和框架都有广泛的支持。")])])}),[],!1,null,null,null);v.default=e.exports}}]);