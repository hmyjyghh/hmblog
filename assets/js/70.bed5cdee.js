(window.webpackJsonp=window.webpackJsonp||[]).push([[70],{556:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("LangChain 有很多优秀的替代方案，每个都有不同的设计哲学和适用场景。以下是主要的替代方案：")]),t._v(" "),s("h2",{attrs:{id:"_1-llamaindex-最流行的替代品"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-llamaindex-最流行的替代品"}},[t._v("#")]),t._v(" 1. "),s("strong",[t._v("LlamaIndex")]),t._v("（最流行的替代品）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" llama_index "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VectorStoreIndex"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" SimpleDirectoryReader\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" llama_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("llms "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OpenAI\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更专注于数据连接和检索")]),t._v("\ndocuments "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SimpleDirectoryReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nindex "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VectorStoreIndex"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_documents"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nquery_engine "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("as_query_engine"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresponse "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" query_engine"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"你的问题"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("特点")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("专长于数据连接和检索增强生成(RAG)")]),t._v(" "),s("li",[t._v("更简单的API，学习曲线平缓")]),t._v(" "),s("li",[t._v("优秀的文档和向量存储集成")])]),t._v(" "),s("h2",{attrs:{id:"_2-haystack-企业级选择"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-haystack-企业级选择"}},[t._v("#")]),t._v(" 2. "),s("strong",[t._v("Haystack")]),t._v("（企业级选择）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" haystack "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" haystack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("document_stores "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" InMemoryDocumentStore\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" haystack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" EmbeddingRetriever"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" PromptNode\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 面向生产环境的管道设计")]),t._v("\ndocument_store "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" InMemoryDocumentStore"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nretriever "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" EmbeddingRetriever"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("document_store"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("document_store"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nprompt_node "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PromptNode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name_or_path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpt-3.5-turbo"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\npipeline "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("component"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("retriever"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Retriever"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inputs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Query"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("component"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("prompt_node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PromptNode"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inputs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Retriever"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("特点")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("企业级稳定性和性能")]),t._v(" "),s("li",[t._v("强大的管道(Pipeline)系统")]),t._v(" "),s("li",[t._v("优秀的监控和部署工具")])]),t._v(" "),s("h2",{attrs:{id:"_3-semantic-kernel-微软出品"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-semantic-kernel-微软出品"}},[t._v("#")]),t._v(" 3. "),s("strong",[t._v("Semantic Kernel")]),t._v("（微软出品）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" semantic_kernel "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sk\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" semantic_kernel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("planning "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BasicPlanner\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 微软的AI应用框架")]),t._v("\nkernel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Kernel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplanner "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BasicPlanner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplan "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" planner"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_plan_async"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"任务描述"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("特点")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("微软官方支持，与Azure深度集成")]),t._v(" "),s("li",[t._v("强大的规划(Planning)能力")]),t._v(" "),s("li",[t._v("良好的企业级特性")])]),t._v(" "),s("h2",{attrs:{id:"_4-autogpt-agentgpt-自主代理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-autogpt-agentgpt-自主代理"}},[t._v("#")]),t._v(" 4. "),s("strong",[t._v("AutoGPT")]),t._v(" / "),s("strong",[t._v("AgentGPT")]),t._v("（自主代理）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 专注于自主AI代理")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" autogpt "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoGPT\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" autogpt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agent "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Agent\n\nagent "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Agent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"BusinessAnalyst"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nagent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"分析市场数据并生成报告"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("特点")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("专注于自主任务执行")]),t._v(" "),s("li",[t._v("强大的工具使用能力")]),t._v(" "),s("li",[t._v("适合自动化工作流")])]),t._v(" "),s("h2",{attrs:{id:"_5-transformers-agents-hugging-face"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-transformers-agents-hugging-face"}},[t._v("#")]),t._v(" 5. "),s("strong",[t._v("Transformers Agents")]),t._v("（Hugging Face）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" HfAgent\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 直接使用Hugging Face模型")]),t._v("\nagent "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" HfAgent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://api-inference.huggingface.co/models/bigcode/starcoder"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" agent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"生成一个数据可视化代码"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("特点")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("直接集成Hugging Face生态系统")]),t._v(" "),s("li",[t._v("丰富的预训练模型")]),t._v(" "),s("li",[t._v("开源友好")])]),t._v(" "),s("h2",{attrs:{id:"_6-langchain-lite-替代方案"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-langchain-lite-替代方案"}},[t._v("#")]),t._v(" 6. "),s("strong",[t._v("LangChain Lite")]),t._v(" 替代方案")]),t._v(" "),s("h3",{attrs:{id:"minichain-极简替代"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#minichain-极简替代"}},[t._v("#")]),t._v(" "),s("strong",[t._v("MiniChain")]),t._v("（极简替代）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 超轻量级的链式操作")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" minichain "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OpenAI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform\n\n"),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@prompt")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("OpenAI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("answer_question")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"""基于以下上下文：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("\n    问题：")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('\n    答案："""')])]),t._v("\n\nresponse "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" answer_question"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"上下文内容"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"问题"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"chainlit-专注于ui"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#chainlit-专注于ui"}},[t._v("#")]),t._v(" "),s("strong",[t._v("Chainlit")]),t._v("（专注于UI）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" chainlit "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" cl\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 快速构建ChatGPT式界面")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@cl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("on_message")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("async")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("message"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 处理消息")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" cl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Message"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"回复"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("send"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"_7-专业领域替代方案"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-专业领域替代方案"}},[t._v("#")]),t._v(" 7. "),s("strong",[t._v("专业领域替代方案")])]),t._v(" "),s("h3",{attrs:{id:"对于数据分析-pandas-ai"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#对于数据分析-pandas-ai"}},[t._v("#")]),t._v(" "),s("strong",[t._v("对于数据分析")]),t._v("：Pandas AI")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pandasai "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SmartDataframe\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsdf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SmartDataframe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresponse "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sdf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("chat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"绘制销售趋势图"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"对于对话ai-rasa-botpress"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#对于对话ai-rasa-botpress"}},[t._v("#")]),t._v(" "),s("strong",[t._v("对于对话AI")]),t._v("：Rasa / Botpress")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 企业级对话AI平台")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更适合复杂的对话流程管理")]),t._v("\n")])])]),s("h3",{attrs:{id:"对于搜索引擎-weaviate-chroma"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#对于搜索引擎-weaviate-chroma"}},[t._v("#")]),t._v(" "),s("strong",[t._v("对于搜索引擎")]),t._v("：Weaviate / Chroma")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 专注于向量搜索")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" weaviate\nclient "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weaviate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Client"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"http://localhost:8080"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更专业的向量数据库解决方案")]),t._v("\n")])])]),s("h2",{attrs:{id:"详细对比表格"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#详细对比表格"}},[t._v("#")]),t._v(" 详细对比表格")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("框架")]),t._v(" "),s("th",[t._v("优点")]),t._v(" "),s("th",[t._v("缺点")]),t._v(" "),s("th",[t._v("适用场景")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[s("strong",[t._v("LangChain")])]),t._v(" "),s("td",[t._v("功能全面，生态丰富")]),t._v(" "),s("td",[t._v("学习曲线陡峭，API复杂")]),t._v(" "),s("td",[t._v("复杂AI应用，原型开发")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("LlamaIndex")])]),t._v(" "),s("td",[t._v("RAG专家，API简洁")]),t._v(" "),s("td",[t._v("功能相对专注")]),t._v(" "),s("td",[t._v("文档问答，数据检索")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("Haystack")])]),t._v(" "),s("td",[t._v("企业级稳定，监控完善")]),t._v(" "),s("td",[t._v("相对笨重")]),t._v(" "),s("td",[t._v("生产环境，企业应用")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("Semantic Kernel")])]),t._v(" "),s("td",[t._v("微软生态集成，规划强大")]),t._v(" "),s("td",[t._v("主要面向.NET/Python")]),t._v(" "),s("td",[t._v("企业级，Azure用户")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("AutoGPT")])]),t._v(" "),s("td",[t._v("自主性强，自动化好")]),t._v(" "),s("td",[t._v("资源消耗大")]),t._v(" "),s("td",[t._v("自动化任务，AI代理")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("Transformers Agents")])]),t._v(" "),s("td",[t._v("HF生态丰富，开源友好")]),t._v(" "),s("td",[t._v("依赖HF基础设施")]),t._v(" "),s("td",[t._v("研究，开源项目")])])])]),t._v(" "),s("h2",{attrs:{id:"选择建议"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#选择建议"}},[t._v("#")]),t._v(" 选择建议")]),t._v(" "),s("h3",{attrs:{id:"根据需求选择"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#根据需求选择"}},[t._v("#")]),t._v(" "),s("strong",[t._v("根据需求选择")]),t._v("：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("快速原型开发")]),t._v("：LangChain 或 LlamaIndex")]),t._v(" "),s("li",[s("strong",[t._v("生产环境")]),t._v("：Haystack 或 Semantic Kernel")]),t._v(" "),s("li",[s("strong",[t._v("专注RAG")]),t._v("：LlamaIndex")]),t._v(" "),s("li",[s("strong",[t._v("自主代理")]),t._v("：AutoGPT")]),t._v(" "),s("li",[s("strong",[t._v("轻量级应用")]),t._v("：MiniChain 或 直接使用OpenAI API")]),t._v(" "),s("li",[s("strong",[t._v("企业级")]),t._v("：Haystack、Semantic Kernel")])]),t._v(" "),s("h3",{attrs:{id:"简单项目推荐直接使用sdk"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#简单项目推荐直接使用sdk"}},[t._v("#")]),t._v(" "),s("strong",[t._v("简单项目推荐直接使用SDK")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 很多时候不需要完整框架")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" openai "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" OpenAI\nclient "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" OpenAI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nresponse "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" client"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("chat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("completions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpt-3.5-turbo"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    messages"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"role"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"user"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"content"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"你好"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"迁移示例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#迁移示例"}},[t._v("#")]),t._v(" 迁移示例")]),t._v(" "),s("h3",{attrs:{id:"从-langchain-迁移到-llamaindex"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#从-langchain-迁移到-llamaindex"}},[t._v("#")]),t._v(" 从 LangChain 迁移到 LlamaIndex：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LangChain 方式")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("document_loaders "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TextLoader\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text_splitter "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" CharacterTextSplitter\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vectorstores "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Chroma\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LlamaIndex 方式（更简洁）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" llama_index "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SimpleDirectoryReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VectorStoreIndex\ndocuments "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SimpleDirectoryReader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nindex "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VectorStoreIndex"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_documents"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("documents"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"从-langchain-迁移到-haystack"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#从-langchain-迁移到-haystack"}},[t._v("#")]),t._v(" 从 LangChain 迁移到 Haystack：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LangChain 方式")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agents "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" initialize_agent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Tool\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Haystack 方式（更模块化）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" haystack "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" haystack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nodes "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" PromptNode\n")])])]),s("h2",{attrs:{id:"总结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),s("p",[s("strong",[t._v("最佳选择策略")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("初学者/简单项目")]),t._v("：LlamaIndex 或 直接使用SDK")]),t._v(" "),s("li",[s("strong",[t._v("企业生产环境")]),t._v("：Haystack")]),t._v(" "),s("li",[s("strong",[t._v("研究/实验")]),t._v("：LangChain（生态丰富）")]),t._v(" "),s("li",[s("strong",[t._v("微软生态")]),t._v("：Semantic Kernel")]),t._v(" "),s("li",[s("strong",[t._v("想要更现代API")]),t._v("：Agno")])]),t._v(" "),s("p",[t._v('没有"最好"的框架，只有最适合你项目需求的框架。对于大多数场景，'),s("strong",[t._v("LlamaIndex")]),t._v(" 是 LangChain 最直接和友好的替代品。")]),t._v(" "),s("p",[t._v("当前中文向量模型的发展非常活跃，多家机构和公司都推出了性能卓越的模型。为了让你能快速了解，我将主流的选择整理成了下面的表格。")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("模型系列 / 来源")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("代表性模型举例")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("核心特点与适用场景")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("阿里通义千问 (Qwen)")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("Qwen3-Embedding")]),t._v("系列 (如8B, 4B版本)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("专注于"),s("strong",[t._v("纯文本任务")]),t._v("，在MTEB等权威榜单上表现优异，支持超100种语言，适合智能搜索、文本分类和RAG 。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("智源研究院 (BGE)")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("BGE-Code-v1")]),t._v(", "),s("code",[t._v("BGE-VL-v1.5")]),t._v(", "),s("code",[t._v("BGE-VL-Screenshot")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("生态丰富，覆盖"),s("strong",[t._v("代码、图文、截图")]),t._v("等多模态检索，在特定领域基准测试中成绩领先，适合需要处理非纯文本数据的场景 。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("火山引擎 (Seed)")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("Seed1.6-Embedding")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("强调"),s("strong",[t._v("全模态")]),t._v("能力，支持文本、图像、视频的混合检索，提供自定义指令功能，适合复杂的多模态和跨模态搜索需求 。")])])])]),t._v(" "),s("h3",{attrs:{id:"💡-如何选择适合的模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#💡-如何选择适合的模型"}},[t._v("#")]),t._v(" 💡 如何选择适合的模型？")]),t._v(" "),s("p",[t._v("面对这些优秀的模型，你可以根据以下思路来做出决策：")]),t._v(" "),s("ul",[s("li",[s("p",[s("strong",[t._v("明确你的任务类型")])]),t._v(" "),s("ul",[s("li",[t._v("如果你的应用场景完全是"),s("strong",[t._v("文本处理")]),t._v("（如文档检索、问答系统），那么"),s("strong",[t._v("Qwen3-Embedding")]),t._v("系列是一个性能强劲且经过广泛验证的选择 。")]),t._v(" "),s("li",[t._v("如果你的需求涉及"),s("strong",[t._v("代码理解")]),t._v("（如代码库搜索）、"),s("strong",[t._v("图像描述检索")]),t._v("或"),s("strong",[t._v("网页/文档截图分析")]),t._v("，那么"),s("strong",[t._v("智源BGE")]),t._v("系列的专用模型会更有优势 。")]),t._v(" "),s("li",[t._v("如果你的业务需要同时处理"),s("strong",[t._v("文本、图片甚至视频")]),t._v("，并希望它们能在同一个向量空间中进行混合检索，可以重点关注"),s("strong",[t._v("火山引擎的Seed模型")]),t._v(" 。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("考虑部署条件")])]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("开源部署")]),t._v("：上述模型基本都在Hugging Face、魔搭（ModelScope）等平台开源，适合希望自行部署、微调以掌控数据和模型的团队 。")]),t._v(" "),s("li",[s("strong",[t._v("API服务调用")]),t._v("：如果你希望快速集成、避免维护模型的复杂性，可以关注模型提供商官方的API服务，例如通过阿里云百炼调用Qwen模型，或通过火山方舟调用Seed模型 。")])])])]),t._v(" "),s("p",[t._v("希望这份梳理能帮助你更好地了解当前的主流选择。如果你能分享一下你计划将模型用在什么具体项目上，或许我可以提供更细致的建议。")]),t._v(" "),s("p",[t._v("好的，作为您的“高级大模型应用开发工程师”同事，我非常乐意为您深入剖析一下 "),s("strong",[t._v("Prompt-tuning")]),t._v(" 这项关键且实用的技术。")]),t._v(" "),s("p",[t._v("如果说传统的"),s("strong",[t._v("微调（Fine-tuning）")]),t._v(" 是给预训练大模型“动手术”，通过更新"),s("strong",[t._v("所有")]),t._v("参数来让它适应新任务，那么 "),s("strong",[t._v("Prompt-tuning 可以看作是一种“催眠术”或“引导术”")]),t._v("。它的核心思想是："),s("strong",[t._v("保持预训练大模型的所有参数冻结（不动），只通过优化一小段可学习的“软提示”（Soft Prompt），来激发模型解决特定任务的潜能。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"一、核心思想-从-硬提示-到-软提示"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、核心思想-从-硬提示-到-软提示"}},[t._v("#")]),t._v(" 一、核心思想：从“硬提示”到“软提示”")]),t._v(" "),s("p",[t._v("为了更好地理解，我们先看一个对比：")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("硬提示（Hard Prompt / 手工提示）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("这是我们平时与ChatGPT等模型交互的方式。我们通过自然语言精心设计指令，例如：\n"),s("blockquote",[s("p",[s("strong",[t._v("指令")]),t._v("：“将以下文本分类为正面或负面情感。文本：'这部电影太精彩了！'”")])])]),t._v(" "),s("li",[s("strong",[t._v("缺点")]),t._v("：这需要大量的人工尝试和调整（Prompt Engineering），效果不稳定，且找到最优提示如同大海捞针。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("软提示（Soft Prompt）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("Prompt-tuning 不再使用人类可读的词语作为提示。而是"),s("strong",[t._v("引入一小段可训练的、由模型嵌入空间中的向量（一组数字）组成的“提示”")]),t._v("。")]),t._v(" "),s("li",[t._v("这些向量没有直接的语义对应，它们是通过梯度下降算法在数据上"),s("strong",[t._v("自动学习出来的“最优指令”")]),t._v("。可以把它理解为一种模型能“心领神会”的密语。")])])])]),t._v(" "),s("p",[t._v("下面的流程图清晰地展示了Prompt-tuning在应用时与Fine-tuning的区别：")]),t._v(" "),s("div",{staticClass:"language-mermaid extra-class"},[s("pre",{pre:!0,attrs:{class:"language-mermaid"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("flowchart")]),t._v(" TD\n    A"),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[输入文本]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" B\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("subgraph")]),t._v(" B "),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[方法选择]")]),t._v("\n        B1"),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[Fine-tuning<br>更新全部模型参数]")]),t._v("\n        B2"),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[Prompt-tuning<br>仅优化提示向量]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("end")]),t._v("\n\n    B1 "),s("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" C1"),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[模型参数被改变<br>“动了手术”]")]),t._v("\n    B2 "),s("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" C2"),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[模型参数保持冻结<br>“被催眠引导”]")]),t._v("\n\n    C1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" C2 "),s("span",{pre:!0,attrs:{class:"token arrow operator"}},[t._v("--\x3e")]),t._v(" D"),s("span",{pre:!0,attrs:{class:"token text string"}},[t._v("[任务输出]")]),t._v("\n")])])]),s("hr"),t._v(" "),s("h3",{attrs:{id:"二、prompt-tuning-的主要优势"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、prompt-tuning-的主要优势"}},[t._v("#")]),t._v(" 二、Prompt-tuning 的主要优势")]),t._v(" "),s("p",[t._v("为什么我们要使用Prompt-tuning？相比于传统的Fine-tuning，它带来了革命性的好处：")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("参数效率极高（Parameter-Efficient）")])]),t._v(" "),s("ul",[s("li",[t._v("传统微调需要更新整个模型（可能包含数十亿甚至数千亿参数），计算和存储成本巨大。")]),t._v(" "),s("li",[t._v("Prompt-tuning 只优化极少量的参数（通常只占模型总参数的0.01%~1%），大大降低了硬件门槛。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("避免灾难性遗忘（Catastrophic Forgetting）")])]),t._v(" "),s("ul",[s("li",[t._v("传统微调可能会为了让模型适应新任务而覆盖掉预训练时学到的通用知识。")]),t._v(" "),s("li",[t._v("Prompt-tuning 保持原模型参数不变，完美保留了模型在预训练阶段获得的所有知识和能力。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("易于部署和切换")])]),t._v(" "),s("ul",[s("li",[t._v("对于一个基础模型（如LLaMA 3），你可以为不同任务（情感分析、命名实体识别、文本摘要）训练出多个不同的“软提示”。")]),t._v(" "),s("li",[t._v("部署时，你只需要保存和加载这些小小的提示文件（几KB到几MB），即可让同一个模型实例瞬间切换为不同任务的专家，而无需维护多个完整的模型副本（每个都可能是几个GB）。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("效果逼近全量微调")])]),t._v(" "),s("ul",[s("li",[t._v("尤其是在模型规模足够大（例如超过100亿参数）时，Prompt-tuning 的性能可以逼近甚至在某些任务上超越全量微调。")])])])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"三、prompt-tuning-的家族成员"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、prompt-tuning-的家族成员"}},[t._v("#")]),t._v(" 三、Prompt-tuning 的家族成员")]),t._v(" "),s("p",[t._v("Prompt-tuning 是一个大家族，主要包括以下几种演进的技术：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("技术名称")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("核心思想")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("示意图简释")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("Prompt Tuning")]),t._v(" (狭义)")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("只在输入层添加可训练的提示向量。")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("[可训练向量] + [输入文本] -> [冻结的模型]")])])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("Prefix-Tuning")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("不仅在输入层，还在模型的每一层（或中间层）前面添加可训练的前缀向量，控制能力更强。")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("[可训练向量] + [每层的激活值] -> [冻结的下一层]")])])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("P-Tuning")]),t._v(" / "),s("strong",[t._v("P-Tuning v2")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("一种更先进的优化方式，使用一个轻量级的“提示编码器”（如LSTM或MLP）来生成提示向量，而非直接优化，效果更稳定。P-Tuning v2 将其扩展到了所有层，类似Prefix-Tuning。")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("[输入文本] -> [轻量级提示编码器] -> [可训练向量] -> [冻结的模型]")])])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("LoRA (Low-Rank Adaptation)")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("虽然不完全是Prompt-tuning，但同属"),s("strong",[t._v("参数高效性微调（PEFT）")]),t._v(" 家族。它通过向模型内部注入低秩矩阵来间接模拟参数更新，而非添加提示。")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("code",[t._v("[冻结的模型权重] + [低秩矩阵A&B] -> [适应后的效果]")])])])])]),t._v(" "),s("p",[t._v("现在，这些方法通常被统称为 "),s("strong",[t._v("PEFT")]),t._v("。")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"四、一个简单的类比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四、一个简单的类比"}},[t._v("#")]),t._v(" 四、一个简单的类比")]),t._v(" "),s("p",[t._v("为了让您印象更深刻，我们来做一个有趣的类比：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("预训练大模型")]),t._v("：一位"),s("strong",[t._v("知识渊博的全科医生")]),t._v("，掌握了全面的医学理论。")]),t._v(" "),s("li",[s("strong",[t._v("新任务")]),t._v("：快速成为一名"),s("strong",[t._v("顶级的皮肤科专家")]),t._v("。")]),t._v(" "),s("li",[s("strong",[t._v("全量微调（Fine-tuning）")]),t._v("：让这位全科医生"),s("strong",[t._v("重新回医学院，花几年时间重修所有科目，但重点强化皮肤科")]),t._v("。代价巨大，而且可能忘了其他科的知识。")]),t._v(" "),s("li",[s("strong",[t._v("提示工程（Hard Prompt）")]),t._v("：你每次都对他说：“"),s("strong",[t._v("请你现在扮演一个皮肤科专家，看看这个病例……")]),t._v("” 效果取决于你的指令水平，不稳定。")]),t._v(" "),s("li",[s("strong",[t._v("提示调优（Prompt-tuning）")]),t._v("：你通过一种“催眠术”，为他植入一段"),s("strong",[t._v("特定的“思维导图”（软提示）")]),t._v("。一旦植入，他看任何病患时，都会自动切换到皮肤科专家的思维模式，无需你再每次口头指令。这段“思维导图”就是学习出来的提示向量。")])]),t._v(" "),s("h3",{attrs:{id:"总结-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#总结-2"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),s("p",[s("strong",[t._v("Prompt-tuning/PEFT 技术是大模型时代应用落地的关键技术之一。")]),t._v(" 它极大地降低了定制和使用大模型的成本，让更多的研究者和开发者能够在不具备强大算力的情况下，高效地让通用大模型为自身的专属任务服务。")]),t._v(" "),s("p",[t._v("在实际工作中，当我们拿到一个如 Qwen、ChatGLM 这样的优秀基座模型后，第一步往往就是尝试使用 Prompt-tuning 或 LoRA 等技术对其进行高效适配，而不是直接进行全量微调。")]),t._v(" "),s("p",[t._v("好的，我们来深入探讨一下 "),s("strong",[t._v("Prefix Tuning")]),t._v("。作为大模型应用开发者，理解这项技术至关重要，因为它是一种非常经典且高效的参数高效性微调（PEFT）方法。")]),t._v(" "),s("p",[t._v("如果说传统的微调（Fine-tuning）是给大模型“换脑手术”，那么 "),s("strong",[t._v("Prefix-Tuning 可以看作是一种“深度催眠”或“前置引导”")]),t._v("。它比我们之前讨论的普通Prompt-Tuning更进了一步，引导得更深、更有效。")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"一、核心思想-在每一层之前添加可学习的-前缀"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、核心思想-在每一层之前添加可学习的-前缀"}},[t._v("#")]),t._v(" 一、核心思想：在每一层之前添加可学习的“前缀”")]),t._v(" "),s("p",[t._v("为了直观理解它的工作原理，我们将其与普通微调和基础Prompt-Tuning进行对比：")]),t._v(" "),s("p",[s("img",{attrs:{src:"/hmblog//images/fine-tuning/prefix-tuning.png",alt:"Prefix Tuning"}})]),t._v(" "),s("p",[t._v("从上图可以看出Prefix-Tuning的独特之处：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("普通Prompt-Tuning")]),t._v("：只在模型的"),s("strong",[t._v("输入嵌入层（Input Embedding Layer）")]),t._v(" 添加可训练的提示向量。这相当于在对话开始时给模型一个总体的指令。")]),t._v(" "),s("li",[s("strong",[t._v("Prefix-Tuning")]),t._v("：不仅在输入层，而是在"),s("strong",[t._v("模型的每一层（或某几层）的激活（activation）之前")]),t._v("，都添加一组可训练的前缀向量。这相当于在模型思考的每一个步骤、每一个阶段都不断地进行引导和提醒，确保它不偏离轨道。")])]),t._v(" "),s("p",[s("strong",[t._v("关键比喻：")]),t._v("\n想象你在指导一位实习生写报告。")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("Fine-tuning")]),t._v("：你把他送去重新培训，改变他的整个知识结构。")]),t._v(" "),s("li",[s("strong",[t._v("Prompt-Tuning")]),t._v("：你在任务开始时给他一份详细的书面指令。")]),t._v(" "),s("li",[s("strong",[t._v("Prefix-Tuning")]),t._v("：你不仅给了书面指令，还在他写报告的每一个章节（引言、正文、结论）开始时，都在他旁边给予针对性的提示和引导。这种“持续陪伴”式的指导显然更深入、更有效。")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"二、技术细节-它是如何工作的"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、技术细节-它是如何工作的"}},[t._v("#")]),t._v(" 二、技术细节：它是如何工作的？")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("冻结主模型")]),t._v("：预训练大模型的所有参数被"),s("strong",[t._v("冻结（Freeze）")]),t._v("，不允许更新。这最大程度保留了模型原有的知识，避免了灾难性遗忘。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("引入前缀矩阵（Prefix Matrix）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("我们为模型需要添加前缀的每一层，都初始化一个小的、可训练的矩阵，称为前缀矩阵。")]),t._v(" "),s("li",[t._v("这个矩阵的每一行都是一个向量，代表一个“前缀token”。这些向量不是自然语言词汇的嵌入，而是"),s("strong",[t._v("在模型的隐藏空间（Hidden Space）中通过训练学习得到的“软提示”（Soft Prompts）")]),t._v("。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("拼接（Concatenation）与前向传播（Forward Pass）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("在模型每一层的前向计算过程中，我们将学习到的前缀向量（"),s("code",[t._v("[PREFIX]")]),t._v("）与该层原始的输入序列向量（"),s("code",[t._v("[INPUT]")]),t._v("）进行拼接，形成 "),s("code",[t._v("[PREFIX; INPUT]")]),t._v("。")]),t._v(" "),s("li",[t._v("然后，将这个拼接后的序列送给该层的Transformer模块进行处理。")]),t._v(" "),s("li",[t._v("由于前缀向量是可训练的，在训练过程中，通过梯度下降来优化这些向量，使得它们能够最有效地“刺激”或“引导”冻结的模型去完成特定任务。")])])])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"三、prefix-tuning-的主要优势"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、prefix-tuning-的主要优势"}},[t._v("#")]),t._v(" 三、Prefix-Tuning 的主要优势")]),t._v(" "),s("p",[t._v("相比于普通的Prompt-Tuning和全量微调，Prefix-Tuning的优势非常突出：")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("更强的表现力与控制力")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("由于引导信号（前缀）被注入到了模型的更深层，它能更直接地影响模型每一层的注意力机制（Attention）和前馈网络（FFN）的计算，从而对模型的行为实现更精细、更强大的控制。效果通常比只在输入层添加提示的Prompt-Tuning更好。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("极高的参数效率")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("和所有PEFT方法一样，它只训练极小部分的参数（仅占模型总参数的0.1%~3%），极大地降低了计算和存储成本。你可以为一个基础模型训练无数个不同的“前缀”，来应对不同的任务，部署时只需切换这个小文件即可。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("保留通用知识")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("主模型参数被冻结，其强大的语言理解和生成能力得以完整保留，完美避免了灾难性遗忘。")])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("训练更稳定")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("相较于早期直接优化嵌入向量的Prompt-Tuning，Prefix-Tuning的效果通常更稳定。后来的一些改进（如将前缀向量通过一个小的神经网络来生成，而非直接优化）进一步提升了其稳定性。")])])])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"四、与相关技术的对比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四、与相关技术的对比"}},[t._v("#")]),t._v(" 四、与相关技术的对比")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("技术")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("训练参数")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("作用位置")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("特点")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("Full Fine-tuning（全量微调）")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("全部（100%）")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("模型所有参数")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("效果可能最好，但成本极高，易遗忘，产生模型副本。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("Prompt-Tuning")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("极少（~0.01%）")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("仅输入嵌入层")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("最简单，参数量最少，但在模型较小时效果不佳。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("Prefix-Tuning")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("少（~0.1%-3%）")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("每一层（或中间层）的激活之前")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("控制力强，效果通常优于Prompt-Tuning，是更优的选择。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("LoRA")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("少（~0.1%-3%）")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[s("strong",[t._v("注入到注意力层的旁路矩阵")])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("通过低秩适应间接更新权重，效果与Prefix-Tuning相当，常是首选。")])])])]),t._v(" "),s("h3",{attrs:{id:"总结-3"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#总结-3"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),s("p",[s("strong",[t._v("Prefix-Tuning 的核心贡献在于，它证明了不需要改动庞大的模型参数，只需要在模型的合适深度（每一层）插入极少量可学习的“刺激信号”，就能高效地引导大模型完成特定任务。")])]),t._v(" "),s("p",[t._v("它是通向更高效、更模块化的大模型定制之路上的一个关键里程碑。在实际应用中，如今我们更常使用其思想的后继者（如 "),s("strong",[t._v("P-Tuning v2")]),t._v("）或另一种流行技术 "),s("strong",[t._v("LoRA")]),t._v("，但它们的基本理念与Prefix-Tuning一脉相承："),s("strong",[t._v("用最小的代价，实现最有效的引导。")])]),t._v(" "),s("p",[t._v("希望这个解释能帮助您透彻地理解Prefix-Tuning！")]),t._v(" "),s("p",[t._v("直接问 deepseek\nhttps://github.com/huggingface/peft/blob/main/examples/sft/run_peft.sh")]),t._v(" "),s("p",[t._v('这个文件怎么运行，里面的  --model_name_or_path "mistralai/Mistral-7B-v0.1" '),s("br"),t._v('\n--dataset_name "smangrul/ultrachat-10k-chatml" \\')]),t._v(" "),s("p",[t._v("模型  和  dataset   是不是都要下载到本地")]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("回答：")]),t._v(" "),s("p",[t._v("看了这个脚本，我来详细解释一下运行方式：")]),t._v(" "),s("h2",{attrs:{id:"运行方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#运行方式"}},[t._v("#")]),t._v(" 运行方式")]),t._v(" "),s("h3",{attrs:{id:"_1-直接运行-推荐网络条件好的情况"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-直接运行-推荐网络条件好的情况"}},[t._v("#")]),t._v(" 1. 直接运行（推荐网络条件好的情况）")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 克隆仓库")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("git")]),t._v(" clone https://github.com/huggingface/peft.git\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" peft/examples/sft\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 直接运行脚本")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("bash")]),t._v(" run_peft.sh\n")])])]),s("h3",{attrs:{id:"_2-模型和数据集下载问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-模型和数据集下载问题"}},[t._v("#")]),t._v(" 2. 模型和数据集下载问题")]),t._v(" "),s("p",[s("strong",[t._v("模型（mistralai/Mistral-7B-v0.1）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("默认")]),t._v("：会自动从Hugging Face Hub下载（需要约15GB空间）")]),t._v(" "),s("li",[s("strong",[t._v("本地已有")]),t._v("：如果本地有该模型，可以修改路径指向本地目录")])]),t._v(" "),s("p",[s("strong",[t._v("数据集（smangrul/ultrachat-10k-chatml）")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("默认")]),t._v("：会自动下载（数据量较小）")]),t._v(" "),s("li",[s("strong",[t._v("本地已有")]),t._v("：同样可以修改路径")])]),t._v(" "),s("h2",{attrs:{id:"本地下载方案"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#本地下载方案"}},[t._v("#")]),t._v(" 本地下载方案")]),t._v(" "),s("p",[t._v("如果你希望预先下载到本地：")]),t._v(" "),s("h3",{attrs:{id:"下载模型到本地"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#下载模型到本地"}},[t._v("#")]),t._v(" 下载模型到本地")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用huggingface-cli")]),t._v("\npip "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" huggingface_hub\nhuggingface-cli download mistralai/Mistral-7B-v0.1 --local-dir ./mistral-7b-local\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 或者使用Python")]),t._v("\nfrom transformers "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" AutoModel, AutoTokenizer\nmodel "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoModel.from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mistralai/Mistral-7B-v0.1"')]),t._v(", "),s("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("cache_dir")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./models"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer.from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mistralai/Mistral-7B-v0.1"')]),t._v(", "),s("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("cache_dir")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./models"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"修改脚本使用本地路径"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#修改脚本使用本地路径"}},[t._v("#")]),t._v(" 修改脚本使用本地路径")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 修改run_peft.sh中的相关行")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--model_name_or_path")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./mistral-7b-local"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--dataset_name")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./local-dataset"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n")])])]),s("h2",{attrs:{id:"注意事项"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#注意事项"}},[t._v("#")]),t._v(" 注意事项")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("存储空间")]),t._v("：Mistral-7B模型需要约15GB空间")]),t._v(" "),s("li",[s("strong",[t._v("网络要求")]),t._v("：首次运行需要稳定网络连接下载资源")]),t._v(" "),s("li",[s("strong",[t._v("内存要求")]),t._v("：需要足够RAM来加载模型（建议16GB+）")]),t._v(" "),s("li",[s("strong",[t._v("GPU要求")]),t._v("：脚本默认使用GPU，如需CPU运行需要修改相关参数")])]),t._v(" "),s("h2",{attrs:{id:"替代方案-如果下载困难"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#替代方案-如果下载困难"}},[t._v("#")]),t._v(" 替代方案（如果下载困难）")]),t._v(" "),s("p",[t._v("如果下载大模型有困难，可以考虑：")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("使用小模型")]),t._v("：替换为较小的模型如：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--model_name_or_path")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"facebook/opt-1.3b"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n")])])])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("使用本地小数据集")]),t._v("：准备自己的数据集格式")])])]),t._v(" "),s("p",[t._v("需要我帮你修改脚本适配本地资源或提供更详细的配置说明吗？")]),t._v(" "),s("h3",{attrs:{id:"langchain-练习"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#langchain-练习"}},[t._v("#")]),t._v(" langchain 练习")]),t._v(" "),s("p",[t._v("我来帮你将 Agently 的提示词改造为 Langchain 的 PromptTemplate 写法：")]),t._v(" "),s("h2",{attrs:{id:"_1-完整的-langchain-prompt-实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-完整的-langchain-prompt-实现"}},[t._v("#")]),t._v(" 1. 完整的 Langchain Prompt 实现")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prompts "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" SystemMessagePromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" HumanMessagePromptTemplate\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schema "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SystemMessage"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" HumanMessage\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建系统提示词")]),t._v("\nsystem_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请遵循以下要求：\n1. 分析用户提供的文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n请以以下JSON格式输出结果：\n{{\n    "result": [\n        {{\n            "speaker": "发言人的中文姓名(不含角色)",\n            "content": "发言的详细内容", \n            "is_fraud": "true或false，表示该发言中是否存在明确具体、不可辩驳的欺诈内容"\n        }}\n    ]\n}}\n"""')]),t._v("\n\nsystem_message_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SystemMessagePromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("system_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建用户输入模板")]),t._v("\nhuman_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""请分析以下诈骗案例文本：\n\n{text}"""')]),t._v("\n\nhuman_message_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" HumanMessagePromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("human_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 组合成完整的 ChatPromptTemplate")]),t._v("\nchat_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    system_message_prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    human_message_prompt\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"_2-使用方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-使用方式"}},[t._v("#")]),t._v(" 2. 使用方式")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_community"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("chat_models "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ChatOpenAI\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化 DeepSeek 模型")]),t._v("\nllm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatOpenAI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    model"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"deepseek-chat"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    openai_api_key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"your_deepseek_api_key"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    openai_api_base"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://api.deepseek.com/v1"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    temperature"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_case")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    分析诈骗案例并生成对话\n    """')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建提示词")]),t._v("\n    messages "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chat_prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("format_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用模型")]),t._v("\n    response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 解析响应")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hasattr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'content'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 清理可能的markdown格式")]),t._v("\n            content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content\n            content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'```json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'```'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JSONDecodeError "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"JSON解析错误: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"原始响应: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用示例")]),t._v("\ntext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("column_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" analyze_fraud_case"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indent"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"_3-更简洁的版本-使用字符串模板"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-更简洁的版本-使用字符串模板"}},[t._v("#")]),t._v(" 3. 更简洁的版本（使用字符串模板）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prompts "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" PromptTemplate\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建完整的提示词模板")]),t._v("\nprompt_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    input_variables"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    template"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家，你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n\n请分析以下诈骗案例文本：\n{text}\n\n要求：\n1. 分析文本中描述的案发过程，识别出诈骗者和受害者角色，并为每个角色生成一个中文姓名。\n2. 发挥你的想象力，按照前后经过，将案发过程以诈骗者和受害者之间的对话来循序渐进地展开，目的是还原整个诈骗案件的详细对话过程。\n3. 诈骗者的发言内容需要尽可能的伪装，让自己的身份和操作都听起来正规，以避免受害者怀疑。\n4. 从第一句发言开始，依次分析每条发言内容是否存在明确的欺诈内容。\n\n请以以下JSON格式输出结果：\n{{\n    "result": [\n        {{\n            "speaker": "发言人的中文姓名(不含角色)",\n            "content": "发言的详细内容", \n            "is_fraud": "true或false，表示该发言中是否存在明确具体、不可辩驳的欺诈内容"\n        }}\n    ]\n}}\n\n请直接返回JSON格式的结果，不要添加其他解释。"""')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用方式")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_case_simple")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    formatted_prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt_template"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("formatted_prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hasattr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'content'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content\n            content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'```json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'```'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JSONDecodeError"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"解析失败，返回原始内容"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"raw_response"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n")])])]),s("h2",{attrs:{id:"_4-使用-lcel-链式调用-推荐"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-使用-lcel-链式调用-推荐"}},[t._v("#")]),t._v(" 4. 使用 LCEL 链式调用（推荐）")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output_parsers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" JsonOutputParser\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prompts "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ChatPromptTemplate\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建输出解析器")]),t._v("\noutput_parser "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" JsonOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建提示词模板")]),t._v("\nprompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ChatPromptTemplate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"system"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家。你的任务是将诈骗的案发过程以口语对话的形式完整的还原。\n    \n要求：\n1. 分析案发过程，识别诈骗者和受害者角色，为每个角色生成中文姓名\n2. 以对话形式循序渐进展开案发过程\n3. 诈骗者发言要伪装得正规可信\n4. 分析每条发言是否存在明确欺诈内容"""')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"请分析以下诈骗案例：{text}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"请以指定JSON格式输出：{format_instructions}"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建链")]),t._v("\nchain "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" llm "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" output_parser\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用链")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_with_chain")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"format_instructions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" output_parser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_format_instructions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" Exception "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"分析失败: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用示例")]),t._v("\ntext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("column_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" analyze_fraud_with_chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"主要变化说明"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#主要变化说明"}},[t._v("#")]),t._v(" 主要变化说明：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("角色定义")]),t._v("：从 "),s("code",[t._v(".role()")]),t._v(" 改为 SystemMessage")]),t._v(" "),s("li",[s("strong",[t._v("输入处理")]),t._v("：从 "),s("code",[t._v(".input()")]),t._v(" 改为 HumanMessage 中的变量替换")]),t._v(" "),s("li",[s("strong",[t._v("指令")]),t._v("：从 "),s("code",[t._v(".instruct()")]),t._v(" 改为提示词中的具体要求列表")]),t._v(" "),s("li",[s("strong",[t._v("输出格式")]),t._v("：从 "),s("code",[t._v(".output()")]),t._v(" 改为在提示词中描述JSON格式")]),t._v(" "),s("li",[s("strong",[t._v("执行")]),t._v("：从 "),s("code",[t._v(".start()")]),t._v(" 改为 "),s("code",[t._v(".invoke()")])])]),t._v(" "),s("p",[t._v("这样的改造保持了原有的功能，同时符合 Langchain 的编程范式。")]),t._v(" "),s("p",[t._v("你遇到的问题是 Python 字典的默认显示方式，不是模型生成的问题。让我解释一下：")]),t._v(" "),s("h2",{attrs:{id:"问题分析"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#问题分析"}},[t._v("#")]),t._v(" 问题分析")]),t._v(" "),s("p",[t._v("你看到的单引号和首字母大写的 True/False 是 "),s("strong",[t._v("Python 字典的默认表示形式")]),t._v("，不是模型生成的 JSON 格式问题。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这是 Python 字典的显示方式（单引号，True/False 首字母大写）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'result'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'speaker'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'张伟'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'content'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'...'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is_fraud'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这是标准的 JSON 格式（双引号，true/false 小写）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"result"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"speaker"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"张伟"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"content"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"..."')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"is_fraud"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" true"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h2",{attrs:{id:"验证方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#验证方法"}},[t._v("#")]),t._v(" 验证方法")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查实际的 JSON 内容")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 假设 result 是你得到的字典")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'result'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'speaker'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'张伟'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'content'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'...'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is_fraud'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 转换为 JSON 字符串查看实际格式")]),t._v("\njson_str "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"JSON 字符串格式:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json_str"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# 输出: {"result": [{"speaker": "张伟", "content": "...", "is_fraud": true}]}')]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 检查是否可以正常解析")]),t._v("\nparsed "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json_str"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"解析后的类型:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("parsed"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'result'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is_fraud'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"解决方案"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#解决方案"}},[t._v("#")]),t._v(" 解决方案")]),t._v(" "),s("h3",{attrs:{id:"方法1-直接使用-json-处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#方法1-直接使用-json-处理"}},[t._v("#")]),t._v(" 方法1：直接使用 JSON 处理")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_case")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ... 调用模型的代码 ...")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hasattr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'content'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 清理响应内容")]),t._v("\n        content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'```json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'```'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 直接解析为 Python 对象")]),t._v("\n            result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loads"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JSONDecodeError "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"JSON解析错误: ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("e"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用时，结果已经是正确的 Python 字典")]),t._v("\nresult "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" analyze_fraud_case"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"方法2-增强提示词约束"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#方法2-增强提示词约束"}},[t._v("#")]),t._v(" 方法2：增强提示词约束")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("system_template "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""你是一个分析诈骗案例并且擅长写作的专家。任务：将诈骗案发过程以口语对话形式还原。\n\n要求：\n1. 识别诈骗者和受害者角色，生成中文姓名\n2. 以对话形式展开案发过程  \n3. 诈骗者发言要伪装正规\n4. 分析每条发言是否存在欺诈内容\n\n严格的JSON格式要求：\n- 必须使用双引号包围所有字符串键和值\n- 布尔值必须使用小写 true 或 false\n- 不要使用单引号\n- 确保是有效的JSON格式\n\n输出示例：\n{\n    "result": [\n        {\n            "speaker": "张三",\n            "content": "对话内容",\n            "is_fraud": true\n        }\n    ]\n}\n\n重要：直接输出纯净JSON，不要任何其他文字！"""')]),t._v("\n")])])]),s("h3",{attrs:{id:"方法3-使用-langchain-的-json-解析器"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#方法3-使用-langchain-的-json-解析器"}},[t._v("#")]),t._v(" 方法3：使用 Langchain 的 JSON 解析器")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" langchain_core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("output_parsers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" JsonOutputParser\n\noutput_parser "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" JsonOutputParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("analyze_fraud_case")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建提示词")]),t._v("\n    messages "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"system"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"你是诈骗案例分析专家..."')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"分析案例：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"human"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"格式要求：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("output_parser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_format_instructions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用模型并解析")]),t._v("\n    response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" llm"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("invoke"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("messages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" output_parser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n")])])]),s("h2",{attrs:{id:"验证代码正确性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#验证代码正确性"}},[t._v("#")]),t._v(" 验证代码正确性")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 测试你的结果")]),t._v("\ntest_result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'result'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'speaker'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'张伟'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'content'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'...'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is_fraud'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 检查是否可以序列化为标准 JSON")]),t._v("\njson_str "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"标准JSON格式:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" json_str"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 检查是否可以保存到文件并读取")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test_output.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf-8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dump"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" indent"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test_output.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf-8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    file_content "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"从文件读取的内容:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" file_content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("总结")]),t._v("：你看到的单引号和首字母大写的 True/False 只是 Python 字典的显示方式，实际在 JSON 序列化时会自动转换为标准的双引号和小写 true/false。你的代码很可能是正确的！")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"_10-03"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_10-03"}},[t._v("#")]),t._v(" 10.03")]),t._v(" "),s("p",[t._v("很好！你的 "),s("code",[t._v("label2id")]),t._v(" 映射是正确的。现在基于你已有的标签映射，我来提供一个完整的 "),s("code",[t._v("collote_fn")]),t._v(" 实现：")]),t._v(" "),s("h2",{attrs:{id:"完整的-collote-fn-函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#完整的-collote-fn-函数"}},[t._v("#")]),t._v(" 完整的 collote_fn 函数")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("collote_fn")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_samples"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    batch_sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_char_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从batch samples中提取句子并生成字符级标签")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" sample "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" batch_samples"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        sentence "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        batch_sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 生成字符级BIO标签序列")]),t._v("\n        char_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'O'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化为'O'")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据实体标注信息填充标签")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pos "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pos "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'B-")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("label_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'I-")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("label_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),t._v("\n        \n        batch_char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对文本进行分词")]),t._v("\n    batch_inputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        batch_sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        truncation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n        max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pt"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        is_split_into_words"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 重要：告诉tokenizer我们处理的是字符级任务")]),t._v("\n        return_offsets_mapping"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取字符到token的映射关系")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对齐标签：将字符级标签映射到token级标签")]),t._v("\n    aligned_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        word_ids "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" batch_inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_index"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取每个token对应的原始字符位置")]),t._v("\n        char_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" batch_char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 该句子的字符级标签")]),t._v("\n        token_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于存储token级标签")]),t._v("\n        \n        previous_word_idx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 处理特殊token: [CLS], [SEP], [PAD]等，标签设为-100（PyTorch忽略的索引）")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 直接使用对应字符位置的标签")]),t._v("\n                label "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("label2id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n            previous_word_idx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" word_idx\n        \n        aligned_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将标签转换为tensor")]),t._v("\n    batch_inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aligned_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 移除offsets_mapping，因为训练时不需要")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'offset_mapping'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" batch_inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        batch_inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'offset_mapping'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" batch_inputs\n")])])]),s("h2",{attrs:{id:"如果你想要在dataset中预先生成char-labels"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#如果你想要在dataset中预先生成char-labels"}},[t._v("#")]),t._v(" 如果你想要在Dataset中预先生成char_labels")]),t._v(" "),s("p",[t._v("这样可以提高训练时的效率：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("NERDataset")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("load_data")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        Data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf-8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n\\n'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" line "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" line"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("continue")]),t._v("\n                    \n                sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                items "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("item "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" item "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" line"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                \n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 首先构建句子和实体标注")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    char"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tag "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    sentence "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" char\n                    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" tag"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("startswith"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" char"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tag"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" tag"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("startswith"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'I'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" i\n                        labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" char\n                \n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 生成字符级标签序列")]),t._v("\n                char_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'O'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pos "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pos "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                            char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'B-")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("label_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),t._v("\n                        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                            char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("pos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'I-")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("label_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),t._v("\n                \n                Data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'char_labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" char_labels  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 预先生成的字符级标签")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v("f'数据集中包含的实体类型有：")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("categories"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" Data\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__len__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__getitem__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 简化版的collote_fn")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("collote_fn")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_samples"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    batch_sentence "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sentence'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" sample "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" batch_samples"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    batch_char_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'char_labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" sample "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" batch_samples"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    \n    batch_inputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        batch_sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        truncation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pt"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        is_split_into_words"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    aligned_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" char_labels "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        word_ids "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" batch_inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_index"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        token_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word_idx "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("label2id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word_idx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        aligned_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    batch_inputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'labels'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aligned_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" batch_inputs\n")])])]),s("h2",{attrs:{id:"关键点说明"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#关键点说明"}},[t._v("#")]),t._v(" 关键点说明")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("标签对齐")]),t._v("：使用 "),s("code",[t._v("word_ids()")]),t._v(" 方法获取每个token对应的原始字符位置")]),t._v(" "),s("li",[s("strong",[t._v("特殊token处理")]),t._v("："),s("code",[t._v("[CLS]")]),t._v(", "),s("code",[t._v("[SEP]")]),t._v(", "),s("code",[t._v("[PAD]")]),t._v(" 的标签设为 "),s("code",[t._v("-100")]),t._v("，训练时会被忽略")]),t._v(" "),s("li",[s("strong",[t._v("你的label2id映射")]),t._v("：完全正确，包含了所有10个实体类型的BIO标签")]),t._v(" "),s("li",[s("strong",[t._v("字符级到token级映射")]),t._v("：由于中文BERT通常是字符级分词，所以大多数情况下是1:1映射")])]),t._v(" "),s("p",[t._v("这样实现后，你的模型就能正确学习到中医文本中的实体识别任务了！")]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("您观察得很仔细！这是一个非常典型的 "),s("strong",[t._v("Python 解包（Unpacking）")]),t._v(" 用法，完全合法且很常用。让我详细解释：")]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"📦-解包机制解析"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#📦-解包机制解析"}},[t._v("#")]),t._v(" "),s("strong",[t._v("📦 解包机制解析")])]),t._v(" "),s("h3",{attrs:{id:"数据源结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据源结构"}},[t._v("#")]),t._v(" "),s("strong",[t._v("数据源结构")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'口苦'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'临床表现'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每个子列表有4个元素：")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 索引0: 3 → start (起始位置)")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 索引1: 4 → end (结束位置)  ")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 索引2: '口苦' → 实体文本")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 索引3: '临床表现' → 标签类型")]),t._v("\n")])])]),s("h3",{attrs:{id:"循环中的解包过程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#循环中的解包过程"}},[t._v("#")]),t._v(" "),s("strong",[t._v("循环中的解包过程")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 等价于：")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# temp = [3, 4, '口苦', '临床表现']  # 取出第一个子列表")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# start = temp[0]   # 3")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# end = temp[1]     # 4  ")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# _ = temp[2]       # '口苦' (用_表示忽略这个变量)")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# label_type = temp[3]  # '临床表现'")]),t._v("\n")])])]),s("hr"),t._v(" "),s("h2",{attrs:{id:"🎯-参数对应关系"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🎯-参数对应关系"}},[t._v("#")]),t._v(" "),s("strong",[t._v("🎯 参数对应关系")])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("循环变量")]),t._v(" "),s("th",[t._v("对应数据")]),t._v(" "),s("th",[t._v("作用")]),t._v(" "),s("th",[t._v("是否必需")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[s("code",[t._v("start")])]),t._v(" "),s("td",[s("code",[t._v("3")])]),t._v(" "),s("td",[t._v("实体起始位置")]),t._v(" "),s("td",[t._v("✅ 必需")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("end")])]),t._v(" "),s("td",[s("code",[t._v("4")])]),t._v(" "),s("td",[t._v("实体结束位置")]),t._v(" "),s("td",[t._v("✅ 必需")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("_")])]),t._v(" "),s("td",[s("code",[t._v("'口苦'")])]),t._v(" "),s("td",[s("strong",[t._v("实体文本")])]),t._v(" "),s("td",[t._v("❌ 用"),s("code",[t._v("_")]),t._v("忽略")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("label_type")])]),t._v(" "),s("td",[s("code",[t._v("'临床表现'")])]),t._v(" "),s("td",[t._v("实体类型")]),t._v(" "),s("td",[t._v("✅ 必需")])])])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"🔍-代码执行过程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🔍-代码执行过程"}},[t._v("#")]),t._v(" "),s("strong",[t._v("🔍 代码执行过程")])]),t._v(" "),s("h3",{attrs:{id:"步骤分解"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#步骤分解"}},[t._v("#")]),t._v(" "),s("strong",[t._v("步骤分解")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("sentence "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'现头昏口苦'")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 长度5，索引: 0:现, 1:头, 2:昏, 3:口, 4:苦")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化: ['O', 'O', 'O', 'O', 'O']")]),t._v("\nchar_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'O'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 循环开始:")]),t._v("\nstart"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'口苦'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'临床表现'")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pos "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# range(3, 4+1) → [3, 4]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pos "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 第一个位置 → '口'")]),t._v("\n        char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B-临床表现'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("         "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pos == 4 → '苦'  ")]),t._v("\n        char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'I-临床表现'")]),t._v("\n")])])]),s("h3",{attrs:{id:"最终结果"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#最终结果"}},[t._v("#")]),t._v(" "),s("strong",[t._v("最终结果")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("char_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ['O', 'O', 'O', 'B-临床表现', 'I-临床表现']")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  现    头    昏    口        苦")]),t._v("\n")])])]),s("hr"),t._v(" "),s("h2",{attrs:{id:"💡-为什么用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#💡-为什么用"}},[t._v("#")]),t._v(" "),s("strong",[t._v("💡 为什么用 "),s("code",[t._v("_")]),t._v(" ？")])]),t._v(" "),s("p",[s("code",[t._v("_")]),t._v(" 是 Python 约定俗成的 "),s("strong",[t._v('"忽略变量"')]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("这个位置的数据 ("),s("code",[t._v("'口苦'")]),t._v(") 在后续代码中"),s("strong",[t._v("不会使用")])]),t._v(" "),s("li",[t._v("用 "),s("code",[t._v("_")]),t._v(' 明确告诉阅读者："我知道这里有数据，但故意不用"')]),t._v(" "),s("li",[t._v("避免创建无用的变量名污染命名空间")])]),t._v(" "),s("h3",{attrs:{id:"如果不用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#如果不用"}},[t._v("#")]),t._v(" "),s("strong",[t._v("如果不用 "),s("code",[t._v("_")]),t._v("：")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 方式1：全部接收（浪费一个变量名）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" entity_text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# entity_text 根本用不到，但还占着变量名")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 方式2：用_明确忽略（推荐）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 清晰表达意图")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 方式3：索引访问（不优雅）")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" item "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    start"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    label_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("hr"),t._v(" "),s("h2",{attrs:{id:"🚀-这种用法的优势"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🚀-这种用法的优势"}},[t._v("#")]),t._v(" "),s("strong",[t._v("🚀 这种用法的优势")])]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("代码简洁")]),t._v("：一行完成多个变量赋值")]),t._v(" "),s("li",[s("strong",[t._v("意图清晰")]),t._v("：直接看出数据结构")]),t._v(" "),s("li",[s("strong",[t._v("类型安全")]),t._v("：如果数据格式不对会立即报错")]),t._v(" "),s("li",[s("strong",[t._v("Pythonic")]),t._v("：符合 Python 的优雅风格")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"📚-类似的应用场景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#📚-类似的应用场景"}},[t._v("#")]),t._v(" "),s("strong",[t._v("📚 类似的应用场景")])]),t._v(" "),s("h3",{attrs:{id:"遍历字典的键值对"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#遍历字典的键值对"}},[t._v("#")]),t._v(" "),s("strong",[t._v("遍历字典的键值对")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("person "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'张三'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'age'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'北京'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" person"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v(": ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"函数返回多值"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#函数返回多值"}},[t._v("#")]),t._v(" "),s("strong",[t._v("函数返回多值")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_coordinates")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回元组")]),t._v("\n\nx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" z "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_coordinates"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 直接解包")]),t._v("\n")])])]),s("h3",{attrs:{id:"嵌套解包"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#嵌套解包"}},[t._v("#")]),t._v(" "),s("strong",[t._v("嵌套解包")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'张三'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'工程师'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'李四'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'医生'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("age"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("今年")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("age"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("岁，职业是")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("总结")]),t._v("：您看到的代码是标准的 Python 解包语法，"),s("code",[t._v("start, end, _, label_type")]),t._v(" 分别对应列表中的四个元素，用 "),s("code",[t._v("_")]),t._v(" 忽略不需要的实体文本数据。这是处理序列标注任务时的常见模式！")])])}),[],!1,null,null,null);s.default=e.exports}}]);