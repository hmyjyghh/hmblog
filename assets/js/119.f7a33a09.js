(window.webpackJsonp=window.webpackJsonp||[]).push([[119],{562:function(t,_,v){"use strict";v.r(_);var a=v(3),n=Object(a.a)({},(function(){var t=this,_=t._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h2",{attrs:{id:"量化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#量化"}},[t._v("#")]),t._v(" 量化")]),t._v(" "),_("h3",{attrs:{id:"_1-训练后量化-ptq-和量化感知训练-qat-与什么区别"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-训练后量化-ptq-和量化感知训练-qat-与什么区别"}},[t._v("#")]),t._v(" 1. 训练后量化(PTQ)和量化感知训练(QAT)与什么区别?")]),t._v(" "),_("blockquote",[_("p",[t._v("大模型两种常见的量化方法: 训练后量化 和 量化感知训练")])]),t._v(" "),_("ul",[_("li",[t._v("训练后量化(Post-Training Quantization,PTQ):模型首先经过训练，已达到收敛, 然后我们将其权重转换为较低的精度,而无需进行更多训练。与训练相比,实施起来通常相当便宜。")]),t._v(" "),_("li",[t._v("量化感知训练(Quantization-Aware Training, QAT): "),_("strong",[t._v("在预训练或进一步微调期间应用量化。")]),t._v(" QAT能够获得更好的性能, 但需要额外的计算资源和对代表性训练数据的访问。")])]),t._v(" "),_("h3",{attrs:{id:"_2-什么是wight-only-量化技术-它有什么优点"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2-什么是wight-only-量化技术-它有什么优点"}},[t._v("#")]),t._v(" 2. 什么是Wight Only 量化技术，它有什么优点?")]),t._v(" "),_("blockquote",[_("p",[t._v("许多关于LLM模型量化的研究都有相同的观察结果:")])]),t._v(" "),_("ul",[_("li",[t._v("简单的低精度(例如8bit) "),_("strong",[t._v("训练后量化")]),t._v("会导致性能显着下降,这主要是由于动态的activation和静态的weight量化策略无法保持一致。")]),t._v(" "),_("li",[t._v("为了不损失精度而提高性能,一般会采用Weight Only量化技术: "),_("strong",[t._v("即只把Weight量化成int8格式")]),t._v(",以降低访存压力。到实际Kernel内部再Dequantize回fp16,进行矩阵乘计算。")]),t._v(" "),_("li",[t._v("Weight Only量化的典型案例是 AWQ:Activation-awareWeight Quartization, 即只对weight进行量化以实现压缩和加速的效果。")])]),t._v(" "),_("blockquote",[_("p",[t._v("Weight Only 量化的完整流程：")])]),t._v(" "),_("ol",[_("li",[t._v("存储/传输时：只把权重（Weight）量化为 int8（输入 Feature 仍保持 fp16/fp32），目的是减少权重的访存带宽（int8 比 fp16 存储空间小一半）；")]),t._v(" "),_("li",[t._v("Kernel 计算时：先把 int8 权重 "),_("strong",[t._v("反量化（Dequantize）回 fp16")]),t._v("（保证计算精度，因为输入本身是 fp16，两者类型一致才能运算）；")]),t._v(" "),_("li",[t._v("核心运算：反量化后的 fp16 权重，与 fp16 输入特征执行 "),_("strong",[t._v("矩阵乘计算")]),t._v("（这正是模型前向推理的核心运算步骤）。")])]),t._v(" "),_("p",[t._v("简单说：“矩阵乘”是"),_("strong",[t._v("深度学习模型中权重和输入的核心运算形式")]),t._v("，Weight Only 量化只是优化了权重的存储/传输格式，并没有改变核心运算的本质，所以这里的表述是准确的～")]),t._v(" "),_("h3",{attrs:{id:"_3-llms中-量化权重和量化激活的区别是什么"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3-llms中-量化权重和量化激活的区别是什么"}},[t._v("#")]),t._v(" 3. LLMS中,量化权重和量化激活的区别是什么?")]),t._v(" "),_("ul",[_("li",[t._v("量化是"),_("strong",[t._v("减少模型权重和激活的精度")]),t._v("的过程。"),_("strong",[t._v("和数据转换息息相关")])]),t._v(" "),_("li",[t._v("大多数模型使用32位或16位的精度进行训练, 其中每个参数和激活元素占据32位或16位的存储器空间, 即单精度浮点数。然而,大多数深度学习模型可以使用八位甚至更少的位数有效表示。")])]),t._v(" "),_("p",[_("strong",[t._v("减少模型的精度可以带来多个好处。")])]),t._v(" "),_("ol",[_("li",[t._v("如果模型在内存中占用的空间较小, 您可以在相同的硬件上放置更大的模型。")]),t._v(" "),_("li",[_("strong",[t._v("量化还意味着您可以在相同的带宽上传输更多的参数")]),t._v(", 这有助于加速受带宽限制的模型。")])]),t._v(" "),_("p",[t._v("对于LLM,有许多不同的量化技术,涉及降低激活、权重或两者精度。量化权重要容易得多,因为它们在训练后\n是固定的。然而,这可能会浪费一些性能,因为激活仍然在更高的精度度上保持。GPU没有用于乘法INT8和FP16数\n字的专用硬件,因此权重必须再转换为更高精度以进行实际操作。")]),t._v(" "),_("p",[t._v("也可以量化激活,即transformer块和网络层的输入,但这带来了自己的挑战。激活向量通常包含异常值,有效地\n增加了它们的动态范围,使得以比权重更低的精度表示这些值更具挑战划生。")]),t._v(" "),_("h3",{attrs:{id:"_4-llm-int8-量化流程是什么-有什么缺陷"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-llm-int8-量化流程是什么-有什么缺陷"}},[t._v("#")]),t._v(" 4. LLM.Int8()量化流程是什么,有什么缺陷?")]),t._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/quan/Int-8.png",alt:"Int 8"}})]),t._v(" "),_("h3",{attrs:{id:"_5-说一下gptq量化过程以及优缺点"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_5-说一下gptq量化过程以及优缺点"}},[t._v("#")]),t._v(" 5. 说一下GPTQ量化过程以及优缺点")]),t._v(" "),_("p",[t._v("GPTQ目前几乎是4bit/3bit方案的默认首选,但也仅限于开源世界可用，离落地认定的靠谱精度还是有比较大的距离")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("该方法的思路为:利用hessian信息作为准则判定每个权重量化后对输出loss(通常定义为MSE)造成的影响,量化影\n响最大的权重(即最敏感)挑选出来先进行量化,然后对其他权重进行更新来补偿该权重量化导致的影响,如此往复,\n直至全部量化结果.")])]),t._v(" "),_("li",[_("p",[t._v("当然,在GPTQ中作了一些简化,比如是基于列元素进行量化循环,来减少算法的运行时间,")])])]),t._v(" "),_("h3",{attrs:{id:"_6-awq量化的步骤是什么"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_6-awq量化的步骤是什么"}},[t._v("#")]),t._v(" 6. AWQ量化的步骤是什么?")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("首先, **AWQ使用group量化将权重分组为多个子矩阵。**然后,AWQ使用activation-aware的方法来量化每个\n子矩阵。")])]),t._v(" "),_("li",[_("p",[t._v("最后,AWQ使用无重新排序的在线反量化来提高量化性能。AWQ的activation-aware方法可以提高\n量化精度,这是因为激活值在量化后的影响可以通过量化系数进行补偿。具体来说,AWQ首先计算每个子矩阵的\n激活分布,然后使用该分布来生成量化系数。")])])]),t._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/quan/quan-2.png",alt:"Int 8"}})]),t._v(" "),_("p",[t._v("AWQ的无重新排序的在线反量化可以提高量化性能,这是因为它不需要对权重进行重新排序,可以直接在量化后\n的权重上进行反量化。\nAWQ在各种LLM上进行了实验,结果表明,AWQ可以实现3/4位量化,在相同精度下,AWQ的模型大小比\n原始模型小1/4,推理速度比GPTQ快1.45倍。")]),t._v(" "),_("h3",{attrs:{id:"_7-请列举一些大型语言模型的模型压缩技术。"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_7-请列举一些大型语言模型的模型压缩技术。"}},[t._v("#")]),t._v(" 7. 请列举一些大型语言模型的模型压缩技术。")]),t._v(" "),_("p",[t._v('剪裁: 类似"化学结构式的减肥",将模型结构中对预测结果不重重要的网络结构剪裁掉,使网络结构变得更\n加"瘦身"。比如,在每层网络,有些神经元节点的权重非常小,对模型加载信息的影响微乎其微。如果将\n这些权重较小的神经元删除,则既能保证模型精度不受大影响,又能减小模型大小。')]),t._v(" "),_("ul",[_("li",[_("p",[t._v('量化:类似"量子级别的减肥",神经网络模型的参数一般都用foat32的数据表示,但如果我们将float32的\n数据计算精度变成int8的计算精度, '),_("strong",[t._v("则可以牺牲一点模型精度来换取更快的计算速度。")])])]),t._v(" "),_("li",[_("p",[t._v('蒸馏: 类似"老师教学生", '),_("strong",[t._v("使用一个效果好的大模型指导一个小模型训练,")]),t._v(" 因为大模型可以提供更多的软分类信息量, 所以会训练出一个效果接近大模型的小模型。")])])]),t._v(" "),_("h3",{attrs:{id:"_8-llm-中推理优化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_8-llm-中推理优化"}},[t._v("#")]),t._v(" 8. LLM 中推理优化")]),t._v(" "),_("p",[t._v("LLM 推理服务重点关注两个指标:吞吐量和延时:")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("吞吐量:主要从系统的角度来看,即系统在单位时间内能处理的tokens数量。计算方法为系统处理完成的\ntokens个数除以对应耗时, 其中tokens个数一般指输入序列和输出序列长度之和。吞吐量越高, 代表LLM\n服务系统的资源利用率越高, 对应的系统成本越低。")])]),t._v(" "),_("li",[_("p",[t._v("延时: 主要从用户的视角来看,即用户平均收到每个token所需位时间。计算方法为用户从发出请求到收到\n完整响应所需的时间除以生成序列长度。一般来讲, 当时延不大于500ms/token时, "),_("strong",[t._v("用户使用体验会比较流畅。")])])])]),t._v(" "),_("p",[t._v("吞吐量关注系统成本, 高吞吐量代表系统单位时间处理的请求大系统利用率高。时延关注用户使用体验,即返回\n结果要快。这两个指标一般情况下需要会相互影响, 因此需要权衡。\n例如, 提高吞吐量的方法一般是提升"),_("code",[t._v("batch_size")]),t._v(", 即将用户的请求由串行改为并行。但"),_("code",[t._v("batch_size")]),t._v("的增大会在一\n定程度上损害每个用户的时延,因为以前只计算一个请求,现在合并计算多个请求,每个用户等待的时间变长。")]),t._v(" "),_("h3",{attrs:{id:"_9-什么是kv-cache技术-它具体是如何实现的"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_9-什么是kv-cache技术-它具体是如何实现的"}},[t._v("#")]),t._v(" 9. 什么是KV Cache技术，它具体是如何实现的？")]),t._v(" "),_("p",[t._v("Transformer模型具有自回归推理的特点, 即每次推理只会预测输出一个token, 当前轮输出token与历史输入tokens拼接, 作为下一轮的输入tokens, 反复执行多次。该过程中, 前后两轮的输入只相差一个token, 存在重复计算。"),_("strong",[t._v("KV Cache技术实现了将可复用的键值向量结果保存下来")]),t._v(", 从而避免了重复计算。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/quan/kv-cache.png",alt:"Int 8"}})]),t._v(" "),_("h3",{attrs:{id:"_10-paged-attention的原理是什么-解决了大模型推理中的什么问题"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_10-paged-attention的原理是什么-解决了大模型推理中的什么问题"}},[t._v("#")]),t._v(" 10. Paged Attention的原理是什么, 解决了大模型推理中的什么问题?")]),t._v(" "),_("p",[_("strong",[t._v("LLM推理服务的吞吐量指标主要受制于显存限制。")])]),t._v(" "),_("ul",[_("li",[t._v("研究团队发现现有有系统由于缺乏精细的显存管理方法而浪费了60%至80%的显存, 浪费的显存主要来自"),_("code",[t._v("KV Cache")]),t._v("。因此, 有效管理"),_("code",[t._v("KV Cache")]),t._v("是一个重大挑战。")])]),t._v(" "),_("blockquote",[_("p",[t._v("具体来讲, Paged Attention 将每个序列的"),_("code",[t._v("KV Cache")]),t._v("分成若干块, 每个块包含固定数量token的键和值。")])]),t._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/quan/Paged_Attention.png",alt:"Paged Attention"}})]),t._v(" "),_("p",[t._v("total_gpu_mermory为物理显存量, block_size为块大小(默认设为16)。")]),t._v(" "),_("p",[t._v("在实际推理过程中,")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("维护一个逻辑块到物理块的映射表, 多个逻辑块可以对应一个物理块, 通过"),_("strong",[t._v("引用计数")]),t._v("来表示物理块被引用的次数。")])]),t._v(" "),_("li",[_("p",[t._v("当引用计数大于一时,代表该物理块被使用, 当引用计数等于零时, 代表该物理块被释放。")])]),t._v(" "),_("li",[_("p",[t._v("通过该方式即可实现将地址不连续的物理块串联在一起统一管理。")])]),t._v(" "),_("li",[_("p",[t._v("Paged Attention 参考了操作系统中的内存分页的形式，应用到KV Cache的管理中, "),_("strong",[t._v("提高了显存利用效率")]),t._v("。")])]),t._v(" "),_("li",[_("p",[t._v("另外, 通过token块粒度的显存管理, 系统可以精确计算出剩余显存可容纳的token块的个数, 配合后文"),_("code",[t._v("Dynamic Batching")]),t._v("技术, 即可避免系统发生显存溢出的问题。")])])]),t._v(" "),_("blockquote",[_("p",[t._v("总结一下：")])]),t._v(" "),_("ul",[_("li",[t._v("把物理上不连续的块，在逻辑上做成一个连续的")]),t._v(" "),_("li",[t._v("为了提升显存的利用率")])]),t._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/quan/Paged_Attention-2.png",alt:"Paged Attention"}})]),t._v(" "),_("h3",{attrs:{id:"_11-什么是dynamic-batching技术"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_11-什么是dynamic-batching技术"}},[t._v("#")]),t._v(" 11. 什么是Dynamic Batching技术?")]),t._v(" "),_("p",[t._v("也就是：动态批处理技术")]),t._v(" "),_("ul",[_("li",[t._v("LLM中不同的任务。从聊天机器人的简单问答响应到文档的摘要或长代码块的生成, 工作负载是高度动态的, 输出的大小相差几个数量级。")])]),t._v(" "),_("p",[t._v("这种多样性可能使得"),_("strong",[t._v("分批处理请求并有效地并行执行")]),t._v("成为一种具有有挑战性的优化方法, "),_("strong",[t._v("eg: 一些请求比其他请求提前完成。")])]),t._v(" "),_("p",[_("img",{attrs:{src:"/hmblog/images/llm/quan/Dynamic_Batching.png",alt:"Dynamic Batching"}})]),t._v(" "),_("p",[t._v("为了管理这些动态负载, 许多LLM服务器解决方案包括一种优化的调度技术, 称为连续批处理。")]),t._v(" "),_("ul",[_("li",[t._v("它利用了将LLM的整体文本生成过程"),_("strong",[t._v("分解为对模型的多次执行迭代")]),t._v("。")]),t._v(" "),_("li",[t._v("使用在飞行中批处理时, 服务器运行时"),_("strong",[t._v("在整个批处理完成之前就开始将完成的序列从批次中删除。")])]),t._v(" "),_("li",[t._v("然后它开始执行新的请求, 而其他请求仍在进行中。在实际使用案例中, "),_("strong",[t._v("连续批处理可以大大增加GPU的整体利用率。")])])]),t._v(" "),_("ol",{attrs:{start:"12"}},[_("li",[_("p",[t._v("什么是continuous batching技术, 为什么他的效率比动态batching效率高?")])]),t._v(" "),_("li",[_("p",[t._v("大模型训练时有哪些模型内存的优化方法?")])]),t._v(" "),_("li",[_("p",[_("strong",[t._v("模型并行化")])])])]),t._v(" "),_("ul",[_("li",[t._v("它可以将模型的不同部分分配到不同的GPU或机器上进行处理。例如我们可以采用参数服务器架构, 将模\n型参数分布在多台机器上,从而分散内存需求。")])]),t._v(" "),_("ol",{attrs:{start:"2"}},[_("li",[_("strong",[t._v("数据并行化")])])]),t._v(" "),_("ul",[_("li",[t._v("可以将训练数据分成小批次并行处理,每个GPU处理一个小批次数据,然后合并梯度进行模型更新。通常会选择同步数据并行或异步数据并行的方法来实现这点。")])]),t._v(" "),_("ol",{attrs:{start:"3"}},[_("li",[_("strong",[t._v("梯度检查点")])])]),t._v(" "),_("ul",[_("li",[t._v("它可以节省内存, 通过在前向传播时选择性地保存部分中间线结果,可以减少内存消耗。")]),t._v(" "),_("li",[t._v("这些中间结果可以在反向传播时重新计算出来, 从而节省内存。")])]),t._v(" "),_("ol",{attrs:{start:"4"}},[_("li",[_("p",[_("strong",[t._v("混合精度训练")]),t._v("\n通过使用半精度浮点数(FP16)代替全精度浮点数(FP32)进行计算, 我们可以减少内存使用, 同时保持训练的稳定性和精度。")])]),t._v(" "),_("li",[_("p",[_("strong",[t._v("零冗余优化器(ZeRO)")]),t._v("\n它将优化器状态、梯度和参数分片存储在不同的设备上, 避免在所有设备上冗余地存储所有参数和梯度, 从而显著减少内存占用。")])]),t._v(" "),_("li",[_("p",[_("strong",[t._v("分布式训练框架")]),t._v("\n如Horovod、DeepSpeed和PyTorch Lightning提供了许多内存优化和高效并行训练的工具和策略。这些\n工具能够帮助我们更好地管理和优化内存使用, 提高大模型川练的效率和可扩展性。")])])])])}),[],!1,null,null,null);_.default=n.exports}}]);