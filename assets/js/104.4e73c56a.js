(window.webpackJsonp=window.webpackJsonp||[]).push([[104],{551:function(_,v,t){"use strict";t.r(v);var l=t(3),n=Object(l.a)({},(function(){var _=this,v=_._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[v("h2",{attrs:{id:"阶段一"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段一"}},[_._v("#")]),_._v(" 阶段一")]),_._v(" "),v("ol",[v("li",[_._v("说一下ChatGPT的优缺点")])]),_._v(" "),v("ul",[v("li",[_._v("优点：\n"),v("ul",[v("li",[_._v("强大的语言理解和生成能力")]),_._v(" "),v("li",[_._v("广泛的应用场景")]),_._v(" "),v("li",[_._v("出色的上下文理解和多轮对话能力")]),_._v(" "),v("li",[_._v("多功能性与通用性")]),_._v(" "),v("li",[_._v("大幅提升效率")])])]),_._v(" "),v("li",[_._v("缺点：\n"),v("ul",[v("li",[_._v('模型"幻觉"')]),_._v(" "),v("li",[_._v("训练数据有截止日期、知识陈旧与缺乏实时性")]),_._v(" "),v("li",[_._v("计算资源需求高")]),_._v(" "),v("li",[_._v("可能产生偏见与有害内容")]),_._v(" "),v("li",[_._v("在专业领域深度不足")]),_._v(" "),v("li",[_._v("上下文长度限制")]),_._v(" "),v("li",[_._v("会让人们对它产生依赖，渐渐地缺乏独立思考的能力")])])])]),_._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[_._v("请简述下Transformer基本流程")])]),_._v(" "),v("blockquote",[v("p",[_._v("简化流程: 输入 -> 编码（通过自注意力理解源序列） -> 解码（通过交叉注意力参考编码结果，并通过掩码自注意力生成目标序列） -> 输出。")])]),_._v(" "),v("ul",[v("li",[_._v("输入层：将输入的文本序列转换为向量表示")]),_._v(" "),v("li",[_._v("编码器层：将输入向量编码为上下文表示")]),_._v(" "),v("li",[_._v("解码器层：根据上下文表示生成输出序列")]),_._v(" "),v("li",[_._v("输出层：将输出向量转换为文本序列")]),_._v(" "),v("li",[_._v("位置编码：为了保留序列中单词的位置信息，引入位置编码")]),_._v(" "),v("li",[_._v("自注意力机制：模型通过自注意力机制来学习输入序列中不同位置之间的依赖关系")]),_._v(" "),v("li",[_._v("多头注意力机制：模型通过多头注意力机制来学习输入序列中不同位置之间的依赖关系")]),_._v(" "),v("li",[_._v("前馈神经网络：每个位置的表示通过前馈神经网络进行处理")]),_._v(" "),v("li",[_._v("层归一化：在每个子层之间引入层归一化，提高模型的训练稳定性")]),_._v(" "),v("li",[_._v("残差连接：引入残差连接，解决深度神经网络训练中的梯度消失问题")])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("为什么基于Transformer的架构需要多头注意力机制？")])]),_._v(" "),v("ul",[v("li",[_._v("考察点，多头注意力机制，多头，多个专家，多个角度去分析和理解")])]),_._v(" "),v("blockquote",[v("p",[_._v("简单来说，"),v("strong",[_._v("多头机制允许模型同时从不同的“表示子空间”和不同“角度”关注输入信息，极大地增强了模型的表征能力和泛化能力。")])])]),_._v(" "),v("ol",[v("li",[_._v("克服单头注意力的局限性：增强模型的“视角”")]),_._v(" "),v("li",[_._v("并行捕捉多种类型的关系")]),_._v(" "),v("li",[_._v("增加模型的表征能力和稳健性")])]),_._v(" "),v("p",[_._v("#技术实现简述")]),_._v(" "),v("p",[_._v("在技术上，多头注意力的实现非常优雅：")]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("线性投影")]),_._v("：对于给定的输入，通过 "),v("code",[_._v("h")]),_._v(" 个（头的数量）不同的线性投影矩阵，分别生成 "),v("code",[_._v("h")]),_._v(" 套 "),v("strong",[_._v("Query、Key、Value")]),_._v(" 向量。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("并行计算")]),_._v("：在这 "),v("code",[_._v("h")]),_._v(" 套投影上并行地执行缩放点积注意力计算，得到 "),v("code",[_._v("h")]),_._v(" 个输出矩阵。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("拼接与融合")]),_._v("：将这 "),v("code",[_._v("h")]),_._v(" 个输出矩阵拼接起来，最后通过一个线性层进行融合，将信息整合回原始的维度。")])]),_._v(" "),v("li",[v("p",[_._v("编码器，解码器，编解码LLM的区别？")]),_._v(" "),v("ul",[v("li",[_._v("编码器：将输入序列编码为上下文表示")]),_._v(" "),v("li",[_._v("解码器：根据上下文表示生成输出序列")]),_._v(" "),v("li",[_._v("编解码LLM：同时进行编码和解码，生成输出序列")])])])]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[_._v("特性")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("编码器-仅编码")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("解码器-仅解码")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("编码器-解码器")])])]),_._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("核心注意力")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("双向注意力")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("因果/掩码注意力")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("编码器：双向"),v("br"),_._v("解码器：因果 + "),v("strong",[_._v("交叉注意力")])])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("训练目标")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("掩码语言建模")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("自回归语言建模")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("序列到序列学习")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("信息流")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("看到整个输入")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("只能看到左侧上下文")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("编码器看全源序列，解码器自回归生成目标序列")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("核心能力")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("理解")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("生成")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("转换")])])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("代表模型")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("BERT, RoBERTa")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("GPT系列, LLaMA")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("T5, BART, 原始Transformer")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("类比")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("阅读理解专家")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("故事讲述者")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("翻译官")])])])])]),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[v("p",[_._v("在语言模型中强化学习的概念?它如何应用于ChatGPT？")]),_._v(" "),v("ul",[v("li",[_._v("强化学习里面包含，Action、State、Reward Model、Policy、Value Function等")]),_._v(" "),v("li",[_._v("Action：模型可以执行的操作")]),_._v(" "),v("li",[_._v("State：模型当前的状态")]),_._v(" "),v("li",[_._v("Reward Model：用于评估模型执行 Action 后的奖励")]),_._v(" "),v("li",[_._v("Policy：模型的策略，用于选择 Action")]),_._v(" "),v("li",[_._v("Value Function：模型的价值函数，用于评估模型在当前状态下的价值")]),_._v(" "),v("li",[_._v("强化学习，可以根据用户反馈或动作，根据得分及时更新策略，以循序渐进地形式，得到最大期望回报")]),_._v(" "),v("li",[_._v("强化学习里面的奖励信号可以是用户的反馈，也可以是模型自己的反馈")]),_._v(" "),v("li",[_._v("强化学习是一种通过奖励信号来训练模型的方法")]),_._v(" "),v("li",[_._v("强化学习可以应用于ChatGPT，通过与用户的交互来训练模型，使模型能够生成更符合用户需求的文本")])])]),_._v(" "),v("li",[v("p",[_._v("在GPT模型中，什么是温度系数？")])])]),_._v(" "),v("blockquote",[v("p",[_._v("温度系数是一个控制生成文本随机性和创造性的超参数。")])]),_._v(" "),v("ul",[v("li",[_._v("温度系数是一个超参数，用于控制模型的输出分布的多样性")]),_._v(" "),v("li",[_._v("温度系数越小，模型的输出分布越集中，模型的输出结果越确定")]),_._v(" "),v("li",[_._v("温度系数越大，模型的输出分布越分散，模型的输出结果越随机")])]),_._v(" "),v("ol",{attrs:{start:"7"}},[v("li",[_._v("什么是旋转位置编码(Rotary Position Encoding)，简称ROPE？")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("旋转位置编码是一种用于Transformer模型的位置编码方法")])]),_._v(" "),v("li",[v("p",[_._v("旋转位置编码的核心思想是，将位置编码的维度分为两部分，分别对这两部分进行旋转")])]),_._v(" "),v("li",[v("p",[_._v("旋转位置编码的优势是，它可以保持模型的平移不变性，同时也可以保持模型的旋转不变性")])]),_._v(" "),v("li",[v("p",[_._v("旋转位置编码的劣势是，它的计算复杂度较高")])]),_._v(" "),v("li",[v("p",[_._v("旋转位置编码是一种能将相对位置信息集成到 "),v("code",[_._v("self-attention")]),_._v(" 中, 进而"),v("strong",[_._v("提升 transformer 架构性能的位置编码方式")]),_._v(", 和绝对位置编码相比, RoPE 具有很好的"),v("code",[_._v("外推性")]),_._v(", 是目前的主流位置编码方式.")])]),_._v(" "),v("li",[v("p",[_._v("外推性的解释, 通俗来说就是训练的时候限制了 512 的上下文长度，那么推理时如果面对超过该长度的文本，LLM 可能无法正确处理.")])])]),_._v(" "),v("p",[_._v("绝对位置编码（比如Transformer原始的正弦余弦编码）是给每个位置一个“专属身份证”，比如位置0、1、2……511各有唯一编码。"),v("strong",[_._v("但训练时没见过512及以后的位置，推理时遇到更长文本")]),_._v("，就只能用“新身份证”（比如重复编码或随机编码），"),v("strong",[_._v("模型自然不认识，导致性能下降。")])]),_._v(" "),v("p",[_._v("而RoPE的核心是用“相对位置”代替“绝对位置”：它通过三角函数计算位置之间的相对距离，比如位置i和j的编码差异只和（i-j）有关，和i、j的绝对数值关系不大。就像两个人站成一排，RoPE记的是“你在我左边第3个”，而不是“你站在第100号位置”。")]),_._v(" "),v("p",[_._v("这样一来，即使推理时文本长度超过训练时的512（比如到1000），"),v("strong",[_._v("模型依然能通过相对距离判断位置关系")]),_._v("，不会因为遇到“没见过的绝对位置号”而混乱，这就是"),v("strong",[_._v("外推性好的直观体现")]),_._v("——"),v("strong",[_._v("能更好地处理比训练时更长的文本")]),_._v("。")]),_._v(" "),v("ol",{attrs:{start:"8"}},[v("li",[_._v("为什么现在的大模型大多是decoder-only的架构？")])]),_._v(" "),v("ul",[v("li",[v("p",[v("strong",[_._v("Decoder 的天然属性")]),_._v("：Decoder-only 架构本身就是为自回归生成而设计的。它的核心机制是"),v("strong",[_._v("因果掩码")]),_._v("，确保在生成每个词时，只能看到它之前的词，而无法看到未来的词。这与文本生成的过程完全一致。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("计算效率")]),_._v("：Decoder-only 模型架构"),v("strong",[_._v("高度统一和简化")]),_._v("。无论是预训练还是微调，它都在执行同一个核心任务：预测下一个词。这意味着整个计算图非常高效，可以更好地利用大规模并行计算（如 GPU/TPU）。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("参数效率")]),_._v("：在 Encoder-Decoder 架构中，参数被分摊到两个结构不同的组件中。而有研究表明，在计算预算固定的情况下，"),v("strong",[_._v("将所有参数集中在一个庞大的、统一的 Decoder 中，比将其分割给 Encoder 和 Decoder 能带来更好的性能。")]),_._v(" 即“大力出奇迹”的策略在 Decoder-only 模型上更有效。")])]),_._v(" "),v("li",[v("p",[_._v("scaling Laws（缩放定律）：OpenAI 等机构提出的缩放定律表明，模型的性能与模型大小、数据量和计算量之间存在可预测的幂律关系。"),v("strong",[_._v("Decoder-only 架构被证明是沿着这条 scaling law 进行扩展的最直接、最可靠的路径。")])])])]),_._v(" "),v("p",[_._v("总之，Decoder-only 架构成为主流，本质上"),v("strong",[_._v("是因为它在“生成能力”这个核心目标上，与任务本身最匹配")]),_._v("，并且在当今“算力为王”的时代，"),v("strong",[_._v("提供了最优的扩展性、效率和性能平衡。")])]),_._v(" "),v("ol",{attrs:{start:"9"}},[v("li",[_._v("ChatGPT的训练步骤有哪些？")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/chat-gpt/chat-gpt-train.png",alt:"ChatGPT的训练步骤"}})]),_._v(" "),v("ol",[v("li",[v("p",[_._v("第1步：预训练 - 构建知识基础")])]),_._v(" "),v("li",[v("p",[_._v("第2步：监督微调 - 学会对话格式")])]),_._v(" "),v("li",[v("p",[_._v("第3步：从人类反馈中强化学习 - 对齐人类偏好")]),_._v(" "),v("ul",[v("li",[_._v("3.1 训练奖励模型")]),_._v(" "),v("li",[_._v("3.2 使用强化学习优化策略")])])]),_._v(" "),v("li",[v("p",[_._v("为什么Transformer模型需要位置编码？")])])]),_._v(" "),v("ul",[v("li",[_._v("在 Transformer 模型中，由于不是循环（RNN）结构，模型本身无法捕捉输入序列中元素的位置信息。")]),_._v(" "),v("li",[_._v("因为Transformer的核心自注意力机制本身是“位置无关”的，它无法感知词语的顺序信息，而顺序对于理解语言至关重要。")])]),_._v(" "),v("ol",{attrs:{start:"11"}},[v("li",[_._v("为什么对于ChatGPT而言，提示工程很重要？")])]),_._v(" "),v("ul",[v("li",[_._v("帮助模型理解用户的意图，从而生成符合用户需求的文本")]),_._v(" "),v("li",[_._v("设定角色与人格：控制输出的风格和立场")]),_._v(" "),v("li",[_._v("提示工程是约束模型、让其忠于事实的强大工具。")]),_._v(" "),v("li",[_._v("实现复杂、多步骤的任务分解")])]),_._v(" "),v("p",[_._v("对于复杂任务，一个简单的指令会让模型不知所措。提示工程教你如何将大任务拆解为清晰的、可执行的步骤。")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("混乱的提示")]),_._v("："),v("code",[_._v('"帮我写一份市场计划，要关于新产品，包括社交媒体、预算和竞争对手分析。"')])]),_._v(" "),v("li",[v("strong",[_._v("工程化的提示")]),_._v("：\n"),v("ol",[v("li",[v("strong",[_._v("目标")]),_._v("："),v("code",[_._v('"为一款新的环保水瓶制定一份简要的市场计划。"')])]),_._v(" "),v("li",[v("strong",[_._v("步骤一")]),_._v("："),v("code",[_._v('"首先，分析当前市场上3个主要竞争对手及其优劣势。"')])]),_._v(" "),v("li",[v("strong",[_._v("步骤二")]),_._v("："),v("code",[_._v('"其次，提出一个针对千禧一代的社交媒体推广策略，包括建议使用的平台和内容类型。"')])]),_._v(" "),v("li",[v("strong",[_._v("步骤三")]),_._v("："),v("code",[_._v('"最后，草拟一个初步的月度预算分配表。"')])])])])]),_._v(" "),v("ol",{attrs:{start:"12"}},[v("li",[_._v("如何缓解LLMs复读机问题？")])]),_._v(" "),v("p",[_._v("要缓解这个问题，我们需要一个多管齐下的策略，主要从"),v("strong",[_._v("解码策略")]),_._v("、"),v("strong",[_._v("提示工程")]),_._v("和"),v("strong",[_._v("模型层面")]),_._v("入手。")]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/chat-gpt/LLM-repeat.png",alt:""}})]),_._v(" "),v("ul",[v("li",[_._v("因为LLMs模型的输出结果，是根据输入的提示词，生成的文本")]),_._v(" "),v("li",[_._v("所以，为了避免LLMs模型重复输出相同的文本，我们可以在提示词中，加入一些随机的噪声，如添加随机的单词、句子、段落等")]),_._v(" "),v("li",[_._v("这样，就可以避免LLMs模型重复输出相同的文本")]),_._v(" "),v("li",[_._v("同时，也可以避免LLMs模型输出重复的句子")])]),_._v(" "),v("h2",{attrs:{id:"阶段二"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段二"}},[_._v("#")]),_._v(" 阶段二")]),_._v(" "),v("ol",[v("li",[_._v("解释下langchain Agent的概念？")])]),_._v(" "),v("blockquote",[v("p",[_._v("LangChain Agent核心是让大模型"),v("code",[_._v("自主决策")]),_._v("、"),v("code",[_._v("调用工具解决复杂任务")]),_._v(", 而非单纯生成文本。")])]),_._v(" "),v("ul",[v("li",[_._v("可以根据用户的输入决定调用哪个工具。")]),_._v(" "),v("li",[_._v("帮助构建复杂的应用程序, 这些应用程序需要自适应和物特定于上下文的响应。")]),_._v(" "),v("li",[_._v("LLM 作为代理的“大脑”，负责推理、规划和决策，而工具则是它完成任务所依赖的外部手段。")]),_._v(" "),v("li",[_._v("具有 记忆能力 和  推理能力")]),_._v(" "),v("li",[_._v("LangChain提供了执行器(Agent Executor)，用来运行代理并执行其决策的工具。")])]),_._v(" "),v("ol",[v("li",[_._v("langchain的6大核心组件是什么，它们的作用分别是什么？")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("LLM 和提示:LangChain使管理提示、优化它们以及为所有LLM1创建通用界面变得容易。此外,它还包括\n一些用于处理 LLM 的便捷实用程序。")])]),_._v(" "),v("li",[v("p",[_._v("链(Chain): 这些是对LLM或其他实用程序的调用序列。LangChain为链提供标准接口,与各种工具集成, 为流行应用提供端到端的链。")])]),_._v(" "),v("li",[v("p",[_._v("数据增强生成:LangChain使链能够与外部数据源交互以收集生成步骤的数据。例如,它可以帮助总结长文本或使用特定数据源回答问题。")])]),_._v(" "),v("li",[v("p",[_._v("Agents: Agents让LLM做出有关行动的决定, 采取这些行动, 检查结果, 并继续前进直到工作完成。\nLangChain提供了代理的标准接口,多种代理可供选择,以及端到端的代理示例。")])]),_._v(" "),v("li",[v("p",[_._v("内存（Memory）:LangChain有一个标准的内存接口,有助于维护链或代理调用之间的状态。它还提供了一系列内存实现和使用内存的链或代理的示例。")])]),_._v(" "),v("li",[v("p",[_._v("模型（Models）：包含各大语言模型的LangChain接口和调用细节，以及输出解析机制。")])]),_._v(" "),v("li",[v("p",[_._v("提示模板（Prompts）：使提示工程流线化，进一步激发大语言模型的潜力。")])]),_._v(" "),v("li",[v("p",[_._v("数据检索（Indexes）：构建并操作文档的方法，接受用户的查询并返回最相关的文档，轻松搭建本地知识库。")])]),_._v(" "),v("li",[v("p",[_._v("记忆（Memory）：通过短时记忆和长时记忆，在对话过程中存储和检索数据，让ChatBot记住你。")])]),_._v(" "),v("li",[v("p",[_._v("链（Chains），以特定方式封装各种功能，并通过一系列的组合，自动而灵活地完成任务。")])]),_._v(" "),v("li",[v("p",[_._v("代理（Agents）：通过“代理”让大模型自主调用外部工具和内部工具，使智能Agent成为可能。")])])]),_._v(" "),v("ol",[v("li",[_._v("langchain有哪些优点和明显的缺点？")])]),_._v(" "),v("blockquote",[v("p",[_._v("优点")])]),_._v(" "),v("div",{staticClass:"language- extra-class"},[v("pre",{pre:!0,attrs:{class:"language-text"}},[v("code",[_._v("1. 提供了大量的预置组件，让开发者可以快速构建应用\n2. 组件生态丰富\n3. LangChain提出的Chain、Agent、Tool、Memory等概念已经成为行业标准\n4. 社区活跃，文档完善\n")])])]),v("ol",[v("li",[_._v("快速原型开发能力")])]),_._v(" "),v("ul",[v("li",[_._v("它提供了高度抽象的概念和大量预制组件，让开发者可以像搭乐高一样快速构建LLM应用。")]),_._v(" "),v("li",[_._v("几行代码就能构建一个完整的RAG系统.")])]),_._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[_._v("组件生态丰富")])]),_._v(" "),v("ul",[v("li",[_._v("50+ 文档加载器（PDF、HTML、Word、Notion等）")]),_._v(" "),v("li",[_._v("20+ 文本分割策略")]),_._v(" "),v("li",[_._v("30+ 向量数据库集成")]),_._v(" "),v("li",[_._v("10+ LLM提供商支持")])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("LangChain提出的Chain、Agent、Tool、Memory等概念已经成为行业标准，帮助开发者建立心智模型：")])]),_._v(" "),v("ul",[v("li",[v("code",[_._v("Chain")]),_._v("：可复用的任务流水线")]),_._v(" "),v("li",[v("code",[_._v("Agent")]),_._v("：LLM驱动的决策系统")]),_._v(" "),v("li",[v("code",[_._v("Tool")]),_._v("：Agent可调用的函数")]),_._v(" "),v("li",[v("code",[_._v("Memory")]),_._v("：对话状态管理")])]),_._v(" "),v("ol",{attrs:{start:"4"}},[v("li",[v("strong",[_._v("社区活跃和文档完善")])])]),_._v(" "),v("ul",[v("li",[_._v("GitHub 80k+ stars，庞大的用户社区")]),_._v(" "),v("li",[_._v("详细的文档和示例代码")]),_._v(" "),v("li",[_._v("遇到问题容易找到解决方案")])]),_._v(" "),v("blockquote",[v("p",[_._v("缺点")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("调试困难, 很难追踪问题")])]),_._v(" "),v("li",[_._v("性能开销")]),_._v(" "),v("li",[_._v("API 不稳定与快速迭代\n"),v("ul",[v("li",[_._v("LangChain 的 API 变化非常频繁，导致代码很容易过时，维护成本高。")])])]),_._v(" "),v("li",[_._v("灵活性不够\n"),v("ul",[v("li",[_._v("当需要实现一个非常定制化、非标准的功能时，LangChain 的“框框”可能会限制你。")])])])]),_._v(" "),v("ol",{attrs:{start:"4"}},[v("li",[_._v("langchain有哪些替代方案？")])]),_._v(" "),v("p",[_._v("轻量级替代品的出现：像 LlamaIndex（更专注于 RAG 场景）和 LangGraph（由 LangChain 官方推出，用于构建有状态、多参与者的 Agent 应用）等工具，提供了更专注、更清晰的范式。")]),_._v(" "),v("ol",[v("li",[_._v("LlamaIndex")]),_._v(" "),v("li",[_._v("Haystack")]),_._v(" "),v("li",[_._v("Semantic Kernel（微软出品）")])]),_._v(" "),v("p",[_._v("#横向对比总结")]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",[_._v("维度")]),_._v(" "),v("th",[_._v("LangChain")]),_._v(" "),v("th",[_._v("LlamaIndex")]),_._v(" "),v("th",[_._v("Haystack")]),_._v(" "),v("th",[_._v("Semantic Kernel")])])]),_._v(" "),v("tbody",[v("tr",[v("td",[v("strong",[_._v("核心定位")])]),_._v(" "),v("td",[_._v("通用LLM应用框架")]),_._v(" "),v("td",[v("strong",[_._v("专业RAG框架")])]),_._v(" "),v("td",[_._v("NLP生产流水线")]),_._v(" "),v("td",[_._v("AI规划与集成框架")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("设计哲学")])]),_._v(" "),v("td",[_._v("模块化组合")]),_._v(" "),v("td",[_._v("数据与检索优化")]),_._v(" "),v("td",[v("strong",[_._v("稳定流水线")])]),_._v(" "),v("td",[_._v("传统代码+AI规划")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("学习曲线")])]),_._v(" "),v("td",[_._v("陡峭")]),_._v(" "),v("td",[_._v("中等")]),_._v(" "),v("td",[_._v("中等")]),_._v(" "),v("td",[_._v("较陡峭（概念独特）")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("调试难度")])]),_._v(" "),v("td",[_._v("高（黑盒）")]),_._v(" "),v("td",[_._v("中等")]),_._v(" "),v("td",[v("strong",[_._v("低（清晰流水线）")])]),_._v(" "),v("td",[_._v("中等")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("生产就绪")])]),_._v(" "),v("td",[_._v("中等")]),_._v(" "),v("td",[_._v("中等")]),_._v(" "),v("td",[v("strong",[_._v("高")])]),_._v(" "),v("td",[_._v("中等")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("RAG专业性")])]),_._v(" "),v("td",[_._v("通用")]),_._v(" "),v("td",[v("strong",[_._v("最优")])]),_._v(" "),v("td",[_._v("良好")]),_._v(" "),v("td",[_._v("基础")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("Agent支持")])]),_._v(" "),v("td",[v("strong",[_._v("强大")])]),_._v(" "),v("td",[_._v("有限")]),_._v(" "),v("td",[_._v("有限")]),_._v(" "),v("td",[_._v("独特规划方式")])]),_._v(" "),v("tr",[v("td",[v("strong",[_._v("生态集成")])]),_._v(" "),v("td",[v("strong",[_._v("最丰富")])]),_._v(" "),v("td",[_._v("专注数据源")]),_._v(" "),v("td",[_._v("良好")]),_._v(" "),v("td",[_._v("微软生态")])])])]),_._v(" "),v("p",[_._v("#如何选择？")]),_._v(" "),v("p",[v("strong",[_._v("选择 LlamaIndex 如果：")])]),_._v(" "),v("ul",[v("li",[_._v("你主要就是做 RAG 应用")]),_._v(" "),v("li",[_._v("对检索质量有很高要求")]),_._v(" "),v("li",[_._v("想要更简单直接的 API")])]),_._v(" "),v("p",[v("strong",[_._v("选择 Haystack 如果：")])]),_._v(" "),v("ul",[v("li",[_._v("需要稳定部署到生产环境")]),_._v(" "),v("li",[_._v("重视可观测性和可维护性")]),_._v(" "),v("li",[_._v("构建的是复杂、多步骤的 NLP 流水线")])]),_._v(" "),v("p",[v("strong",[_._v("选择 Semantic Kernel 如果：")])]),_._v(" "),v("ul",[v("li",[_._v("你是微软技术栈用户")]),_._v(" "),v("li",[_._v("需要将 AI 深度集成到现有业务系统中")]),_._v(" "),v("li",[_._v("需要 AI 进行复杂的任务规划和分解")])]),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[_._v("什么是检索增强生成(RAG)？")])]),_._v(" "),v("blockquote",[v("p",[_._v("通过检索外部数据，增强⼤模型的⽣成效果。")])]),_._v(" "),v("ul",[v("li",[_._v("RAG即检索增强⽣成，为LLM提供了从某些数据源检索到的信息，并基于此修正⽣成的答案。")]),_._v(" "),v("li",[_._v("RAG 基本上是Search + LLM 提示，可以通过⼤模型回答查询，并将搜索算法所找到的信息作为⼤模型的上下⽂。")]),_._v(" "),v("li",[_._v("查询和检索到的上下⽂都会被注⼊到发送到 LLM 的提示语中")])]),_._v(" "),v("ol",{attrs:{start:"6"}},[v("li",[_._v("在做知识增强检索时，文本切分有哪些方法？")])]),_._v(" "),v("ul",[v("li",[_._v("按照句⼦来切分")]),_._v(" "),v("li",[_._v("按照字符数来切分")]),_._v(" "),v("li",[_._v("按固定字符数 结合overlap")]),_._v(" "),v("li",[_._v("递归⽅法：RecursiveCharacterTextSplitter")])]),_._v(" "),v("ol",{attrs:{start:"7"}},[v("li",[_._v("目前主流的中文向量模型有哪些？")])]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[_._v("模型系列 / 来源")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("代表性模型举例")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("核心特点与适用场景")])])]),_._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("阿里通义千问 (Qwen)")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("code",[_._v("Qwen3-Embedding")]),_._v("系列 (如8B, 4B版本)")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("专注于"),v("strong",[_._v("纯文本任务")]),_._v("，在MTEB等权威榜单上表现优异，支持超100种语言，适合智能搜索、文本分类和RAG 。")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("智源研究院 (BGE)")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("code",[_._v("BGE-Code-v1")]),_._v(", "),v("code",[_._v("BGE-VL-v1.5")]),_._v(", "),v("code",[_._v("BGE-VL-Screenshot")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("生态丰富，覆盖"),v("strong",[_._v("代码、图文、截图")]),_._v("等多模态检索，在特定领域基准测试中成绩领先，适合需要处理非纯文本数据的场景 。")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("火山引擎 (Seed)")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("code",[_._v("Seed1.6-Embedding")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("强调"),v("strong",[_._v("全模态")]),_._v("能力，支持文本、图像、视频的混合检索，提供自定义指令功能，适合复杂的多模态和跨模态搜索需求 。")])])])]),_._v(" "),v("blockquote",[v("p",[_._v("如果您的主要目的是构建一个高效的语义检索系统（例如用于RAG），那么选择BGE或Qwen-Embedding这类专用模型是更直接、更有效的方案。")])]),_._v(" "),v("ol",{attrs:{start:"8"}},[v("li",[_._v("相比模型直接生成，RAG的优势是什么？")])]),_._v(" "),v("ul",[v("li",[_._v("因为RAG是基于检索外部知识来生成的的，可以提高模型回答问题的准确性")]),_._v(" "),v("li",[_._v("直接生成是模型凭“记忆”和“直觉”回答问题，而 RAG 是让模型先“查阅资料”再回答问题。")])]),_._v(" "),v("blockquote",[v("p",[_._v("解决了直接生成的三大核心痛点")])]),_._v(" "),v("ol",[v("li",[v("p",[_._v("知识滞后与事实性错误（幻觉）")])]),_._v(" "),v("li",[v("p",[_._v("无法溯源，可信度低")])]),_._v(" "),v("li",[v("p",[_._v("处理私有/领域知识时能力不足")])]),_._v(" "),v("li",[v("p",[_._v("SELF-RAG 是什么，SELF-RAG如何提升大型语言模型的质量和准确性？")])])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("SELF-RAG")]),_._v(" 是一种让LLM在生成过程中"),v("strong",[_._v("自我评估")]),_._v("、"),v("strong",[_._v("自我纠正")]),_._v("的框架。")]),_._v(" "),v("li",[_._v("与传统RAG不同，SELF-RAG让模型在生成每个段落时都主动决定是否需要检索、如何利用检索结果，以及评估生成内容的质量。")])]),_._v(" "),v("blockquote",[v("p",[_._v("SELF-RAG 流程")])]),_._v(" "),v("p",[_._v("用户提问 → 检索相关文档 → LLM 对文档进行批判性评估（反思） → 根据评估结果，有选择地、智能地利用文档 → 生成最终答案。")]),_._v(" "),v("p",[_._v("“反思”步骤是通过生成特殊的"),v("strong",[_._v("反思令牌（Special Tokens）")]),_._v(" 来实现的。这些令牌分为以下几类：")]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("检索令牌 - “要不要检索？”")])]),_._v(" "),v("ul",[v("li",[_._v("在生成答案的每一个阶段，模型会先自问：“我需要检索外部文档来帮助生成下一个段落吗？”")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("相关性令牌 - “检索到的东西有用吗？”")])]),_._v(" "),v("ul",[v("li",[_._v("如果执行了检索，模型会评估每一篇检索到的文档。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("支持令牌 - “检索到的信息支持我的说法吗？”")])]),_._v(" "),v("ul",[v("li",[_._v("模型在生成具体陈述时，会判断该陈述是否得到了检索文档的支持。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("效用令牌 - “我生成的答案整体好吗？”")])]),_._v(" "),v("ul",[v("li",[_._v("最后，模型会对生成的完整段落进行整体评估。")]),_._v(" "),v("li",[_._v("评判标准："),v("code",[_._v("Good")]),_._v("， "),v("code",[_._v("Fair")]),_._v("， "),v("code",[_._v("Poor")]),_._v("。这确保了最终输出的综合质量。")])])])]),_._v(" "),v("blockquote",[v("p",[_._v("SELF-RAG 如何提升LLM的质量和准确性？")])]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("SELF-RAG")]),_._v("：通过 “支持令牌” ，模型只会生成有证据支持的陈述。如果证据不足或存在矛盾，模型会选择不生成或明确指出来，从而避免了传播错误信息。")])]),_._v(" "),v("li",[v("ul",[v("li",[_._v("模型不再是信息的被动接收者，而是主动的"),v("strong",[_._v("评估者")]),_._v("。它只会筛选并利用那些被它判定为 "),v("strong",[_._v("“相关”")]),_._v(" 和 "),v("strong",[_._v("“准确”")]),_._v(" 的信息，使得最终答案的根基更加牢固。")])])]),_._v(" "),v("li",[v("ul",[v("li",[_._v("而SELF-RAG可以识别出哪几篇是高度相关的，并"),v("strong",[_._v("重点利用")]),_._v("这些高质量信息，同时忽略或减少使用低质量信息。")])])]),_._v(" "),v("li",[v("ul",[v("li",[_._v("SELF-RAG 的输出可以附带其反思过程（例如，标注出哪些陈述有充分支持）。这为用户提供了"),v("strong",[_._v("可验证的线索")]),_._v("，让用户知道答案的来源和可信度，增强了模型的透明度和可信度。")])])]),_._v(" "),v("li",[v("ul",[v("li",[_._v("当模型自身知识足够时，它选择 "),v("code",[_._v("No Retrieve")]),_._v("，自信地回答；当不确定时，它主动检索 "),v("code",[_._v("Retrieve")]),_._v("；当检索结果不好时，它承认知识的局限性。这使得模型表现得更像一位严谨的专家，而不是一个不懂装懂的学生。")])])]),_._v(" "),v("li",[v("p",[_._v("RAG 和 微调的区别是什么？")])])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("RAG（检索增强⽣成）是把内部的⽂档数据先进⾏embedding，借助检索先获得⼤致的知识范围答案，再结合prompt给到LLM，让LLM⽣成最终的答案")])]),_._v(" "),v("li",[v("p",[_._v("Fine-tuning（微调）是⽤⼀定量的数据集对LLM进⾏局部参数的调整，以期望LLM更加理解我们的业务逻辑，有更好的zero-shot能⼒。")])])]),_._v(" "),v("ol",{attrs:{start:"11"}},[v("li",[_._v("什么是GraphRAG？")])]),_._v(" "),v("p",[_._v("GraphRAG 的核心思想是：将文档库中的信息提取并构建成"),v("strong",[_._v("一个结构化的知识图谱")]),_._v("，然后利用这个图谱来增强大模型的检索和推理能力。")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("GraphRAG")]),_._v("：先从文档中提取实体（如人物、地点、概念、事件）和它们之间的关系，构建一个知识图谱。检索时进行图遍历和关系推理。")])]),_._v(" "),v("blockquote",[v("p",[_._v("GraphRAG VS RAG")])]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[_._v("特性")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("传统 RAG")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("GraphRAG")])])]),_._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("信息组织")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("扁平的文本片段")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("结构化的关系网络")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("检索方式")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("向量相似度")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("关系与结构推理")])])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("推理能力")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("弱，基于局部上下文")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("强，基于全局关系")])])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("可解释性")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("低（为什么检索这些片段？）")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("高（答案路径清晰可见）")])])])])]),_._v(" "),v("blockquote",[v("p",[_._v("GraphRAG 的典型应用场景\nGraphRAG 的优势决定了它更适合 “关联密集、需要推理” 的场景，而非简单的 “文本问答”：")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("知识图谱问答（KGQA）")]),_._v("：如 “查询某明星的合作导演及其代表作”“某药物的适应症和禁忌症关联的疾病”；")]),_._v(" "),v("li",[v("strong",[_._v("商业情报分析")]),_._v("：如 “梳理某行业的产业链关系（上游原材料→中游制造→下游品牌）”“分析某公司的投资版图和竞争对手网络”；")]),_._v(" "),v("li",[v("strong",[_._v("事件脉络梳理")]),_._v("：如 “还原某历史事件的关键人物、时间线和因果关系（如‘二战主要战役的发起方和影响’）”；")]),_._v(" "),v("li",[v("strong",[_._v("合规与风控")]),_._v("：如 “核查某企业的股权穿透关系（是否存在关联交易）”“识别金融诈骗中的人物 - 账户 - 资金流向关联”。")])]),_._v(" "),v("h2",{attrs:{id:"阶段三"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段三"}},[_._v("#")]),_._v(" 阶段三")]),_._v(" "),v("ol",[v("li",[_._v("Prompt design, Prompt Tuning， Fine-tuning 有什么区别？")])]),_._v(" "),v("blockquote",[v("p",[_._v("Prompt design：提示设计，或者叫"),v("code",[_._v("提示工程")])])]),_._v(" "),v("ol",[v("li",[_._v("**学习如何向模型“提问”。**你不动模型本身，只优化你的指令。")])]),_._v(" "),v("blockquote",[v("p",[_._v("Prompt Tuning：提示微调，即微调模型的提示词")])]),_._v(" "),v("ol",[v("li",[_._v("**教模型学会一种新的“内部提问方式”。**你给模型一些可以学习的“软提示”，而不是修改它的核心知识。")]),_._v(" "),v("li",[_._v("软提示有2种，1种是：soft prompt  1种是：hard prompt")])]),_._v(" "),v("blockquote",[v("p",[_._v("Fine-tuning：微调，即微调模型的参数")])]),_._v(" "),v("ul",[v("li",[_._v("全量微调的话: **给模型“回炉重造”，学习新专业。**你直接修改模型的核心参数，让它适应新任务。")])]),_._v(" "),v("ol",[v("li",[v("p",[_._v("基于开源大模型进行微调，有全量参数微调，和 PEFT: 参数高效微调，也就是只有微调少量参数")])]),_._v(" "),v("li",[v("p",[_._v("参数高效的fine-tuning(PEFT)是什么？\nParameter-Efficient Fine-Tuning")]),_._v(" "),v("ul",[v("li",[_._v("用一些策略，仅训练少量参数，大部分模型参数冻结")]),_._v(" "),v("li",[_._v("方法有：\n"),v("ul",[v("li",[_._v("适配器(Adapter)、软提示(Soft Prompts): Prompt Tuning、P-Tuning、Prefix-Tuning")]),_._v(" "),v("li",[_._v("模型参数冻结")]),_._v(" "),v("li",[_._v("LORA")])])])])]),_._v(" "),v("li",[v("p",[_._v("介绍一下Prompt-tuning 技术？")])])]),_._v(" "),v("p",[_._v("它的核心思想是："),v("strong",[_._v("冻结主模型参数，在训练数据前，加入一小段Prompt（只通过优化一小段可学习的“软提示”（Soft Prompt）），来激发模型解决特定任务的潜能。")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("只训练Prompt 的表示层，也就是一个Embedding 模块。")])]),_._v(" "),v("li",[v("p",[_._v("其中， Prompt 有两种形式， 一种是: hard prompt ， 一种是: soft prompt")])])]),_._v(" "),v("p",[_._v("#代码演示")]),_._v(" "),v("div",{staticClass:"language-py extra-class"},[v("pre",{pre:!0,attrs:{class:"language-py"}},[v("code",[v("span",{pre:!0,attrs:{class:"token keyword"}},[_._v("from")]),_._v(" peft "),v("span",{pre:!0,attrs:{class:"token keyword"}},[_._v("import")]),_._v(" PromptTuningConfig"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v(" get_peft_model"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v(" TaskType"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v(" PromptTuningInit\n\n"),v("span",{pre:!0,attrs:{class:"token comment"}},[_._v("# Soft Prompt 不需要显示指定 prompt")]),_._v("\n"),v("span",{pre:!0,attrs:{class:"token comment"}},[_._v("# config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)")]),_._v("\n\n"),v("span",{pre:!0,attrs:{class:"token comment"}},[_._v("# Hard Prompt")]),_._v("\nconfig "),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),_._v(" PromptTuningConfig"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("(")]),_._v("\n   task_type"),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),_._v("TaskType"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(".")]),_._v("CAUSAL_LM"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v("\n   prompt_tuning_init"),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),_._v("PromptTuningInit"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(".")]),_._v("TEXT"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v("\n   prompt_tuning_init_text"),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('"下面是一段人与机器人的对话。"')]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v("\n   num_virtual_tokens"),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),v("span",{pre:!0,attrs:{class:"token builtin"}},[_._v("len")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("(")]),_._v("tokenizer"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("(")]),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('"下面是一段人与机器人的对话。"')]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(")")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("[")]),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('"input_ids"')]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("]")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(")")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v("\n   tokenizer_name_or_path"),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),v("span",{pre:!0,attrs:{class:"token string"}},[_._v('"Langboat/bloom-1b4-zh"')]),_._v("\n"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(")")]),_._v("\n")])])]),v("ol",{attrs:{start:"4"}},[v("li",[_._v("什么是Prefix Tuning？")])]),_._v(" "),v("ul",[v("li",[_._v("核心思想：在每一层之前添加可学习的“前缀”。")]),_._v(" "),v("li",[_._v("相较于Prompt-Tuning和P-tuning,\n"),v("ul",[v("li",[_._v("Prefix-Tuning不再将Prompt加在输入的Embedding层,")]),_._v(" "),v("li",[_._v("而是将其作为可学习的前缀, 放置在Transsformer模型中的每一层中, 具体表现形式为past_key_values。")])])])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/fine-tuning/Prefix-Tuning.png",alt:"Prefix Tuning"}})]),_._v(" "),v("ul",[v("li",[_._v("图中的Prefix Encoder 不会跟 右边的 Embedding 拼起来，而是会放到Transformer Blocks 层里，参与计算，")]),_._v(" "),v("li",[_._v("它是通过past_key_values 形式放进去的")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/fine-tuning/past_key_values.png",alt:"past_key_values"}})]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("普通Prompt-Tuning")]),_._v("：只在模型的"),v("strong",[_._v("输入嵌入层（Input Embedding Layer）")]),_._v(" 添加可训练的提示向量。这相当于在对话开始时给模型一个总体的指令。")]),_._v(" "),v("li",[v("strong",[_._v("Prefix-Tuning")]),_._v("：不仅在输入层，而是在"),v("strong",[_._v("模型的每一层（或某几层）的激活（activation）之前")]),_._v("，都添加一组可训练的前缀向量。这相当于在模型思考的每一个步骤、每一个阶段都不断地进行引导和提醒，确保它不偏离轨道。")])]),_._v(" "),v("hr"),_._v(" "),v("p",[v("strong",[_._v("Prompt Tuning")]),_._v(" VS "),v("strong",[_._v("Prefix Tuning")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("思想")]),_._v("：不修改模型本身，而是在输入序列前添加一些可训练的"),v("strong",[_._v("软提示向量")]),_._v("。模型通过这些提示向量来适应下游任务。")]),_._v(" "),v("li",[v("strong",[_._v("区别")]),_._v(":\n"),v("ul",[v("li",[_._v("Prompt Tuning 直接训练这些向量；")]),_._v(" "),v("li",[_._v("Prefix Tuning 通过一个小型的前馈网络来生成这些向量，训练的是这个网络的参数。")])])])]),_._v(" "),v("hr"),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[_._v("介绍下LORA微调？")])]),_._v(" "),v("ul",[v("li",[_._v("在每一个要计算的大的矩阵(权重)旁边，新起一条分支，")]),_._v(" "),v("li",[_._v("这个分支的话，是由两个小矩阵组成, LoRA_A  和  LoRA_B")]),_._v(" "),v("li",[_._v("那我更新的时候只更新这两个小矩阵")]),_._v(" "),v("li",[_._v("训练完成之后，再把它合并回去，这就是LoRA")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/fine-tuning/Lora1.png",alt:"LoRA 的思想"}})]),_._v(" "),v("ul",[v("li",[_._v("LoRA 原理")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/fine-tuning/Lora2.png",alt:"Lora"}})]),_._v(" "),v("ol",{attrs:{start:"6"}},[v("li",[_._v("相比LORA，AdaLoRA的改进点是什么？")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("思想")]),_._v("：LoRA 的自适应版本。它不是为所有选定的层分配固定的秩，而是根据重要性评分"),v("code",[_._v("动态地调整每个 LoRA 模块的秩（参数预算）")]),_._v("，将更多的参数分配给更重要的模块。")]),_._v(" "),v("li",[v("strong",[_._v("优势")]),_._v("：在相同的参数预算下，通常能获得比标准 LoRA 更好的性能。")])]),_._v(" "),v("blockquote",[v("p",[_._v("引入了3种关键技术")])]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("参数重要性评分")])]),_._v(" "),v("li",[v("strong",[_._v("动态的预算分配和秩调整")])]),_._v(" "),v("li",[v("strong",[_._v("通过SVD参数化进行高效调整")])])]),_._v(" "),v("p",[_._v("#总结与类比")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("LoRA")]),_._v(" 就像给公司每个部门分配"),v("strong",[_._v("固定且相同")]),_._v("的预算，不管这个部门是核心研发还是后勤支持。")]),_._v(" "),v("li",[v("strong",[_._v("AdaLoRA")]),_._v(" 则像一位"),v("strong",[_._v("精明的CEO")]),_._v("，他会根据每个部门的业绩（重要性分数）和公司总预算，定期进行审查：削减表现不佳部门的预算（修剪），并将资源重新分配给高绩效、高潜力的部门（生长）。")])]),_._v(" "),v("p",[_._v("因此，AdaLoRA的主要优势在于其"),v("strong",[_._v("自适应性")]),_._v("和"),v("strong",[_._v("效率")]),_._v("。")]),_._v(" "),v("ol",{attrs:{start:"7"}},[v("li",[v("p",[_._v("QLORA模型有什么创新点？")])]),_._v(" "),v("li",[v("p",[_._v("稀疏微调是怎么工作的，有哪几个步骤？")])])]),_._v(" "),v("p",[v("strong",[_._v("只更新模型庞大参数中的一小部分（稀疏），而冻结其余大部分参数。")])]),_._v(" "),v("p",[_._v("目标： 极大地减少微调时需要更新的参数量。")]),_._v(" "),v("ul",[v("li",[_._v("核心思想是："),v("strong",[_._v("大型预训练模型已经包含了丰富的通用知识，对于特定的下游任务，我们只需要激发或调整其中与之相关的一小部分神经元或参数子集就足够了，而不需要动整个网络。")])])]),_._v(" "),v("p",[v("strong",[_._v("常见示例与步骤：")])]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("只微调偏置项")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("步骤")]),_._v("：\n"),v("ul",[v("li",[_._v("冻结模型中所有的权重矩阵（如Linear、LayerNorm的权重）。")]),_._v(" "),v("li",[_._v("只将模型中所有的偏置项设置为可训练。")]),_._v(" "),v("li",[_._v("进行训练。由于偏置项的数量远少于权重，实现了稀疏更新。")])])])])]),_._v(" "),v("li",[v("strong",[_._v("只微调某几层")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("步骤")]),_._v("：\n"),v("ul",[v("li",[_._v("冻结模型的大部分层（例如，只更新最后4层Transformer块）。")]),_._v(" "),v("li",[_._v("只解冻靠近输出端的少数几层进行微调。")])])])])]),_._v(" "),v("li",[v("strong",[_._v("只微调注意力模块中的特定部分")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("步骤")]),_._v("：\n"),v("ul",[v("li",[_._v("冻结前馈神经网络。")]),_._v(" "),v("li",[_._v("只更新自注意力机制中的参数（如Query, Key, Value投影矩阵）。")])])])])])]),_._v(" "),v("p",[_._v("方法三：基于附加稀疏适配器的微调（目前最流行）")]),_._v(" "),v("p",[_._v("这是目前最主流的“稀疏微调”方式，也是"),v("strong",[_._v("Parameter-Efficient Fine-Tuning (PEFT)")]),_._v(" 的核心。它"),v("strong",[_._v("不直接更新原始模型参数")]),_._v("，而是"),v("strong",[_._v("引入一小部分额外的、可训练的参数（适配器）")]),_._v("，模型主体保持冻结。")]),_._v(" "),v("p",[v("strong",[_._v("工作原理：")])]),_._v(" "),v("ul",[v("li",[_._v("在预训练模型的架构中，插入一些小的、可训练的模块。")]),_._v(" "),v("li",[_._v("在微调时，"),v("strong",[_._v("只训练这些新增的适配器")]),_._v("，原始模型的"),v("strong",[_._v("所有参数都被冻结")]),_._v("。从参数更新的角度看，这是“稀疏”的，因为只有新增的那一小部分参数被更新。")])]),_._v(" "),v("p",[v("strong",[_._v("常见示例与步骤：")])]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("LoRA及其变种")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("步骤")]),_._v("：\n"),v("ul",[v("li",[_._v("冻结整个预训练模型。")]),_._v(" "),v("li",[_._v("在原有的权重矩阵 ( W ) 旁，注入一个低秩适配器 ΔW = BA。")]),_._v(" "),v("li",[v("strong",[_._v("只训练 ( A ) 和 ( B ) 这两个小矩阵")]),_._v("。前向传播变为：( h = Wx + BAx )。")])])])])]),_._v(" "),v("li",[v("strong",[_._v("Adapter模块")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("步骤")]),_._v("：\n"),v("ul",[v("li",[_._v("冻结整个预训练模型。")]),_._v(" "),v("li",[_._v("在Transformer块中的前馈网络或注意力模块之后，插入一个小的前馈神经网络（Adapter）。")]),_._v(" "),v("li",[v("strong",[_._v("只训练这些插入的Adapter模块")]),_._v("。")])])])])])]),_._v(" "),v("p",[v("strong",[_._v("特点：")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("附加式")]),_._v("：不改变原参数，而是增加新参数。")]),_._v(" "),v("li",[v("strong",[_._v("极高效")]),_._v("：通常参数量极小（仅为原模型的0.01%~1%）。")]),_._v(" "),v("li",[v("strong",[_._v("模块化与安全")]),_._v("：由于原模型不动，可以为一个模型创建多个适配器用于不同任务，且没有灾难性遗忘的风险。")])]),_._v(" "),v("ol",{attrs:{start:"9"}},[v("li",[_._v("监督微调SFT后LLM表现下降的原因？")])]),_._v(" "),v("ul",[v("li",[_._v("监督微调后模型表现下降，通常被称为 “灾难性遗忘” 或 “对齐税” 。")])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/question/SFT-LLM.png",alt:"SFT 后LLM表现下降的原因分析"}})]),_._v(" "),v("p",[_._v("#如何缓解这些问题？")]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("提升数据质量")]),_._v("：精心清洗和构建SFT数据，确保正确性、多样性和高质量。"),v("strong",[_._v("“质量远胜于数量”")]),_._v(" 在SFT中尤其正确。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("谨慎选择超参数")]),_._v("：使用"),v("strong",[_._v("较低的学习率")]),_._v("和"),v("strong",[_._v("较少的训练轮数")]),_._v("（通常1-3个epoch）。始终使用验证集来监控训练，防止过拟合。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("使用参数高效微调技术")]),_._v("：如 "),v("strong",[_._v("LoRA")]),_._v("。这种方法只更新一小部分参数，大部分预训练参数保持不变，从而极大地减轻了灾难性遗忘。")])]),_._v(" "),v("li",[v("p",[_._v("什么是P-Tuning？")])])]),_._v(" "),v("p",[_._v("P-Tuning的思想: 在Prompt-Tuning的基础上,对Prompt部分进行进一步的编码计算,加速收敛。")]),_._v(" "),v("ul",[v("li",[_._v("PEFT中支持两种编码方式,一种是LSTM,一种是MLP。")]),_._v(" "),v("li",[_._v("与Prompt-Tuning不同的是, Prompt的形式只有Soft Prompt。\n"),v("ul",[v("li",[_._v("MLP: 全连接层，3层全连接层")]),_._v(" "),v("li",[_._v("LSTM： 一层LSTM，2层全连接层")])])])]),_._v(" "),v("p",[v("img",{attrs:{src:"/hmblog/images/fine-tuning/P-Tuning.png",alt:"P-Tuning"}})]),_._v(" "),v("ol",{attrs:{start:"11"}},[v("li",[_._v("多轮对话任务如何微调模型？")])]),_._v(" "),v("p",[_._v("一. 选择PEFT 参数高效微调")]),_._v(" "),v("ol",[v("li",[_._v("选择 LoRA / QLoRA  或者 "),v("code",[_._v("Adapter")])]),_._v(" "),v("li",[_._v("对话格式构建与损失计算")]),_._v(" "),v("li",[_._v("step1: 构建带角色的对话模版")]),_._v(" "),v("li",[_._v("step2: 计算仅针对助理回复的损失")]),_._v(" "),v("li",[_._v("模型学会在上下文中生成回复")])]),_._v(" "),v("p",[v("strong",[_._v("LoRA")]),_._v(" 和 "),v("strong",[_._v("QLoRA")]),_._v("。它在全参数微调的基础上，引入了可训练的旁路矩阵，极大地降低了计算成本。")]),_._v(" "),v("p",[v("strong",[_._v("工作原理：")])]),_._v(" "),v("ol",[v("li",[_._v("冻结预训练模型的所有参数。")]),_._v(" "),v("li",[_._v("在模型的注意力层和全连接层旁，注入少量的可训练参数（LoRA 秩分解矩阵）。")]),_._v(" "),v("li",[_._v("训练时，只更新这些新增的参数，而不触动原始模型的巨大参数库。")])]),_._v(" "),v("p",[v("strong",[_._v("为什么PEFT对多轮对话特别有益？")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("减轻灾难性遗忘")]),_._v("：多轮对话数据通常远少于预训练数据。PEFT 能更好地保留模型在预训练阶段获得的世界知识和语言能力，防止其因过度专注于学习对话结构而遗忘根本。")]),_._v(" "),v("li",[v("strong",[_._v("高效且成本低")]),_._v("：可以在消费级GPU上微调大模型。")]),_._v(" "),v("li",[v("strong",[_._v("模块化")]),_._v("：可以为不同的对话风格或任务训练不同的 LoRA 适配器，灵活切换。")])]),_._v(" "),v("h2",{attrs:{id:"阶段四"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段四"}},[_._v("#")]),_._v(" 阶段四")]),_._v(" "),v("ol",[v("li",[_._v("请简述下PPO算法。")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("它是一种近端策略优化，核心思想是：避免策略更新幅度过大，不要一次改变太多。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("工作流程")]),_._v("：")]),_._v(" "),v("ul",[v("li",[_._v("用当前策略与环境交互，收集数据（状态、动作、奖励）。")]),_._v(" "),v("li",[_._v("利用收集的数据，根据“新/旧”策略的概率比和优势函数（衡量动作好坏）计算目标。")]),_._v(" "),v("li",[_._v("优化时，通过裁剪概率比，限制更新幅度，防止模型训歪。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("价值函数")]),_._v("：用于计算优势函数，评估状态或动作的相对价值，指导策略应向哪些方向更新。")])]),_._v(" "),v("li",[v("p",[_._v("四个主要模型：")]),_._v(" "),v("ul",[v("li",[_._v("Actor Model（目标语言模型）")]),_._v(" "),v("li",[_._v("Critic Model（预估未来期望收益Vt）")]),_._v(" "),v("li",[_._v("Reward Model（计算即时收益Rt）")]),_._v(" "),v("li",[_._v("Reference Model（约束语言模型行为）  ，参考模型")])])]),_._v(" "),v("li",[v("p",[_._v("模型约束，会加入："),v("code",[_._v("KL散度惩罚")]),_._v(" 这个惩罚项")])]),_._v(" "),v("li",[v("p",[_._v("在训练过程中新旧策略的的变化差异如果过大则不利于学习。")])])]),_._v(" "),v("ol",[v("li",[_._v("介绍一下基于人类反馈的强化学习流程")])]),_._v(" "),v("ul",[v("li",[_._v("构造用于RL的数据集，")]),_._v(" "),v("li",[_._v("采用DPO(直接偏好学习优化)")]),_._v(" "),v("li",[_._v("PPO  或者  GRPO 算法 去不断优化模型行为")])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("奖励模型的数据收集要满足什么要求?")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("一致性")]),_._v("：标注人员对相同或相似的回答，给出的评分标准必须一致。")]),_._v(" "),v("li",[v("strong",[_._v("覆盖性")]),_._v("：数据要覆盖模型可能遇到的各种输入（包括刁钻、古怪的提问）和各种类型的回答（优秀、一般、有害、胡言乱语等）。")]),_._v(" "),v("li",[v("strong",[_._v("高质量")]),_._v("：标注结果必须准确反映人类的真实偏好，避免噪音和错误标签。这需要清晰的标注指南和合格的标注员。")]),_._v(" "),v("li",[v("strong",[_._v("无偏见")]),_._v("：要尽量避免数据集中包含标注者个人的强烈偏见或共同的社会偏见。")])]),_._v(" "),v("ol",{attrs:{start:"4"}},[v("li",[_._v("奖励模型是如何训练的, 它的损失函数是什么?")])]),_._v(" "),v("p",[v("strong",[_._v("奖励模型的训练通常基于人类偏好数据，通过优化损失函数来学习对生成内容的评分能力。")])]),_._v(" "),v("p",[_._v("其核心流程和损失函数设计如下：")]),_._v(" "),v("p",[_._v("一、训练流程")]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("数据准备")]),v("br"),_._v("\n首先通过监督微调（SFT）的语言模型生成多个候选响应。例如，对每个提示（prompt）生成4-9个不同的回答，然后由人类评估者根据质量、相关性等标准进行排序，形成成对比较数据（如“响应A优于响应B”）。\n这些数据以三元组形式存储："),v("code",[_._v("(prompt, better_response, worse_response)")]),_._v("。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("模型架构")]),v("br"),_._v("\n奖励模型通常基于SFT模型改造，将原模型的下一词分类层替换为回归层，输出单个标量奖励值。例如，Transformer架构的奖励模型会将最后一层隐藏状态通过线性层映射为标量。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("训练过程")]),v("br"),_._v("\n使用成对比较数据进行训练，目标是使模型对人类偏好的响应（better_response）的评分显著高于非偏好响应（worse_response）。训练时，将prompt与响应拼接后输入模型，计算两者的奖励差值，并通过损失函数反向传播更新参数。")])])]),_._v(" "),v("p",[_._v("二、损失函数类型\n#1. "),v("strong",[_._v("成对排序损失（Pairwise Ranking Loss）")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("核心思想")]),_._v("：确保偏好响应的奖励值严格高于非偏好响应。")]),_._v(" "),v("li",[v("strong",[_._v("公式")]),_._v("："),v("br"),_._v("\n[\n\\text{loss} = -\\frac{1}{\\binom{K}{2}} \\sum_{(y_w, y_l) \\in \\text{pair}} \\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))\n]\n其中，(K)为每个prompt对应的响应数量，(\\sigma)为sigmoid函数，(y_w)和(y_l)分别表示排序高和低的响应。")]),_._v(" "),v("li",[v("strong",[_._v("实现细节")]),_._v("："),v("br"),_._v("\n通常使用"),v("code",[_._v("BCEWithLogitsLoss")]),_._v("（带logits的二分类交叉熵），将奖励差值作为logits，目标标签设为1（表示偏好关系成立）。例如："),v("div",{staticClass:"language-python extra-class"},[v("pre",{pre:!0,attrs:{class:"language-python"}},[v("code",[_._v("logits "),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),_._v(" r_better "),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("-")]),_._v(" r_worse\nloss "),v("span",{pre:!0,attrs:{class:"token operator"}},[_._v("=")]),_._v(" criterion"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("(")]),_._v("logits"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(",")]),_._v(" torch"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(".")]),_._v("ones_like"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v("(")]),_._v("logits"),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(")")]),v("span",{pre:!0,attrs:{class:"token punctuation"}},[_._v(")")]),_._v("\n")])])]),_._v("这种方法将排序问题转化为二分类问题，计算高效且稳定。")])]),_._v(" "),v("p",[_._v("#2. "),v("strong",[_._v("Bradley-Terry模型")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("概率建模")]),_._v("：假设人类选择响应A而非B的概率为："),v("br"),_._v("\n[\nP(y_w > y_l) = \\frac{\\exp(r_\\theta(x, y_w))}{\\exp(r_\\theta(x, y_w)) + \\exp(r_\\theta(x, y_l))}\n]")]),_._v(" "),v("li",[v("strong",[_._v("损失函数")]),_._v("：最大化数据的对数似然，等价于最小化负对数似然（NLL）："),v("br"),_._v("\n[\n\\text{loss} = -\\sum_{(y_w, y_l)} \\log P(y_w > y_l)\n]\n该模型直接建模偏好概率，适用于成对比较数据。")])]),_._v(" "),v("p",[_._v("#3. "),v("strong",[_._v("Margin Loss（带偏好强度的损失）")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("动机")]),_._v("：区分“显著好”和“略好”的偏好强度。")]),_._v(" "),v("li",[v("strong",[_._v("公式")]),_._v("："),v("br"),_._v("\n[\n\\text{loss} = -\\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l) - m)\n]\n其中，(m)为人工标注的偏好强度（如0.1表示“略好”，0.5表示“显著好”）。当(r_\\theta(x, y_w) - r_\\theta(x, y_l))不足以超过(m)时，损失增大，迫使模型学习更显著的差异。")]),_._v(" "),v("li",[v("strong",[_._v("应用场景")]),_._v("：Llama 2等模型采用此方法，通过动态调整(m)来强化梯度信号，提升模型对偏好强度的捕捉能力。")])]),_._v(" "),v("p",[_._v("#4. "),v("strong",[_._v("KL散度约束")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("作用")]),_._v("：在强化学习阶段（如PPO），奖励模型的输出需与策略模型的行为保持一致。此时会引入KL散度惩罚项："),v("br"),_._v("\n[\n\\text{loss}"),v("em",[_._v("{\\text{kl}} = \\beta \\cdot \\text{KL}(\\pi")]),_._v("{\\phi_{\\text{RL}}}(y|x) | \\pi_{\\text{SFT}}(y|x))\n]\n防止策略模型过度偏离SFT模型的初始分布，避免训练不稳定。")])]),_._v(" "),v("p",[_._v("三、关键优化策略")]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("数据效率")]),v("br"),_._v("\n当每个prompt生成9个响应时，可通过组合生成36对比较数据，同时复用已计算的奖励值，减少计算量。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("模型初始化")]),v("br"),_._v("\n奖励模型常基于SFT模型初始化，利用其已学习的语言理解能力，加速收敛。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("动态调整")]),v("br"),_._v("\n在训练过程中动态调整学习率或Margin参数，平衡模型对不同偏好强度的学习速度。")])])]),_._v(" "),v("p",[_._v("四、应用场景与挑战")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("场景")]),_._v("：广泛用于大语言模型的对齐优化（如GPT-4、Llama 2）、推荐系统排序、对话生成等需人类偏好的任务。")]),_._v(" "),v("li",[v("strong",[_._v("挑战")]),_._v("：\n"),v("ul",[v("li",[v("strong",[_._v("数据标注成本高")]),_._v("：需大量人工排序数据，且标注一致性难以保证。")]),_._v(" "),v("li",[v("strong",[_._v("偏好复杂性")]),_._v("：人类偏好可能涉及多维度（如安全性、创造性），单一奖励模型难以全面捕捉。")]),_._v(" "),v("li",[v("strong",[_._v("泛化能力")]),_._v("：训练好的奖励模型在未见过的领域可能失效，需结合领域自适应技术。")])])])]),_._v(" "),v("p",[_._v("通过上述方法，奖励模型能够将人类偏好转化为可计算的奖励信号，为强化学习提供有效指导，从而提升模型生成内容的质量和一致性。")]),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[_._v("目前RLHF方法有没有什么缺陷?如何改进?")])]),_._v(" "),v("p",[_._v("RLHF（基于人类反馈的强化学习）方法存在一些缺陷，针对这些缺陷也有相应的改进措施，具体如下：")]),_._v(" "),v("p",[_._v("RLHF方法的缺陷")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("缺乏真正的强化学习特征")]),_._v("：RLHF缺乏RL的核心特征，如持续的环境交互和长期目标的追求。它主要通过单步或几步优化来调整模型输出，而不是在动态环境中进行多步骤的策略调整，且通常是离线或半离线进行的，缺乏实时的环境反馈和策略更新。")]),_._v(" "),v("li",[v("strong",[_._v("训练过程复杂且成本高")]),_._v("：RLHF涉及多个步骤，包括奖励模型训练和RL微调，计算成本高且步骤繁琐。它需要反复从策略模型采样生成回答，这也增加了计算开销。")]),_._v(" "),v("li",[v("strong",[_._v("训练稳定性差")]),_._v("：RLHF对超参数敏感，如KL惩罚系数等，容易出现梯度爆炸或策略崩溃等问题，导致训练不稳定，可能出现奖励崩溃或过度优化的情况。")]),_._v(" "),v("li",[v("strong",[_._v("依赖奖励模型")]),_._v("：奖励模型的质量直接影响RL微调的效果，但其可能无法完美拟合人类偏好，存在偏差，从而导致策略优化偏离真实目标，使模型行为失调。")]),_._v(" "),v("li",[v("strong",[_._v("数据质量问题")]),_._v("：RLHF高度依赖人工反馈获取高质量标注数据，人力与时间成本巨大。同时，标注过程中评估者的主观性和不一致性会产生标注偏差，误导生成模型，且数据量不足时，奖励模型难以全面捕捉人类偏好细节。")])]),_._v(" "),v("ol",{attrs:{start:"6"}},[v("li",[_._v("介绍一下LLM的直接偏好优化(DPO)")])]),_._v(" "),v("ul",[v("li",[_._v("是一种，通过正负样本进行学习的方法")]),_._v(" "),v("li",[_._v("DPO本质上是在"),v("strong",[_._v("最大化正样本")]),_._v("的奖励, 并最小化负样本的奖励")]),_._v(" "),v("li",[_._v("其"),v("strong",[_._v("核心技巧")]),_._v("是：通过一个数学推导，将奖励函数的最大化问题，转化为一个"),v("strong",[_._v("纯粹的策略损失函数")]),_._v("，从而可以直接通过监督学习的方式优化模型。")])]),_._v(" "),v("ol",{attrs:{start:"7"}},[v("li",[_._v("LLM训练中, 近端策略优化包含哪几个模型?")])]),_._v(" "),v("ul",[v("li",[_._v("Reference Model(参考模型) 、奖励模型、价值模型、Critic 模型、Actor模型(生成回答)")])]),_._v(" "),v("ol",{attrs:{start:"8"}},[v("li",[_._v("什么是RLAIF?")])]),_._v(" "),v("blockquote",[v("p",[_._v("基于AI反馈的强化学习")])]),_._v(" "),v("ul",[v("li",[_._v("使用"),v("strong",[_._v("AI模型")]),_._v("（通常是预训练的语言奖励模型）来提供反馈，而不直接依赖人类的人工标注。")]),_._v(" "),v("li",[_._v("这里的“AI”也可以是某些规则奖励，例如数学答案/代码解释器...")])]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",[_._v("类型")]),_._v(" "),v("th",[_._v("裁判")]),_._v(" "),v("th",[_._v("优点")]),_._v(" "),v("th",[_._v("缺点")])])]),_._v(" "),v("tbody",[v("tr",[v("td",[_._v("RLHF")]),_._v(" "),v("td",[_._v("人类")]),_._v(" "),v("td",[_._v("更贴近真实人类偏好")]),_._v(" "),v("td",[_._v("成本高、效率低")])]),_._v(" "),v("tr",[v("td",[_._v("RLAIF")]),_._v(" "),v("td",[_._v("模型")]),_._v(" "),v("td",[_._v("自动化、可扩展性强")]),_._v(" "),v("td",[_._v("可能偏离人类真实偏好")])])])]),_._v(" "),v("p",[_._v("RLAIF的最大优势在于可扩展性和On-Policy的特点——不需要昂贵的人工标注，可以生成海量的训练样本，让模型在在线大量试错中快速进化。")]),_._v(" "),v("p",[_._v("本质上，都是通过"),v("strong",[_._v("强化学习的方式")]),_._v('，利用某种形式的"'),v("strong",[_._v("反馈")]),_._v('"来优化模型的行为。')]),_._v(" "),v("ol",{attrs:{start:"9"}},[v("li",[_._v("与有监督学习相比, 强化学习能够给大语言模型带来哪些好处?")])]),_._v(" "),v("ul",[v("li",[_._v("回答符合人类偏好的答案")])]),_._v(" "),v("ol",{attrs:{start:"10"}},[v("li",[_._v("介绍一下RLHF中PPO微调过程?")])]),_._v(" "),v("ul",[v("li")]),_._v(" "),v("ol",{attrs:{start:"11"}},[v("li",[_._v("你了解DeepMind提出的ReST对齐算法吗?")])]),_._v(" "),v("h2",{attrs:{id:"阶段五"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段五"}},[_._v("#")]),_._v(" 阶段五")]),_._v(" "),v("ol",[v("li",[_._v("说说你知道的大模型训练or推理的常用优化手段")])]),_._v(" "),v("ul",[v("li",[_._v("KV Cache")]),_._v(" "),v("li",[_._v("静态批处理")]),_._v(" "),v("li",[_._v("动态批处理")]),_._v(" "),v("li",[_._v("连续批处理")])]),_._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[v("p",[_._v("一般会对哪些大模型里面的算子做算子融合,说说你知道的")])]),_._v(" "),v("li",[v("p",[_._v("什么是KV Cache技术, 它具体是如何实现的?")])])]),_._v(" "),v("ul",[v("li",[_._v("在注意力计算的时候，对矩阵里面的K 和 V 进行缓存，")]),_._v(" "),v("li",[_._v("将生成的前面的标记得到KV，进行缓存，在计算下一个标记的时候，直接拿缓存的值，不用去计算之前的值了")])]),_._v(" "),v("p",[v("strong",[_._v("先前标记的这些K和V的值可以被缓存")]),_._v(", 这样我们就不必在每次调用这个计算的时候重新计算它们, 然后只需要进行一个非常轻量级的连接操作,\n在这个操作中"),v("strong",[_._v("我们获取缓存的值, 附加新标记的新K和V值,进行注意力计算,重新缓存这些值,然后生成下一个标记。")])]),_._v(" "),v("ol",{attrs:{start:"4"}},[v("li",[_._v("Paged Attention的原理是什么,它解决了大模型推理中的什么问题?")])]),_._v(" "),v("ul",[v("li",[_._v("核心的一点：把碎片化的显存利用起来，同时把显存的利用率，给利用起来，")]),_._v(" "),v("li",[_._v("具体做法，就是把物理上不连续的片，通过内存分页的形式，在逻辑上把它做成一个连续的，这就是 Paged Attention 的一个主要的做法，包括它里面的一些Block 的设置呀，一般是取16\n它的一些设置呀，包括对显存的一些预留，预留分片，进行中的分片\n都是为了节省显存、提升显存的利用率而做的")])]),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[v("p",[_._v("DeepSpeed推理对算子融合做了哪些优化?")])]),_._v(" "),v("li",[v("p",[_._v("FlashAttention的空间复杂度和对HBM的访问次数是多少?")])]),_._v(" "),v("li",[v("p",[_._v("FlashDecoding在FlashAttention2上做了哪些改进?")])]),_._v(" "),v("li",[v("p",[_._v("FlashDecoding++做了什么优化?")])]),_._v(" "),v("li",[v("p",[_._v("什么是子图融合优化技术,为什么他可以提升推理速度?")])]),_._v(" "),v("li",[v("p",[_._v("MHA,GQA,MQA推理优化技术的区别是什么?")])]),_._v(" "),v("li",[v("p",[_._v("Paged Attention 是如何有效管理具有分页的KV缓存的?")])])]),_._v(" "),v("ul",[v("li",[_._v("可以理解为问的，具有分页的内存管理")])]),_._v(" "),v("p",[_._v("Paged Attention 通过借鉴操作系统的虚拟内存分页技术，将 KV 缓存的管理从「连续内存预分配」重构为「动态分块与非连续存储」，其核心机制包括以下四个层面：")]),_._v(" "),v("ol",{attrs:{start:"12"}},[v("li",[_._v("介绍一下动态批处理技术?")])]),_._v(" "),v("ul",[v("li",[_._v("当一个推理实例（如GPU）空闲时，它不会等待一个固定的批次，而是"),v("strong",[_._v("实时地从请求队列中取出尽可能多的请求，组成一个新批次")]),_._v("进行处理。批次大小 "),v("code",[_._v("batch_size")]),_._v(" 不再是固定的，而是根据当前队列长度动态变化的。")]),_._v(" "),v("li",[_._v("但是一个批次内，"),v("strong",[_._v("如果某个请求需要生成的输出很长")]),_._v("（比如1000个token），而其他请求很短（比如10个token），那么生成长文本的请求会拖累整个批次，导致短请求也要等待很久才能返回。")])]),_._v(" "),v("ol",[v("li",[v("p",[_._v("请问什么是猜测推理技术? 请举例说明")])]),_._v(" "),v("li",[v("p",[_._v("什么是continuous batching技术,为什么他的效率比动态batching效率高?")])])]),_._v(" "),v("ul",[v("li",[_._v("连续批处理技术，它是做了迭代的Token级调度，一个Batch 完成，就在其位置插入一个新的，从而实现一个比静态批处理更高的一个GPU的利用率")]),_._v(" "),v("li",[_._v("可以将continuous batching 类比为一列列车，token 是列车上的人，可以随时上车下车")]),_._v(" "),v("li",[_._v("如果乘客是比较密集的，列车可以始终达到一个满员的状态，可以极大地提高GPU的利用率")])]),_._v(" "),v("p",[_._v("动态批处理，一个批次里处理多个请求，但是遇到生成多个标记的请求， 其他请求需要等待这个多标记生成的请求，先完成")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("优点")]),_._v("：\n"),v("ul",[v("li",[v("strong",[_._v("彻底解决了“木桶效应”")]),_._v("：短请求可以快速完成并返回，不会被长请求阻塞。")]),_._v(" "),v("li",[v("strong",[_._v("极高的GPU利用率")]),_._v("：GPU几乎在每个时刻都在处理满负荷的计算，因为新的请求可以立即填补已完成请求留下的空位。")]),_._v(" "),v("li",[v("strong",[_._v("显著降低延迟")]),_._v("：尤其是对于短文本交互场景，用户体验大幅提升。")])])])]),_._v(" "),v("ol",{attrs:{start:"15"}},[v("li",[_._v("优化CUDA程序的访存效率, 你可以想到哪些?")]),_._v(" "),v("li",[_._v("优化CUDA程序的计算效率, 你又可以想到哪些?")])]),_._v(" "),v("h2",{attrs:{id:"阶段六"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#阶段六"}},[_._v("#")]),_._v(" 阶段六")]),_._v(" "),v("ol",[v("li",[_._v("ViT的大致原理")])]),_._v(" "),v("blockquote",[v("p",[_._v("ViT 将图像分割成n个patches，转化为Embedding数据后，"),v("strong",[_._v("通过 Transformer 编码器进行全局特征学习")]),_._v("，最终完成分类等任务。")])]),_._v(" "),v("p",[_._v("具体的流程：")]),_._v(" "),v("ol",[v("li",[v("p",[_._v("首先将图片分成一个个patches")])]),_._v(" "),v("li",[v("p",[_._v("然后通过embedding层得到对应的token")])]),_._v(" "),v("li",[v("p",[_._v("然后再经过 Transformer encoder， 在输入Transformer Encoder之前需要加上[class]token，以及Position Embedding")])]),_._v(" "),v("li",[v("p",[_._v("然后再通过MLP Head 得到最终的一个输出")])]),_._v(" "),v("li",[v("p",[_._v("说一下CLIP模型是如何做图文对齐的")])])]),_._v(" "),v("blockquote",[v("p",[_._v("CLIP 采用双编码器结构，分别将图像和文本转换成特征向量，")])]),_._v(" "),v("ul",[v("li",[_._v("文字Encoder")]),_._v(" "),v("li",[_._v("图片Encoder")])]),_._v(" "),v("p",[_._v("CLIP， 输入的是两路： 图片、文字, 将"),v("strong",[_._v("图片和文字编码")]),_._v("， 然后将文字编码后的结果"),v("strong",[_._v("转秩")]),_._v("一下，以便和 "),v("strong",[_._v("图片编码后得到的向量")]),_._v(", "),v("strong",[_._v("做点积")]),_._v("计算")]),_._v(" "),v("ul",[v("li",[_._v("输入的文字，要进行 Tokenizer 分词，Embedding")]),_._v(" "),v("li",[_._v("image_embedding 和 text_embedding  做点积")])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("介绍一下stable diffusion的原理")])]),_._v(" "),v("blockquote",[v("p",[_._v("stable diffusion是：以文本为条件的扩散模型")])]),_._v(" "),v("ul",[v("li",[_._v("输入一个提示词，使用Text Encoder 编码器， 输出出来Embedding， 然后给 VAE 解码器，解码，输出图像")])]),_._v(" "),v("ol",{attrs:{start:"4"}},[v("li",[_._v("Qwen-VL的三个训练流程分别是什么, 有什么作用")])]),_._v(" "),v("ul",[v("li",[_._v("预训练\n"),v("ul",[v("li",[_._v("核心作用： 赋予模型基础的视觉-语言关联与理解能力。")])])]),_._v(" "),v("li",[_._v("SFT\n"),v("ul",[v("li",[_._v("核心作用： 激发和塑造模型的特定能力，使其能遵循指令并完成复杂任务。")])])]),_._v(" "),v("li",[_._v("强化学习\n"),v("ul",[v("li",[_._v("GRPO 训练 or DPO")]),_._v(" "),v("li",[_._v("核心作用： 优化模型的输出偏好，生成符合人类偏好的回答（更有用、诚实、无害）。")])])])]),_._v(" "),v("ol",{attrs:{start:"5"}},[v("li",[_._v("目前的文生图、图生图的方法有哪些?")])]),_._v(" "),v("ul",[v("li",[_._v("Google Imagen")]),_._v(" "),v("li",[_._v("DALL-E")]),_._v(" "),v("li",[_._v("VQGAN")]),_._v(" "),v("li",[_._v("艺术创作型：Midjourney v6.1")])]),_._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[_._v("技术方向")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("核心功能")]),_._v(" "),v("th",{staticStyle:{"text-align":"left"}},[_._v("代表性模型/工具")])])]),_._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("文生图 (Text-to-Image)")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("根据文本描述生成图像")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("Stable Diffusion, DALL·E, Imagen, Nano Banana Pro")])]),_._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[_._v("图生图 (Image-to-Image)")])]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("以输入图像为参考进行生成或编辑")]),_._v(" "),v("td",{staticStyle:{"text-align":"left"}},[_._v("Qwen-VLo, Nano Banana Pro 的图像编辑功能")])])])]),_._v(" "),v("ol",{attrs:{start:"6"}},[v("li",[_._v("多模态大模型如何处理视频输入?")])]),_._v(" "),v("p",[_._v("与图像相比，处理视频的难点在于：")]),_._v(" "),v("p",[_._v("海量数据：一秒30帧的1080p视频，数据量是单张图像的30倍。直接处理所有帧计算成本无法承受。")]),_._v(" "),v("p",[_._v("时间冗余：连续帧之间内容高度相似，存在大量冗余信息。")]),_._v(" "),v("p",[_._v("时序建模：模型必须理解帧与帧之间的时序关系，才能捕捉动作、因果和故事线。")]),_._v(" "),v("p",[_._v("计算瓶颈：无论是视觉编码还是LLM理解，处理长序列视频 token 对硬件要求极高。")]),_._v(" "),v("p",[_._v("主流技术路线:")]),_._v(" "),v("h4",{attrs:{id:"路线一-稀疏采样-图像编码器-主流且实用"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#路线一-稀疏采样-图像编码器-主流且实用"}},[_._v("#")]),_._v(" 路线一：稀疏采样 + 图像编码器（主流且实用）")]),_._v(" "),v("p",[_._v("这是目前最流行、最实用的方法，许多开源模型（如 "),v("strong",[_._v("Video-LLaVA")]),_._v("、"),v("strong",[_._v("LLaVA-NeXT")]),_._v("）都采用此路线。")]),_._v(" "),v("ul",[v("li",[v("p",[v("strong",[_._v("思路")]),_._v("：既然视频帧冗余度高，那就不全部处理，只"),v("strong",[_._v("智能地选取少量关键帧")]),_._v("。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("具体做法")]),_._v("：")]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("均匀采样")]),_._v("：最简单的方法，每隔N帧取一帧。")]),_._v(" "),v("li",[v("strong",[_._v("稀疏采样")]),_._v("：使用更复杂的策略（如场景变化检测）来选取信息量最大的帧。")]),_._v(" "),v("li",[v("strong",[_._v("处理")]),_._v("：将选取的每一帧都视为独立的图像，通过一个"),v("strong",[_._v("预训练的图像编码器")]),_._v("（如 "),v("strong",[_._v("CLIP的ViT")]),_._v("）进行编码，得到每个帧的视觉特征。")]),_._v(" "),v("li",[v("strong",[_._v("输入LLM")]),_._v("：将这些帧的视觉特征序列（可能加上一个可学习的帧位置嵌入）与文本指令一起输入给大语言模型。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("优点")]),_._v("：")]),_._v(" "),v("ul",[v("li",[_._v("充分利用了成熟的、强大的图像编码器（如CLIP）。")]),_._v(" "),v("li",[_._v("计算高效，实现相对简单。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("缺点")]),_._v("：")]),_._v(" "),v("ul",[v("li",[_._v("可能会丢失一些快速变化或细微的时序信息。")]),_._v(" "),v("li",[_._v("对长时间视频的支持仍然有限。")])])])]),_._v(" "),v("ol",{attrs:{start:"7"}},[v("li",[_._v("ROPE位置编码如何扩展到多模态大模型?")])]),_._v(" "),v("ul",[v("li",[_._v("ROPE（Rotary Position Embedding，旋转位置编码）最初是为Transformer模型（如LLaMA、GPT-J）在NLP任务中设计的，"),v("strong",[_._v("但它可以非常优雅地扩展到多模态大模型中")]),_._v("。")])]),_._v(" "),v("p",[_._v("其核心思想是："),v("strong",[_._v("将不同模态（如文本、图像、音频）的元素统一到一个共享的表示空间中，并在该空间中应用具有模态感知能力的旋转位置编码。")])]),_._v(" "),v("p",[_._v("ROPE通过其"),v("strong",[_._v("可分解的旋转操作")]),_._v("和"),v("strong",[_._v("对绝对与相对位置的优雅处理")]),_._v("，使其能够非常灵活地扩展到多模态领域。")]),_._v(" "),v("p",[_._v("通过"),v("strong",[_._v("为不同模态分配不同的旋转基频")]),_._v("，"),v("strong",[_._v("ROPE成功地让多模态大模型能够理解和区分来自不同模态的元素的位置上下文")]),_._v("，这是构建高效、高性能多模态理解系统的关键一环。")]),_._v(" "),v("ol",{attrs:{start:"8"}},[v("li",[_._v("了解MM-RLHF吗, 是怎么做的?")])]),_._v(" "),v("p",[_._v("简单来说，它是 "),v("code",[_._v("RLHF")]),_._v(" 在多模态领域的扩展和升级。")]),_._v(" "),v("p",[_._v("RLHF 是让 ChatGPT 等大语言模型变得如此“听话”和“有用”的核心技术，而 MM-RLHF 的目标是让能够理解和生成"),v("strong",[_._v("图像、视频、音频")]),_._v("等多模态内容的模型也具备同样的能力。")]),_._v(" "),v("blockquote",[v("p",[_._v("核心思想")])]),_._v(" "),v("p",[_._v("MM-RLHF 的核心思想与 "),v("code",[_._v("RLHF")]),_._v(" 一脉相承：我们不直接告诉模型什么是“好”的答案，而是通过人类对模型不同输出的偏好（喜欢哪个、不喜欢哪个）来间接地“教会”模型我们的价值观和标准。")]),_._v(" "),v("blockquote",[v("p",[_._v("怎么做的?")])]),_._v(" "),v("ul",[v("li",[_._v("阶段一：监督微调\n"),v("ul",[v("li",[_._v("选择基础模型")]),_._v(" "),v("li",[_._v("微调需要的数据，收集一个高质量的、由人类精心编写的“指令-回答”对数据集。")])])]),_._v(" "),v("li",[_._v("阶段二：训练奖励模型\n"),v("ul",[v("li",[_._v("数据收集：")]),_._v(" "),v("li",[_._v("模型训练:")])])]),_._v(" "),v("li",[_._v("阶段三：强化学习优化")])]),_._v(" "),v("p",[_._v("与纯文本的 RLHF 相比，MM-RLHF 更加复杂：")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("偏好标注更难、更主观")]),_._v("：判断两张图片哪张更好，可能比判断两段文字更难，因为涉及更多美学、风格等主观因素。标注成本极高。")]),_._v(" "),v("li",[v("strong",[_._v("奖励模型更难训练")]),_._v("：捕捉图像质量、艺术风格、与文本的一致性等细微差别，比判断文本质量更具挑战性。")]),_._v(" "),v("li",[v("strong",[_._v("生成过程的复杂性")]),_._v("：文生图等模型的生成过程是高度非自回归的，行动空间（像素或潜在空间）巨大且连续，这给强化学习优化带来了巨大困难。")]),_._v(" "),v("li",[v("strong",[_._v("评估困难")]),_._v("：如何量化评估生成模型是否“对齐”了，本身就是一个开放性问题。")])]),_._v(" "),v("ol",{attrs:{start:"9"}},[v("li",[_._v("比较Q-Former和MLP的对齐效果, 为什么现在模型都用MLP而不用Q-Former?")])]),_._v(" "),v("ul",[v("li",[_._v("Q-Former 是什么，核心结构呀")]),_._v(" "),v("li",[_._v("MLP 是什么")])]),_._v(" "),v("blockquote",[v("p",[_._v("MLP（通常是一个简单的投影层）")])]),_._v(" "),v("p",[_._v("对两者进行一个比较，")]),_._v(" "),v("blockquote",[v("p",[_._v("Q-Former VS MLP")])]),_._v(" "),v("p",[_._v("MLP 设计目标： 在计算资源相对充足的情况下，"),v("strong",[_._v("实现图像特征到文本特征空间的直接、无缝映射。")])]),_._v(" "),v("p",[_._v("核心结构：通常就是一个或多个线性层，有时会加入非线性激活函数（如GELU）。")]),_._v(" "),v("ol",[v("li",[_._v("MLP就是简单的矩阵乘法，计算速度极快。Q-Former包含交叉注意力层，在推理时会引入额外的计算开销。")]),_._v(" "),v("li",[_._v("MLP的参数量远小于Q-Former。在一个动辄70亿、130亿参数的模型中，增加一个几十万或几百万参数的MLP几乎可以忽略不计，而Q-Former则会引入数千万的额外参数。")])]),_._v(" "),v("ul",[v("li",[_._v("总结")])]),_._v(" "),v("ol",[v("li",[_._v("MLP 推理速度快、参数量小")]),_._v(" "),v("li",[_._v("MLP结构简单，易于实现和调试")]),_._v(" "),v("li",[_._v("与LLM接口的统一："),v("strong",[_._v("现代LLM本质上是接收一个“词嵌入序列”并预测下一个词。")]),_._v(" "),v("ul",[v("li",[_._v("MLP的输出可以直接被视为一组“视觉词”嵌入，与文本词嵌入无缝拼接。")]),_._v(" "),v("li",[_._v("这种接口非常自然和统一。")]),_._v(" "),v("li",[_._v("虽然Q-Former也输出一个序列，但其内部机制更为复杂。")])])])]),_._v(" "),v("p",[_._v("还有时代背景： Q-Former是算力受限时期的巧妙设计， MLP： 算力充裕、端到端训练成为主流")]),_._v(" "),v("ol",{attrs:{start:"10"}},[v("li",[_._v("对于大模型复杂推理任务, 有哪些主流方法?")])]),_._v(" "),v("ul",[v("li",[v("ol",[v("li",[_._v("COT")])])]),_._v(" "),v("li",[v("ol",{attrs:{start:"2"}},[v("li",[_._v("自我迭代和优化方法")])])]),_._v(" "),v("li",[v("ol",{attrs:{start:"3"}},[v("li",[_._v("工具增强，让模型学会调用外部工具")])])]),_._v(" "),v("li",[v("ol",{attrs:{start:"4"}},[v("li",[_._v("多智能体协作 - 让模型“团队作战”")])]),_._v(" "),v("ul",[v("li",[_._v("代表性框架： AutoGPT, ChatDev, MetaGPT, CrewAI等。")])])]),_._v(" "),v("li",[v("ol",{attrs:{start:"5"}},[v("li",[_._v("图推理与结构化提示\n对于涉及复杂关系、状态变化或逻辑约束的问题，将问题转化为结构化的形式。")])])])]),_._v(" "),v("ol",{attrs:{start:"11"}},[v("li",[_._v("多模态的GRPO怎么做, 讲一下VLM-R1的核心思想和流程")])]),_._v(" "),v("p",[_._v("关于多模态GRPO，可以从三个层面来概括：")]),_._v(" "),v("ol",[v("li",[v("p",[v("strong",[_._v("定位：")]),_._v(" 它是一种"),v("strong",[_._v("无需训练奖励模型")]),_._v("的强化学习方法，直接用来"),v("strong",[_._v("对齐和优化多模态模型")]),_._v("的输出，比如让文生图模型生成更符合人类审美的图片。")])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("核心流程：")]),_._v(" 它的工作流程可以概括为 "),v("strong",[_._v("“采样-评分-优化”")]),_._v(" 三步循环：")]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("采样：")]),_._v(" 让当前模型为一批提示生成大量输出（如图片）。")]),_._v(" "),v("li",[v("strong",[_._v("评分：")]),_._v(" 用一个"),v("strong",[_._v("预定义的、不可微的奖励函数")]),_._v("（比如一个图像审美打分模型）直接为这些输出打分。")]),_._v(" "),v("li",[v("strong",[_._v("优化：")]),_._v(" 使用类似PPO的策略梯度算法，"),v("strong",[_._v("朝着高分方向更新模型，同时约束模型不偏离原始能力太远")]),_._v("。")])])]),_._v(" "),v("li",[v("p",[v("strong",[_._v("价值与趋势：")])]),_._v(" "),v("ul",[v("li",[v("strong",[_._v("它的最大优势是流程简化，避免了训练奖励模型的成本和复杂性。")])]),_._v(" "),v("li",[_._v("在当前实践中，与其思想一脉相承但更稳定、更流行的方法是 "),v("strong",[_._v("DPO")]),_._v("，它已经成为多模态对齐领域的主流技术之一。")])])])]),_._v(" "),v("p",[_._v("总结来说，多模态GRPO/DPO的核心思想，就是"),v("strong",[_._v("用直接的偏好信号作为“罗盘”，来高效地引导大模型朝着我们期望的方向进化。")])]),_._v(" "),v("blockquote",[v("p",[_._v("另外一个点：讲一下VLM-R1的核心思想和流程")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("讲一下VLM-R1的核心思想和流程？")])]),_._v(" "),v("li",[v("p",[_._v("输入一个提示，就可以精准定位图片里的目标的一种手段")])])]),_._v(" "),v("p",[_._v("VLM-R1 是一款基于强化学习技术的视觉语言模型，能够通过自然语言指令精确定位图像目标，并支持多模态推理。")]),_._v(" "),v("ol",[v("li",[v("strong",[_._v("指代表达理解")]),_._v("：解析自然语言指令，精准定位图像中的特定目标。")]),_._v(" "),v("li",[v("strong",[_._v("强化学习优化")]),_._v("：采用 GRPO 技术，在复杂场景下表现出色，提升泛化能力。")])]),_._v(" "),v("p",[_._v("VLM-R1 提供了两种训练方法：GRPO 和 SFT。")]),_._v(" "),v("h2",{attrs:{id:"汇总"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#汇总"}},[_._v("#")]),_._v(" 汇总")]),_._v(" "),v("p",[_._v("vLLM 篇")]),_._v(" "),v("ol",[v("li",[_._v("你用过vLLM, 那你了不了解它的底层原理,讲一下Paged Attention?")])]),_._v(" "),v("ul",[v("li",[_._v("核心的一点："),v("strong",[_._v("把碎片化的显存利用起来，来提高显存的利用率")])]),_._v(" "),v("li",[_._v("具体做法，就是把物理上不连续的片，通过内存分页的形式，在逻辑上把它做成一个连续的，这就是 Paged Attention 的一个主要的做法，包括它里面的一些Block 的设置呀，一般是取16\n它的一些设置呀，包括对显存的一些预留，预留分片，进行中的分片\n都是为了节省显存、提升显存的利用率而做的")])]),_._v(" "),v("blockquote",[v("p",[_._v("Paged Attention如何高效管理kv 缓存的，")])]),_._v(" "),v("ul",[v("li",[v("p",[_._v("Paged Attention是VLLM中用于优化Transformer模型推理过程中KV Cache内存管理的关键技术。")])]),_._v(" "),v("li",[v("p",[_._v("其核心原理是: 参考了操作系统的内存分页技术，将KV Cache分块存储在"),v("code",[_._v("非连续的内存地址")]),_._v("中，以提高显存利用率，提升模型推理性能。")])])]),_._v(" "),v("ol",{attrs:{start:"2"}},[v("li",[_._v("KV Cache")])]),_._v(" "),v("p",[_._v("在语言模型生成文本的过程中，每生成一个新的 token，模型都需要注意力计算，以确定当前位置与之前所有位置的相关性.")]),_._v(" "),v("p",[_._v("不难发现，大模型生成一个 token 后的注意力计算中，总会用到 token 序列的历史 KV 值，导致重复计算，\nKV Cache 的设计正是为了通过缓存历史 KV 值，节省计算开销.")]),_._v(" "),v("p",[v("strong",[_._v("KV Cache 能够有效压缩大模型推理时的显存占用.")])]),_._v(" "),v("ol",{attrs:{start:"3"}},[v("li",[_._v("什么是continuous batching？\n"),v("ul",[v("li",[_._v("连续批处理")])])])]),_._v(" "),v("ul",[v("li",[_._v("它是做了迭代的Token级调度")]),_._v(" "),v("li",[_._v("Batch 的大小是根据每次迭代确定的")]),_._v(" "),v("li",[_._v("一个Batch 完成，就可以在其位置插入一个新的，从而实现一个比静态批处理更高的一个 gpu的利用率\n可以类比，Continuous Batching 就像一辆列车， Token就像列车上的人，可以随时上车、下车\n如果乘客是比较密集的，可以始终达到一个满员的状态，可以极大地提高GPU的利用率")])]),_._v(" "),v("p",[_._v("因此, 我们能够在最开始将它们添加到批处理中,然后在整个过程中继续添加和删除那些10个标记的较小序列。")]),_._v(" "),v("p",[_._v("然后那些落后的、那些100个标记序列最终在未尾完成而不会在整个过程中造成瓶颈。")]),_._v(" "),v("p",[_._v("这基本上就是连续批处理，能够为我们做到的事情。")]),_._v(" "),v("p",[_._v("而这最终不仅是提高LLM推断的吞吐量和延迟的关键驱动因素之一,还是您常见的LLM推断系统中看到的标记流式处理能力的关键,因为我们能够及时处理它们,并从系统中逐个获取标记,我们能够非常迅速地将这些结果返回给用户, 而不必等待这些大的批处理步骤。")])])}),[],!1,null,null,null);v.default=n.exports}}]);