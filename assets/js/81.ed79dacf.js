(window.webpackJsonp=window.webpackJsonp||[]).push([[81],{523:function(t,a,s){"use strict";s.r(a);var n=s(3),e=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("这是一个"),a("strong",[t._v("重要警告")]),t._v("，不是错误，但需要关注。")]),t._v(" "),a("h2",{attrs:{id:"警告含义"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#警告含义"}},[t._v("#")]),t._v(" 警告含义：")]),t._v(" "),a("ol",[a("li",[a("p",[a("strong",[t._v("注意力掩码未设置")]),t._v(" ("),a("code",[t._v("attention_mask")]),t._v(")")]),t._v(" "),a("ul",[a("li",[t._v("模型不知道哪些是真实文本，哪些是填充的padding")]),t._v(" "),a("li",[t._v("这可能导致模型关注padding部分，影响生成质量")])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("填充标记未设置")]),t._v(" ("),a("code",[t._v("pad_token_id")]),t._v(")")]),t._v(" "),a("ul",[a("li",[t._v("系统自动使用结束标记("),a("code",[t._v("eos_token_id")]),t._v(")作为填充标记")]),t._v(" "),a("li",[t._v("这可能在某些情况下造成混淆")])])])]),t._v(" "),a("h2",{attrs:{id:"解决方案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#解决方案"}},[t._v("#")]),t._v(" 解决方案：")]),t._v(" "),a("h3",{attrs:{id:"方法1-设置tokenizer的pad-token"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法1-设置tokenizer的pad-token"}},[t._v("#")]),t._v(" 方法1：设置tokenizer的pad_token")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoTokenizer\n\ntokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"your-model-name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果tokenizer没有pad_token，设置一个")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_token "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_token "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eos_token  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 或者 tokenizer.add_special_tokens({'pad_token': '[PAD]'})")]),t._v("\n")])])]),a("h3",{attrs:{id:"方法2-在datacollator中明确指定"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法2-在datacollator中明确指定"}},[t._v("#")]),t._v(" 方法2：在DataCollator中明确指定")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataCollatorForSeq2Seq\n\ndata_collator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataCollatorForSeq2Seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    tokenizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pad_to_multiple_of"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 可选，用于优化")]),t._v("\n    return_tensors"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pt"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"方法3-在生成时传递attention-mask"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#方法3-在生成时传递attention-mask"}},[t._v("#")]),t._v(" 方法3：在生成时传递attention_mask")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果在使用model.generate()")]),t._v("\noutputs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("generate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    input_ids"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("input_ids"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    attention_mask"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("attention_mask"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 确保传递attention_mask")]),t._v("\n    pad_token_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_token_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eos_token_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ... 其他参数")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"建议"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#建议"}},[t._v("#")]),t._v(" 建议：")]),t._v(" "),a("p",[a("strong",[t._v("强烈建议")]),t._v("在初始化tokenizer后立即设置pad_token，这样可以避免后续所有相关警告。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("在初始化tokenizer后立即设置pad_token， 怎么设置呢，只输出最终的代码就行\n")])])]),a("div",{staticClass:"language-py extra-class"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[t._v("tokenizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"your-model-name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_token "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_token "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eos_token\n")])])])])}),[],!1,null,null,null);a.default=e.exports}}]);