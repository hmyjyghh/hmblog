(window.webpackJsonp=window.webpackJsonp||[]).push([[151],{597:function(a,t,_){"use strict";_.r(t);var e=_(3),n=Object(e.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"special"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#special"}},[a._v("#")]),a._v(" special")]),a._v(" "),t("h3",{attrs:{id:"_1-rrf-粗排原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-rrf-粗排原理"}},[a._v("#")]),a._v(" 1. RRF 粗排原理")]),a._v(" "),t("ul",[t("li",[a._v("Reciprocal Rank Fusion，"),t("code",[a._v("倒数排名融合")])])]),a._v(" "),t("blockquote",[t("p",[a._v("原理分析")])]),a._v(" "),t("ul",[t("li",[a._v("RRF是一种"),t("code",[a._v("融合多个检索结果")]),a._v("列表的算法，"),t("strong",[a._v("特别适用于混合检索场景")]),a._v("（如结合"),t("code",[a._v("稠密向量检索")]),a._v("和"),t("code",[a._v("稀疏词频检索")]),a._v("）。")])]),a._v(" "),t("p",[a._v("使用 Reciprocal Rank Fusion (RRF) 算法"),t("code",[a._v("合并所有检索结果, 结果融合")])]),a._v(" "),t("ul",[t("li",[t("p",[a._v("目的： 提高"),t("strong",[a._v("最相关文档")]),a._v("在结果列表顶部出现的可能性")])]),a._v(" "),t("li",[t("p",[a._v("涉及3个环节")]),a._v(" "),t("ul",[t("li",[t("strong",[a._v("排名分配")]),a._v(":\n"),t("ul",[t("li",[a._v("每个文档在各个排名列表中的位置都会赋予一个分数,分数通常是其排名位置的倒数(即1/排名)。")]),a._v(" "),t("li",[a._v("例如, 排名第一的文档分数为1, 第二名为0.5, 第三名为0.33,以此类推。")])])]),a._v(" "),t("li",[t("strong",[a._v("分数累加")]),a._v(":\n"),t("ul",[t("li",[a._v("每个文档的分数会在所有排名列表中进行累加,出现在多个列表中的文档会获得更高的累加分数。")])])]),a._v(" "),t("li",[t("strong",[a._v("最终排序")]),a._v(":\n"),t("ul",[t("li",[t("strong",[a._v("文档根据其累加分数进行重新排序")]),a._v(", 最终生成一个综合所有排名列表的结果。")])])])])])]),a._v(" "),t("blockquote",[t("p",[a._v("RAG Fusion 里面的内容")])]),a._v(" "),t("ul",[t("li",[t("p",[a._v("倒数排名融合（RRF）：")])]),a._v(" "),t("li",[t("p",[a._v("对每个生成的查询进行基于向量的搜索，形成多路搜索召回。")])]),a._v(" "),t("li",[t("p",[a._v("然后应用"),t("code",[a._v("RRF")]),a._v("算法，根据文档在多个查询中的相关性重新排列文档。")])]),a._v(" "),t("li",[t("p",[t("strong",[a._v("RRF 通过汇总不同搜索请求的排名")]),a._v("，为每个排名列表中的结果分配倒数排名分数，分数按"),t("code",[a._v("1/(rank + k)")]),a._v("计算，其中 "),t("code",[a._v("rank")]),a._v(" "),t("strong",[a._v("是文档在列表中的位置")]),a._v("，k 是一个常量，通常设置为"),t("code",[a._v("60")]),a._v("效果最佳。")])]),a._v(" "),t("li",[t("p",[a._v("最后将从每个搜索系统中"),t("strong",[a._v("获得的倒数排名分数相加")]),a._v("，"),t("strong",[a._v("为每个文档生成合并分数")]),a._v("，并"),t("code",[a._v("根据合并分数对文档进行排名和排序")]),a._v("，提高最相关文档在结果列表顶部出现的可能性。")])]),a._v(" "),t("li",[t("p",[t("strong",[a._v("提示调整")]),a._v("：在提示中要求 "),t("code",[a._v("LLM 更重视原始查询")]),a._v("，以缓解"),t("code",[a._v("多查询可能稀释用户原始意图")]),a._v("的问题。")])])]),a._v(" "),t("h4",{attrs:{id:"计算示例"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#计算示例"}},[a._v("#")]),a._v(" 计算示例")]),a._v(" "),t("ul",[t("li",[a._v("文档1、2、3、4 的rank")])]),a._v(" "),t("p",[t("img",{attrs:{src:"/hmblog/images/search/RRF-1.jpg",alt:"rank"}})]),a._v(" "),t("p",[a._v("假设"),t("code",[a._v("k=60")]),a._v("，文档3在向量搜索中排第1，在关键字搜索中排第4：")]),a._v(" "),t("ul",[t("li",[a._v("文档3向量搜索得分："),t("code",[a._v("1/(60+1) ≈ 0.0164")])]),a._v(" "),t("li",[a._v("文档3关键字搜索得分："),t("code",[a._v("1/(60+4) ≈ 0.0156")])]),a._v(" "),t("li",[a._v("文档3最终得分："),t("code",[a._v("0.0164 + 0.0156 = 0.032")])])]),a._v(" "),t("h4",{attrs:{id:"为什么需要k-60"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#为什么需要k-60"}},[a._v("#")]),a._v(" 为什么需要"),t("code",[a._v("k=60")]),a._v("？")]),a._v(" "),t("ul",[t("li",[a._v("避免“排名1”和“排名2”的"),t("strong",[a._v("得分差距过大")]),a._v("（比如"),t("code",[a._v("k=0")]),a._v("时，"),t("code",[a._v("1/1 - 1/2 = 0.5")]),a._v("；"),t("code",[a._v("k=60")]),a._v("时，"),t("code",[a._v("1/61 - 1/62 ≈ 0.00026")]),a._v("）。")]),a._v(" "),t("li",[a._v("让多个列表中“稳定靠前”的文档得分更高（比如某文档在两个列表中排第2和第3，得分会高于“排第1和第20”的文档）。")])]),a._v(" "),t("h4",{attrs:{id:"适用场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#适用场景"}},[a._v("#")]),a._v(" 适用场景")]),a._v(" "),t("ul",[t("li",[a._v("有多个方法生成的搜索结果, 如果想将它们合并并统一排名, 得到一个单一的结果集，就可以用"),t("code",[a._v("RRF")]),a._v("算法。")])]),a._v(" "),t("h4",{attrs:{id:"参考"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[a._v("#")]),a._v(" 参考")]),a._v(" "),t("ul",[t("li",[t("code",[a._v("k")]),a._v(" 通常取 "),t("code",[a._v("60")]),a._v("（这是学术界和工业界的默认经验值）")]),a._v(" "),t("li",[a._v("实际工程中**必须包含"),t("code",[a._v("k=60")]),a._v("**才能保证效果。")]),a._v(" "),t("li",[a._v("数学语境中明确指 “倒数”，如 reciprocal of 5 即 1/5")]),a._v(" "),t("li",[a._v("RRF 的核心思想是：一个文档在多个排名列表中的位置都很靠前，那么它本身就应该是一个非常相关、非常重要的文档。")])]),a._v(" "),t("div",{staticClass:"language-Text extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("Reciprocal_rank_fusion that takes multiple lists of ranked documents\nand an optional parameter k used in the RRF formula\n\n翻译过来：RRF，接受多个排名文档列表, 以及RRF公式中使用的可选参数k\n")])])]),t("h3",{attrs:{id:"_2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2"}},[a._v("#")]),a._v(" 2.")]),a._v(" "),t("h2",{attrs:{id:"推理优化"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#推理优化"}},[a._v("#")]),a._v(" 推理优化")]),a._v(" "),t("h3",{attrs:{id:"_1-kv-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-kv-cache"}},[a._v("#")]),a._v(" 1. KV Cache")]),a._v(" "),t("ul",[t("li",[a._v("自回归生成模型，每次生成一个词")]),a._v(" "),t("li",[a._v("每次生成新的token, 都要使用历史的KV 值，避免每次重复计算KV 值，设计了KV Cache 优化")]),a._v(" "),t("li",[a._v("每次生成新的token 的时候，将当前轮的 token 的KV 值，和 历史KV 值做一个拼接，"),t("strong",[a._v("进而提高推理效率")]),a._v("。")])]),a._v(" "),t("h3",{attrs:{id:"_2-paged-attention"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-paged-attention"}},[a._v("#")]),a._v(" 2. Paged Attention")]),a._v(" "),t("ul",[t("li",[a._v("参考了，"),t("code",[a._v("操作系统内存分页")]),a._v("的做法")]),a._v(" "),t("li",[a._v("把物理上不连续的分片，在逻辑上给它做成连续的，"),t("strong",[a._v("(也就是把碎片化的显存利用起来)")])])]),a._v(" "),t("blockquote",[t("p",[a._v("怎么做成连续的")])]),a._v(" "),t("ul",[t("li",[t("p",[a._v("在实际推理过程中, 维护一个逻辑块到物理块的映射表, "),t("code",[a._v("多个逻辑块可以对应一个物理块")]),a._v(", 通过"),t("strong",[a._v("引用计数")]),a._v("来表示物理块被引用的次数。")])]),a._v(" "),t("li",[t("p",[a._v("当引用计数大于一时, 代表该物理块被使用, 当引用计数等于零时, 代表该物理块被释放。")])]),a._v(" "),t("li",[t("p",[a._v("通过该方式, 即可实现"),t("strong",[a._v("将地址不连续的物理块串联在一起统一管理")]),a._v("。")])]),a._v(" "),t("li",[t("p",[a._v("块表，block_size, block_size为块大小(默认设为16)。")])])]),a._v(" "),t("p",[t("strong",[a._v("最核心的一点就是")]),a._v("：就是把碎片化的显存利用起来，提高显存的利用率")]),a._v(" "),t("h3",{attrs:{id:"_3-paged-attention-如何管理有效的kv-cache-的"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-paged-attention-如何管理有效的kv-cache-的"}},[a._v("#")]),a._v(" 3. Paged Attention 如何管理有效的KV Cache 的")]),a._v(" "),t("p",[a._v("Paged Attention是 VLLM中用于优化Transformer模型"),t("code",[a._v("推理过程中KV Cache内存管理的关键技术")]),a._v("。")]),a._v(" "),t("p",[a._v("Paged Attention 将每个序列的"),t("code",[a._v("KV Cache")]),a._v("分成若干块, 每个块包含固定数量token的键和值(KV值)。")]),a._v(" "),t("h3",{attrs:{id:"_4-什么是continuous-batching"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-什么是continuous-batching"}},[a._v("#")]),a._v(" 4. 什么是continuous batching？")]),a._v(" "),t("blockquote",[t("p",[a._v("连续批处理")])]),a._v(" "),t("ul",[t("li",[a._v("它是做了迭代的Token级调度")]),a._v(" "),t("li",[a._v("Batch 的大小是根据每次迭代确定的")]),a._v(" "),t("li",[a._v("一个Batch 完成，就可以在其位置插入一个新的，从而实现一个比静态批处理更高的一个 gpu的利用率\n可以类比，Continuous Batching 就像一辆列车， Token就像列车上的人，可以随时上车、下车\n如果乘客是比较密集的，可以始终达到一个满员的状态，可以极大地提高GPU的利用率")])]),a._v(" "),t("p",[a._v("因此, 我们能够在最开始将它们添加到批处理中,然后在整个过程中继续添加和删除那些10个标记的较小序列。")]),a._v(" "),t("p",[a._v("然后那些落后的、那些100个标记序列最终在未尾完成而不会在整个过程中造成瓶颈。")]),a._v(" "),t("p",[a._v("这基本上就是连续批处理，能够为我们做到的事情。")]),a._v(" "),t("p",[a._v("而这最终不仅是提高LLM推断的吞吐量和延迟的关键驱动因素之一, 还是您常见的LLM推断系统中看到的标记流式处理能力的关键, 因为我们能够及时处理它们,")]),a._v(" "),t("ul",[t("li",[a._v("并从系统中逐个获取标记, 我们能够非常迅速地将这些结果返回给用户, "),t("code",[a._v("而不必等待这些大的批处理步骤")]),a._v("。")])]),a._v(" "),t("h3",{attrs:{id:"_5-如何计算kv-cache的大小"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-如何计算kv-cache的大小"}},[a._v("#")]),a._v(" 5. 如何计算KV Cache的大小?")]),a._v(" "),t("ol",[t("li",[a._v("在LLM中, 为了避免重复计算, Self-Attention的Key和Value张量会被缓存下来, 这部分就是我们常说的"),t("code",[a._v("KV Cache")]),a._v("。")]),a._v(" "),t("li",[a._v("虽然能显著提升推理效率, 但在推理时"),t("code",[a._v("KV Cache")]),a._v("的内存开销非常大, 尤其在使用batch推理和长上下文时更明显。")])]),a._v(" "),t("blockquote",[t("p",[a._v("KV Cache的计算方式如下:")])]),a._v(" "),t("ol",[t("li",[a._v("计算单个token的"),t("code",[a._v("KV Cache")]),a._v("大小(以字节为单位):")])]),a._v(" "),t("ul",[t("li",[t("code",[a._v("2")]),a._v("表示"),t("code",[a._v("K")]),a._v("和"),t("code",[a._v("V")]),t("strong",[a._v("两个矩阵都需要缓存")])]),a._v(" "),t("li",[a._v("num_layers是模型的层数")]),a._v(" "),t("li",[a._v("hidden_size是embedding的维度, 等于"),t("code",[a._v("num_heads * dim_head")])]),a._v(" "),t("li",[a._v("precision_in_bytes通常是2("),t("strong",[a._v("对于FP16")]),a._v(")")])]),a._v(" "),t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[a._v("Size1_token "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" num_layers "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" hidden_size "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" precision_in_bytes\n")])])]),t("blockquote",[t("p",[a._v("计算整个"),t("code",[a._v("batch")]),a._v("的"),t("code",[a._v("KV Cache")]),a._v("总大小:")])]),a._v(" "),t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[a._v("Total_Size_KV_Cache "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" batch_size "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" sequence_length "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" num_layers "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" hidden_size "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" precision_in_bytes\n")])])]),t("h4",{attrs:{id:"举例说明"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#举例说明"}},[a._v("#")]),a._v(" 举例说明：")]),a._v(" "),t("p",[a._v("如果一个模型有:")]),a._v(" "),t("ul",[t("li",[a._v("32层(num_layers=32)")]),a._v(" "),t("li",[a._v("hidden_size为4096")]),a._v(" "),t("li",[a._v("使用FP16(每个元素2字节)")]),a._v(" "),t("li",[a._v("batch_size是4")]),a._v(" "),t("li",[a._v("sequence_length 是1024")])]),a._v(" "),t("blockquote",[t("p",[a._v("那么KV Cache的总大小就是:")])]),a._v(" "),t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[a._v("4")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1024")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("32")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("4096")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("*")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("147")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("483")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("648")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token builtin"}},[a._v("bytes")]),a._v(" ≈ 2GB\n")])])]),t("blockquote",[t("p",[a._v("关键点总结:")])]),a._v(" "),t("ul",[t("li",[a._v("2倍系数: 因为要存"),t("code",[a._v("Key")]),a._v("和"),t("code",[a._v("Value")])]),a._v(" "),t("li",[t("strong",[a._v("精度影响内存占用")]),a._v(": 用FP16是2字节,FP8则更小")]),a._v(" "),t("li",[t("code",[a._v("KV Cache")]),a._v("是按"),t("code",[a._v("batch")]),a._v("独立分配的, 所以"),t("code",[a._v("batch")]),a._v("越大占用越高")]),a._v(" "),t("li",[t("code",[a._v("sequence_length")]),a._v("越长, 每个请求要缓存的token越多, 也会大幅提升内存使用")])]),a._v(" "),t("blockquote",[t("p",[a._v("在实际部署中, 如果不开启"),t("code",[a._v("KV Cache")]),a._v(", 计算成本会成倍上升, 但如果开启, 在高并发时要考虑显存瓶颈和缓存策略.")])]),a._v(" "),t("h3",{attrs:{id:"_6-请解释在multi-headed-attention模块中-每一层的维度是怎么计算的"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-请解释在multi-headed-attention模块中-每一层的维度是怎么计算的"}},[a._v("#")]),a._v(" 6. 请解释在"),t("code",[a._v("multi-headed attention")]),a._v("模块中, 每一层的维度是怎么计算的?")]),a._v(" "),t("ul",[t("li",[a._v("待补充")])]),a._v(" "),t("h3",{attrs:{id:"_7-如何确保attention层能够关注输入中正确的部分"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-如何确保attention层能够关注输入中正确的部分"}},[a._v("#")]),a._v(" 7. 如何确保Attention层能够关注输入中正确的部分?")]),a._v(" "),t("p",[a._v("在"),t("code",[a._v("Transformer")]),a._v("中, Masking是确保"),t("code",[a._v("Attention")]),a._v("层聚焦于正确位置的关键机制。")]),a._v(" "),t("ul",[t("li",[t("p",[a._v("尤其是在语言建模和机器翻译等任务中, 如果没有合理的Mask, "),t("strong",[a._v('模型就可能"看到不该看到的内容"')]),a._v(", 从而导致训练偏差或生成错误。")])]),a._v(" "),t("li",[t("p",[a._v("Attention中主要有两种常用的Mask:")])])]),a._v(" "),t("ol",[t("li",[t("p",[a._v('Padding Mask\n目的:\n忽略输入序列中的padding tokens。因为在NLP中,我们往往需要要对齐不同长度的句子,会用特殊的\nPADtoken来补齐。但这些token没有实际语义,不能参与attention。\n工作方式:\nPadding Mask会对输入中 padding的位置进行标记,确保这些位置在attention score的计算中不产生\n影响。\n举例:\n如果输入是[word1,word2,word3,PAD],那么对应的Padding MasSk是[1,1,1,0]。其中\n表示"忽略这个位置",注意力不会关注它。')])]),a._v(" "),t("li",[t("p",[a._v('Look-Ahead Mask (也叫Causal Mask)\n目的:\n防止模型在生成下一个token时提前"偷看"未来的信息,确保模型只关注当前位置及其之前的内容。这\n对auto-regressive模型(如GPT)非常重要。\n工作方式:\n通过一个上三角矩阵来实现,每个位置只能看到自己之前的toker1。未来的token会被mask掉,通常赋\n值为 -inf,使得softmax后对应权重为0。')])])]),a._v(" "),t("h2",{attrs:{id:"hottop-100问"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hottop-100问"}},[a._v("#")]),a._v(" hotTop 100问")]),a._v(" "),t("h3",{attrs:{id:"_1-gpt和bert的区别"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-gpt和bert的区别"}},[a._v("#")]),a._v(" 1. GPT和Bert的区别？")]),a._v(" "),t("ul",[t("li",[a._v("BERT主要用于自然语言理解,")])]),a._v(" "),t("h3",{attrs:{id:"_2-为什么现在的大模型大多是decoder-only的架构-重要"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-为什么现在的大模型大多是decoder-only的架构-重要"}},[a._v("#")]),a._v(" 2. 为什么现在的大模型大多是decoder-only的架构? - 重要")]),a._v(" "),t("h3",{attrs:{id:"_3-chatgpt的训练步骤有哪些"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-chatgpt的训练步骤有哪些"}},[a._v("#")]),a._v(" 3. ChatGPT的训练步骤有哪些?")]),a._v(" "),t("h3",{attrs:{id:"_4-解释llm中token的概念"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-解释llm中token的概念"}},[a._v("#")]),a._v(" 4. 解释LLM中token的概念")]),a._v(" "),t("h3",{attrs:{id:"_5-llm中的因果语言建模clm与屏蔽语言建模mlm有什么区别"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-llm中的因果语言建模clm与屏蔽语言建模mlm有什么区别"}},[a._v("#")]),a._v(" 5. LLM中的因果语言建模CLM与屏蔽语言建模MLM有什么区别?")]),a._v(" "),t("h3",{attrs:{id:"_6-如何评估语言模型的性能"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-如何评估语言模型的性能"}},[a._v("#")]),a._v(" 6. 如何评估语言模型的性能?")]),a._v(" "),t("p",[a._v("在自然语言处理中,有两种评估语言模型性能的方式:内在评估和外在评估。\n·内在评估捕捉模型在捕捉其应该捕捉的内容 (如概率)方面的表现。\n·外在评估(或任务导向评估)捕捉模型在特定任务中的实用性。\n评估语言模型的性能通常涉及多个指标,以衡量其在不同任务上的表现。以下是一些常见的评估方法和指标:\n1.困惑度(Perplexity):困惑度是语言模型性能的一个常用指标,它衡量模型预测给定序列的概率。困惑度\n越低,表示模型对数据的预测越好。困惑度计算公式是所有单词概率的的负对数的指数平均值的倒数。\n2.准确率(Accuracy):对于分类任务,准确率是最直观的评估指示,即模型正确预测的样本数占总样本数的\n比例。\n3.BLEU分数:用于机器翻译和文本生成任务,BLEU (Bilingual Evaluation Understudy)通过比较模型生成\n的序列与人工翻译的参考序列的n-gram重叠度来评估性能。\n4.ROUGE分数:主要用于评估文本摘要,它计算模型生成的摘要与参考摘要的重叠度,提供了一种召回率\n(Recall)度量。\n5.METEOR:另一个用于机器翻译和文本生成的指标,METEOR结合合了精确度、召回率、词干匹配和同义词匹\n配,以更全面地评估生成文本的质量。\n6.F1分数:对于二分类或多分类任务,F1分数结合了精确率和召回率这是它们的调和平均值,适用于不平衡数\n据集。\n7.人类评估:有时候,机器评估的指标可能无法完全反映人类的判断,因此人工评估也是必要的。例如,人类评\n审员可以评估模型生成文本的连贯性、逻辑性和自然度。\n8.AUC-ROC曲线:对于分类任务,计算ROC曲线下的面积(AUC),可以度量模型区分正负样本的能力。\n9.PPLM (Prompt Predictive Log Likelihood):用于评估模型在特定提示下的预测性能。\n10.Zero-/Few-/Semi-Shot Learning:评估模型在没有或只有少量量示例数据的情况下进行新任务学习的能力")]),a._v(" "),t("h3",{attrs:{id:"_7-如何缓解-llm-复读机问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-如何缓解-llm-复读机问题"}},[a._v("#")]),a._v(" 7. 如何缓解 LLM 复读机问题？")]),a._v(" "),t("p",[a._v("LLM中的复读机问题,指的是大型语言模型在生成文本时,出现重复、冗长或循环的内容。缓解复读机问题有以\n下方法:\n多样性训练数据:在训练阶段,尽量使用多样性的语料库来训练模型,避免数据偏差和重复文本的问\n题。\n引入噪声:在生成文本时,可以引入一些随机性或噪声,例如通过采样不同的词或短语,或者引入随机\n的变换操作,以增加生成文本的多样性。\n温度参数调整:温度参数是用来控制生成文本的多样性的一个参数收。通过调整温度参数的值,可以控制\n生成文本的独创性和多样性,从而减少复读机问题的出现。\n后处理和过滤:对生成的文本进行后处理和过滤,去除重复的句子子或短语,以提高生成文本的质量和多\n样性。\nBeam搜索调整:在生成文本时,可以调整Beam搜索算法的参数。Beam搜索是一种常用的生成策\n略,它在生成过程中维护了一个候选序列的集合。通过调整整Beam大小和搜索宽度,可以控制生成文本\n的多样性和创造性。\n·人工干预和控制:对于关键任务或敏感场景,可以引入人工干预和控制机制,对生成的文本进行审查和\n筛选,确保生成结果的准确性和多样性。")]),a._v(" "),t("h3",{attrs:{id:"_8-llama的tokenizer-sentence-piece是怎么"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_8-llama的tokenizer-sentence-piece是怎么"}},[a._v("#")]),a._v(" 8. LLaMA的tokenizer sentence-piece是怎么")]),a._v(" "),t("p",[a._v("处理数字的?\nLLaMA的tokenizer采用了SentencePiece算法来进行文本的分词处理。SentencePiece是一种无语言依赖\n的分词工具,它将文本视为一个无空格的字符串,通过学习子词单元来实现高效的分词。\n在处理数字时,SentencePiece会将数字视为一般的字符序列来进行处理。这样数字不会被特别对待,而\n是和其他字符一样,被拆分成更小的子词或字符单元。例如,长数字串可能会被拆分成多个较小的片段或\n单个数字字符,这样可以确保数字在模型输入中得到有效表示。\nSentencePiece在预处理阶段,会基于大量的文本数据学习一个词汇表,这个词汇表包含了最常见的子词\n单元,包括字母、数字和标点符号等。然后,在实际使用时,输入文本会根据这个词汇表进行分割,生成\n相应的子词序列。对于数字,这意味着它们会被拆分成可以在词汇表中找到的最小单元,这样可以确保模\n型能够识别和处理任何形式的数字输入。")]),a._v(" "),t("ul",[t("li",[t("strong",[a._v("总结一下")]),a._v(", LLaMA的tokenizer利用SentencePiece算法,通过将数字作为普通字符处理,确保模型能够有\n效地处理和理解数字输入。这个方法简单有效,而且在处理多种语言和字符集时具有很强的通用性和适应\n性。")])]),a._v(" "),t("h3",{attrs:{id:"_9-请简述下transformer基本流程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_9-请简述下transformer基本流程"}},[a._v("#")]),a._v(" 9. 请简述下Transformer基本流程")]),a._v(" "),t("p",[a._v("让我们首先将模型看作一个单一的黑盒子。在机器翻译应用中,它会接收一句话的输入(一种语言),然后输出它\n的翻译(另一种语言), 如下图所示,")]),a._v(" "),t("h3",{attrs:{id:"_10-为什么基于transformer的架构需要多头注意力机制"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_10-为什么基于transformer的架构需要多头注意力机制"}},[a._v("#")]),a._v(" 10. 为什么基于Transformer的架构需要多头注意力机制?")]),a._v(" "),t("p",[a._v("在Transformer中使用的多头注意力出现前,基于各种层次的各种fancy的注意力计算方式,层出不穷。而\nTransformer的多头注意力看上去是借鉴了CNN中同一卷积层内倒使用多个卷积核的思想,原文中使用了8个\nscaled dot-product attention,在同一multi-head attention层中,输入均为KQV,同时进行注意力的计\n算,彼此之前参数不共享,最终将结果拼接起来,这样可以允许模型在不同的表示子空间里学习到相关的信息,在\n此之前的AStructured Self-attentive Sentence Embedding也有着类似的思想。")]),a._v(" "),t("h3",{attrs:{id:"_11-强化学习的概念吗-它如何应用于chatgpt"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_11-强化学习的概念吗-它如何应用于chatgpt"}},[a._v("#")]),a._v(" 11. 强化学习的概念吗? 它如何应用于ChatGPT?")]),a._v(" "),t("p",[a._v("从人类反馈中强化学习(也称为从人类偏好中的RL)是一个具有挑战性的概念,因为它涉及多个模型训练过程和\n不同的部署阶段。我们把训练过程分解为三个核心步骤:\n1.预训练一个语言模型(LM)。\n2.收集数据并训练奖励模型(Reward Model,RM)。\n3.用强化学习(RL)方式微调语言模型(LM)。")]),a._v(" "),t("h3",{attrs:{id:"_12-为什么transformers需要位置编码"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_12-为什么transformers需要位置编码"}},[a._v("#")]),a._v(" 12. 为什么transformers需要位置编码？")]),a._v(" "),t("h3",{attrs:{id:"_13-transformer模型很昂贵-有哪些方法可以缩小它们的规模"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_13-transformer模型很昂贵-有哪些方法可以缩小它们的规模"}},[a._v("#")]),a._v(" 13. Transformer模型很昂贵,有哪些方法可以缩小它们的规模?")]),a._v(" "),t("h3",{attrs:{id:"_14-如何使用其他llms评估llms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_14-如何使用其他llms评估llms"}},[a._v("#")]),a._v(" 14. 如何使用其他LLMs评估LLMS?")]),a._v(" "),t("p",[a._v("LLM-as-a-judge提供了两个关键的好处:可扩展性和可解释性,可以减少了对人类参与的需求,实现了可扩展的\n基准和快速迭代。此外,LLM充当裁判不仅提供分数还提供解释,使他们的输出可以解释。\n《Judging LLM-as-a-judge with MT-Bench and Chatbot Arena》是出了3种LLM-as-a-judge的实现方式:\n1)成对比较pairwise comparison\n其思想在于:一个LLM评委被告知一个问题和两个答案,任务是确确定哪一个更好或者宣布一个平局。")]),a._v(" "),t("h3",{attrs:{id:"_15-解释什么是transformer结构中的自注意-self-attention-机制"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_15-解释什么是transformer结构中的自注意-self-attention-机制"}},[a._v("#")]),a._v(" 15. 解释什么是Transformer结构中的自注意(Self-Attention)机制?")]),a._v(" "),t("h3",{attrs:{id:"_16-介绍一下post-layer-norm和pre-layer-norm的区别"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_16-介绍一下post-layer-norm和pre-layer-norm的区别"}},[a._v("#")]),a._v(" 16. 介绍一下post layer norm和pre layer norm的区别")]),a._v(" "),t("h3",{attrs:{id:"_17-什么是过拟合和欠拟合"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_17-什么是过拟合和欠拟合"}},[a._v("#")]),a._v(" 17. 什么是过拟合和欠拟合？")]),a._v(" "),t("p",[a._v("过拟合和欠拟合是机器学习和统计建模中常见的问题,它们分别表示楼型对训练数据拟合程度的两种极端情况。\n过拟合:过拟合是指模型在训练数据上表现非常好,但在未见过的测话式数据或新数据上表现较差。这种情况发生\n是因为模型过于复杂,学习了训练数据中的噪声或异常点,而没有捕捉到数据的真实模式。过拟合模型往往对训练\n数据中的噪声过于敏感,导致泛化能力下降。\n欠拟合:欠拟合则是模型对训练数据和测试数据的拟合都较差,这通常是因为模型过于简单,无法捕获数据的复\n杂结构。欠拟合模型在训练和测试数据上的表现都低于预期,说明模型还需要学习更多的特征或模式。\n如何避免过拟合和欠拟合:\n1.增加数据量:获取更多的训练数据可以提供更多的样本信息,帮助漠型更好地学习数据的真实模式,减少过拟\n合和欠拟合的风险。\n2.特征选择和降维:选择对预测目标最有影响的特征,减少冗余特征,可以降低模型复杂度,防止过拟合。\n3.正则化:如L1和L2正则化,通过添加惩罚项来限制模型参数女的大小,防止过拟合。\n4.Dropout:在神经网络中,随机关闭部分神经元,避免模型过度依赖某些特征,增强模型的泛化能力。\n5.早停法:在验证集上监控模型性能,当验证集性能开始下降时停止训练,防止过拟合\n6.模型复杂度调整:增加或减少模型的复杂度(如神经网络的层数或宽度),找到适合数据的复杂度,防止欠拟\n合和过拟合。\n7.交叉验证:通过使用K折交叉验证来评估模型性能,更准确地评估模型泛化能力,帮助避免过拟合。\n8.集成学习:结合多个模型的预测,如随机森林、梯度提升机等,提高模型的泛化性能\n9.早始化:使用合适的初始化方法,如Xavier初始化或He初始台化,帮助模型在训练初期就能学习到有意义的特\n征。\n10.优化算法和学习率:选择合适的优化算法(如Adam、RMSprop等)和学习率调整策略,以更有效地学习数\n据的模式。")]),a._v(" "),t("h3",{attrs:{id:"_18-如何解决类别不平衡问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_18-如何解决类别不平衡问题"}},[a._v("#")]),a._v(" 18. 如何解决类别不平衡问题?")]),a._v(" "),t("h3",{attrs:{id:"_19-如何选择模型中的超参数-有什么方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_19-如何选择模型中的超参数-有什么方法"}},[a._v("#")]),a._v(" 19. 如何选择模型中的超参数?有什么方法?")]),a._v(" "),t("h3",{attrs:{id:"_20-什么是梯度消失和爆炸-有哪些解决方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_20-什么是梯度消失和爆炸-有哪些解决方法"}},[a._v("#")]),a._v(" 20. 什么是梯度消失和爆炸?有哪些解决方法?")]),a._v(" "),t("p",[a._v('什么是dropout?\nDropout是一种正则化技术,用于防止神经网络中的过拟合。在训练神经经网络时,dropout随机"丢弃"一部分神\n经元,以减少神经元之间的依赖,增加模型的泛化能力。\n具体来说,当应用dropout时,网络在每次训练迭代中会随机选择一部分神经元,并将其输出设为0。选择哪些神\n经元被丢弃是基于一个概率p(保留概率通常是0.5),这意味着每个神经经元在每次训练迭代中都有p的概率被保\n留,其余的概率则被关闭。在测试阶段,为了保持模型的预测能力,所有有神经元都会参与计算,但它们的权重会被\n调整,以模拟在训练过程中被丢弃的情况。这种权重调整通常是通过乘以以保留概率p来完成的。\nDropout有以下几个优点:\n1.正则化:通过随机丢弃神经元,dropout可以防止网络过度依赖某些特定的特征组合,从而提高模型的泛化性\n能。\n2.多样性:dropout鼓励网络学习多个不同的特征表示,这些特征在训练过程中相互独立,有助于提高网络的鲁\n棒性。\n3.模型集成:在某种程度上,dropout可以视为一种模型集成策略,因为在每个训练迭代中,它实际上都在训练\n一个不同的子网络,这些子网络在测试时以某种形式平均其预测。')]),a._v(" "),t("p",[a._v('在参数初始化时,为什么不能全零初始化?\n在神经网络中,全零初始化(初始化所有权重参数为0)会导致放一些问题,尤其是在使用ReLU或其他非线性激活\n函数时。以下是一些原因:\n1.梯度消失:如果所有权重都初始化为0,那么在前向传播过程中,每每一层的激活值将趋近于相同,因为每个神\n经元都将产生相同的输出。在反向传播时,梯度也会沿着每一层以相同的方式传播,导致梯度在深层网络中迅\n速变小,这被称为梯度消失问题。在深度网络中,这可能会使网络难以训练。\n2.对称性问题:当所有神经元具有相同的初始权重时,它们在训练过程中学习到的特征可能会高度相关或完全相\n同,这打破了神经网络层中神经元的多样性。由于它们在反向传播时收女到相同的梯度,它们可能会更新成完全\n相同的权重,导致整个网络的性能下降。\n3.稀疏激活:ReLU激活函数在输入为负时输出为0,如果所有权重都是0,那么在输入为负时,整个网络的激活\n输出将为0,导致"死亡ReLU"问题,即某些神经元在整个训练过程中不会被激活。\n因此,通常使用随机初始化权重,如均匀分布或正态分布初始化,以打丁破对称性,引入非零均值的随机性,使得每\n个神经元有独立学习不同特征的机会,同时避免梯度消失问题。这样可J以促进网络的训练和泛化能力。')]),a._v(" "),t("p",[a._v('1*1卷积的作用是什么?\n1x1卷积在深度学习领域,特别是在卷积神经网络(CNN)中,有以下几个主要作用:\n1.维度调整:1x1卷积的主要功能是改变输入特征图的通道数(即深度)。通过使用不同数量的1x1卷积核,可\n以减少或增加特征图的通道数,无需显著增加计算复杂度。例刚如,它可以用作"通道降维"以减少计算量,或\n者在"通道升维"中引入新的特征组合。\n2.信息融合:1x1卷积可以在不同通道之间进行信息的混合和和交互。通过这种方式,不同特征通道的信息可以相\n互融合,帮助网络学习更复杂的特征表示。\n3.节省计算资源:相比于更大尺寸的卷积核,1x1卷积的计算量小得多,因此可以有效地降低模型的计算复杂度\n和参数数量,而不会显著影响模型的性能。\n4.网络结构设计的灵活性:1x1卷积允许设计者在不改变特征图的空间尺寸(宽度、高度)的情况下,自由调整\n网络的深度和宽度,增加了网络结构设计的灵活性。')]),a._v(" "),t("p",[a._v("解释一下layer normlization和batch\nnormlization\nLayer Normalization和Batch Normalization是深度学习中常用的两种正则化技术,用于加速训练过程和提高模\n型的稳定性。\nLayerNormalization是对每一个样本的所有特征进行归一化处理。它通过计算每一层神经元在一个样本内的均值\n和方差,对每个神经元的激活值进行归一化。公式如下:")]),a._v(" "),t("h2",{attrs:{id:"提示词相关"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#提示词相关"}},[a._v("#")]),a._v(" 提示词相关")]),a._v(" "),t("p",[a._v("提示工程技术大概可以分成四类:\n基于样本提示技术\n·思维链技术\n·自动增强技术\n?交互与推理技术\n具体的有如下代表方法:")]),a._v(" "),t("ul",[t("li",[a._v("这个画过思维导图了")])]),a._v(" "),t("p",[a._v("CoT")]),a._v(" "),t("p",[a._v("采用自我一致性(Self-Consistency)采样\n的好处是什么?")]),a._v(" "),t("p",[a._v("介绍一下思维树(Tree of Thought)\n在CoT的基础上,思维树(ToT)则是通过在每一步探索多种推理可能性来扩展模型性能。ToT首先将问题分解\n为多个思维步骤,每个步骤生成多个思维,从而创建一个树状结构。搜索过程可以是广度优先搜索(BFS)或深度\n优先搜索(DFS),每个状态由分类器(通过提示)或多数票进行评估")]),a._v(" "),t("p",[a._v("说说你对COT(Chain-of-Thought Prompt)\n和Instruction Tuning的理解")]),a._v(" "),t("p",[a._v("这个也整理过了")]),a._v(" "),t("p",[a._v("请阐述一下instructiontuning和prompt\nlearning的区别")]),a._v(" "),t("p",[a._v("这个也整理过了")]),a._v(" "),t("h2",{attrs:{id:"langchain"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#langchain"}},[a._v("#")]),a._v(" langchain")]),a._v(" "),t("p",[a._v("解释一下langchain Agent的概念\nLangChain Agent是框架中驱动决策制定的实体。它可以访问一组工具,并可以根据用户的输入决定调用哪个工\n具。代理帮助构建复杂的应用程序,这些应用程序需要自适应和特定于上下文的响应。当存在取决于用户输入和其\n他因素的未知交互链时,它们特别有用。\n可以简单的理解为它可以动态的帮我们选择和调用Chain或者已有的工具,执行过程可以参考下面这张图:")]),a._v(" "),t("p",[a._v("langchain的6大核心组件是什么, 每个组件有什么作用?")]),a._v(" "),t("h2",{attrs:{id:"向量数据库"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#向量数据库"}},[a._v("#")]),a._v(" 向量数据库")]),a._v(" "),t("p",[a._v("向量数据库有哪些, 你知道的介绍一下")]),a._v(" "),t("p",[a._v("Milvus、Chroma、FAISS")]),a._v(" "),t("h2",{attrs:{id:"强化学习-rl"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#强化学习-rl"}},[a._v("#")]),a._v(" 强化学习: RL")]),a._v(" "),t("p",[a._v("请简述下PPO算法")]),a._v(" "),t("p",[a._v("与有监督学习相比,强化学习能够给大语言\n模型带什么哪些好处")]),a._v(" "),t("h3",{attrs:{id:"rlhf"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rlhf"}},[a._v("#")]),a._v(" RLHF:")]),a._v(" "),t("p",[a._v("介绍一下基于人类反馈的强化学习流程\n基于人类反馈的强化学习主要分为奖励模型训练和近端策略优化两两个步骤。奖励模型通过由人类反馈标注的偏好\n数据来学习人类的偏好,判断模型回复的有用性以及保证内容的无害性。奖励模型模拟了人类的偏好信息,能够\n不断地为模型的训练提供奖励信号。在获得奖励模型后,需要借助强化学习对语言模型继续进行微调。OpenAl\n在大多数任务中使用的强化学习算法都是近端策略优化算法(ProximalPolicyOptimization,PPO)。近端策略优\n化可以根据奖励模型获得的反馈优化模型,通过不断的迭代,让摸型探索和发现更符合人类偏好的回复策略。近端\n策略优化的流程如图所示。")]),a._v(" "),t("p",[a._v("奖励模型是如何训练的,它的损失函数是什么?")])])}),[],!1,null,null,null);t.default=n.exports}}]);