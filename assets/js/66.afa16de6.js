(window.webpackJsonp=window.webpackJsonp||[]).push([[66],{548:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"大模型实战用到的常用方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#大模型实战用到的常用方法"}},[t._v("#")]),t._v(" 大模型实战用到的常用方法")]),t._v(" "),s("h2",{attrs:{id:"常用函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常用函数"}},[t._v("#")]),t._v(" 常用函数")]),t._v(" "),s("h3",{attrs:{id:"_1-autotokenizer-from-pretrained"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-autotokenizer-from-pretrained"}},[t._v("#")]),t._v(" 1. AutoTokenizer.from_pretrained()")]),t._v(" "),s("p",[t._v("根据传的模型，自动加载和实例化与预训练模型对应的分词器（Tokenizer），无需手动指定具体的分词器类。")]),t._v(" "),s("p",[t._v("eg:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 自动选择正确的分词器")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-cased"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# → BertTokenizer")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpt2"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("                   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# → GPT2Tokenizer  ")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"t5-small"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("               "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# → T5Tokenizer")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"facebook/bart-large"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# → BartTokenizer")]),t._v("\n")])])]),s("h3",{attrs:{id:"_2-bert-tokenizer输出的两个特殊标记。"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-bert-tokenizer输出的两个特殊标记。"}},[t._v("#")]),t._v(" 2. BERT tokenizer输出的两个特殊标记。")]),t._v(" "),s("p",[t._v("[CLS] 和 [SEP] 是BERT模型中的特殊标记，它们有非常重要的功能：")]),t._v(" "),s("p",[t._v("[CLS] - Classification Token（分类标记）： 总是出现在序列的开头")]),t._v(" "),s("p",[t._v("[SEP] - Separator Token（分隔标记）： 出现在序列的末尾")]),t._v(" "),s("h2",{attrs:{id:"实战步骤"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#实战步骤"}},[t._v("#")]),t._v(" 实战步骤")]),t._v(" "),s("ol",[s("li",[t._v("首先引入torch.utils.data 中的DataSet类，构建自定义的数据集类，按照Pytorch的规范，需要实现"),s("code",[t._v("__getitem__")]),t._v("和"),s("code",[t._v("__len__")]),t._v("方法。")]),t._v(" "),s("li",[t._v("构建数据集，并使用DataLoader进行数据加载，设置好batch_size、shuffle等参数。")]),t._v(" "),s("li",[t._v("构建模型，并使用预训练的模型参数进行初始化。")]),t._v(" "),s("li",[t._v("定义损失函数和优化器。")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" DataLoader\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BertTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" BertForSequenceClassification\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 自定义数据集类")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyDataset")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data\n    self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BertTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bert-base-uncased'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__getitem__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    text "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    label "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    encoding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" truncation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'max_length'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" return_tensors"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[t._v("数据预处理")])]),t._v(" "),s("p",[t._v("使用DataLoader 库来按 batch 加载数据， 并使用PyTorch的DataLoader类来创建一个数据加载器。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataLoader\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建数据加载器")]),t._v("\ntrain_loader "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataLoader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ol",{attrs:{start:"3"}},[s("li",[t._v("模型构建")])]),t._v(" "),s("p",[t._v("使用预训练的BERT模型作为基础模型，并使用PyTorch的"),s("code",[t._v("nn.Module")]),t._v("类来构建自定义的模型。在模型中，我们只需要定义输入层和输出层，中间层使用预训练的BERT模型。")]),t._v(" "),s("h2",{attrs:{id:"transformers-实战"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#transformers-实战"}},[t._v("#")]),t._v(" transformers 实战")]),t._v(" "),s("h3",{attrs:{id:"_1-nlp领域经典任务"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-nlp领域经典任务"}},[t._v("#")]),t._v(" 1. NLP领域经典任务")]),t._v(" "),s("ul",[s("li",[t._v("序列标注任务")]),t._v(" "),s("li",[t._v("翻译任务")]),t._v(" "),s("li",[t._v("文本摘要任务")]),t._v(" "),s("li",[t._v("抽取式问答")]),t._v(" "),s("li",[t._v("Prompting 情感分析")])]),t._v(" "),s("h2",{attrs:{id:"常用库使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常用库使用"}},[t._v("#")]),t._v(" 常用库使用")]),t._v(" "),s("h3",{attrs:{id:"📌-总结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#📌-总结"}},[t._v("#")]),t._v(" 📌 总结")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("库名")]),t._v(" "),s("th",[t._v("是否必需")]),t._v(" "),s("th",[t._v("作用")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[s("code",[t._v("matplotlib")])]),t._v(" "),s("td",[t._v("✅ 必需")]),t._v(" "),s("td",[t._v("绘制收敛曲线")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("numpy")])]),t._v(" "),s("td",[t._v("✅ 必需")]),t._v(" "),s("td",[t._v("数值计算与数据存储")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("torch")]),t._v(" / "),s("code",[t._v("tensorflow")])]),t._v(" "),s("td",[t._v("✅ 必需")]),t._v(" "),s("td",[t._v("模型定义与训练")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("pandas")])]),t._v(" "),s("td",[t._v("🔁 可选")]),t._v(" "),s("td",[t._v("日志数据处理")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("seaborn")])]),t._v(" "),s("td",[t._v("🔁 可选")]),t._v(" "),s("td",[t._v("美化图表")])]),t._v(" "),s("tr",[s("td",[s("code",[t._v("tqdm")])]),t._v(" "),s("td",[t._v("🔁 可选")]),t._v(" "),s("td",[t._v("显示训练进度")])])])]),t._v(" "),s("p",[t._v("准备好这些库后，你就可以编写训练代码并在训练过程中记录指标，最后用 "),s("code",[t._v("matplotlib")]),t._v(" 绘制出清晰的"),s("strong",[t._v("模型收敛曲线图")]),t._v("了。")]),t._v(" "),s("ul",[s("li",[t._v("Matplotlib提供类似 MATLAB 的绘图接口，支持 NumPy 、 Pandas 等数据处理工具的集成，可高效实现数据可视化流程。")])]),t._v(" "),s("h2",{attrs:{id:"模型的评价指标-bleu"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#模型的评价指标-bleu"}},[t._v("#")]),t._v(" 模型的评价指标 BLEU")]),t._v(" "),s("ul",[s("li",[t._v("BLEU（Bilingual Evaluation Understudy）是一种用于评估机器翻译质量的指标。BLEU 的核心思想是，机器翻译的输出越接近人工翻译的结果，其质量就越高。")])]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sacrebleu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("metrics "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BLEU\n\npredictions "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This plugin lets you translate web pages between several languages automatically."')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nbad_predictions_1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This This This This"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nbad_predictions_2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This plugin"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nreferences "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This plugin allows you to automatically translate web pages between several languages."')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nbleu "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BLEU"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bleu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corpus_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" references"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bleu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corpus_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bad_predictions_1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" references"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bleu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("corpus_score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bad_predictions_2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" references"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("score"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("46.750469682990165")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.683602693167689")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\n\nBLEU 值的范围从 "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" 到 "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("，越高越好。\n")])])]),s("h2",{attrs:{id:"什么是生成式问答"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#什么是生成式问答"}},[t._v("#")]),t._v(" 什么是生成式问答？")]),t._v(" "),s("p",[t._v("是的，这种实现写法属于"),s("strong",[t._v("生成式问答模型")]),t._v("，主要体现在以下几个关键特征上：")]),t._v(" "),s("ol",[s("li",[s("p",[s("strong",[t._v("模型选择")]),t._v("：使用的"),s("code",[t._v("T5ForConditionalGeneration")]),t._v("是专为生成任务设计的模型，其核心能力是根据输入文本生成全新的输出序列，而非从给定文本中抽取片段。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("输出方式")]),t._v("：模型通过"),s("code",[t._v(".generate()")]),t._v("方法直接生成答案文本，这个答案是模型根据上下文和问题“创造”出来的，可能包含原文中没有的词汇组合（尽管在问答任务中通常会贴合原文）。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("与抽取式的区别")]),t._v("：")])])]),t._v(" "),s("ul",[s("li",[t._v("抽取式问答（如BERT用于SQuAD任务）的输出是答案在原文中的起始和结束位置，答案必然是原文的子串")]),t._v(" "),s("li",[t._v("而这段代码中，模型输出的是独立生成的文本序列，理论上可以超越原文范围（实际任务中会受训练数据约束）")])]),t._v(" "),s("ol",{attrs:{start:"4"}},[s("li",[s("strong",[t._v("输入格式")]),t._v("：将问题和上下文拼接为"),s("code",[t._v('"question: ... context: ..."')]),t._v("的形式，这是典型的生成式问答提示（prompt）设计，模型需要理解这种指令并生成符合要求的答案。")])]),t._v(" "),s("p",[t._v("因此，这种实现完全符合生成式问答模型的定义——通过生成全新文本的方式来回答问题，而不是从上下文里抽取现成的答案片段。")]),t._v(" "),s("h2",{attrs:{id:"什么是抽取式问答"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#什么是抽取式问答"}},[t._v("#")]),t._v(" 什么是抽取式问答？")]),t._v(" "),s("ul",[s("li",[t._v("抽取式问答（如BERT用于SQuAD任务）的输出是答案在原文中的起始和结束位置，答案必然是原文的子串")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"transformer-命名实体任务实战"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#transformer-命名实体任务实战"}},[t._v("#")]),t._v(" Transformer 命名实体任务实战")]),t._v(" "),s("p",[s("code",[t._v("word_ids()")]),t._v(" 是 Hugging Face Tokenizer 中一个非常重要的方法，它的作用是 "),s("strong",[t._v("追踪分词后的每个 token 对应原始句子中的哪个单词")]),t._v("。")]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"核心作用-建立分词前后的映射关系"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#核心作用-建立分词前后的映射关系"}},[t._v("#")]),t._v(" "),s("strong",[t._v("核心作用：建立分词前后的映射关系")])]),t._v(" "),s("p",[t._v("当句子被 tokenizer 分割成多个 subword tokens 时，"),s("code",[t._v("word_ids()")]),t._v(" 可以帮助我们理解：")]),t._v(" "),s("ul",[s("li",[t._v("哪些 tokens 属于同一个原始单词")]),t._v(" "),s("li",[t._v("每个 token 在原始句子中的位置信息")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"代码示例与解析"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#代码示例与解析"}},[t._v("#")]),t._v(" "),s("strong",[t._v("代码示例与解析")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AutoTokenizer\n\nsentence "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'海钓比赛地点在厦门与金门之间的海域。'")]),t._v("\ntokenizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-chinese"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 分词并查看结果")]),t._v("\ntokens "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Tokens:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convert_ids_to_tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Word IDs:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出结果：")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Tokens: ['[CLS]', '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。', '[SEP]']")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Word IDs: [None, 0, 0, 1, 1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 9, 9, 10, None]")]),t._v("\n")])])]),s("hr"),t._v(" "),s("h3",{attrs:{id:"结果解析"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#结果解析"}},[t._v("#")]),t._v(" "),s("strong",[t._v("结果解析")])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Token")]),t._v(" "),s("th",[t._v("Word ID")]),t._v(" "),s("th",[t._v("说明")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("[CLS]")]),t._v(" "),s("td",[t._v("None")]),t._v(" "),s("td",[t._v("特殊token，不属于任何单词")])]),t._v(" "),s("tr",[s("td",[t._v("海")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v('属于第0个单词 "海钓"')])]),t._v(" "),s("tr",[s("td",[t._v("钓")]),t._v(" "),s("td",[t._v("0")]),t._v(" "),s("td",[t._v('属于第0个单词 "海钓"')])]),t._v(" "),s("tr",[s("td",[t._v("比")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v('属于第1个单词 "比赛"')])]),t._v(" "),s("tr",[s("td",[t._v("赛")]),t._v(" "),s("td",[t._v("1")]),t._v(" "),s("td",[t._v('属于第1个单词 "比赛"')])]),t._v(" "),s("tr",[s("td",[t._v("地")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v('属于第2个单词 "地点"')])]),t._v(" "),s("tr",[s("td",[t._v("点")]),t._v(" "),s("td",[t._v("2")]),t._v(" "),s("td",[t._v('属于第2个单词 "地点"')])]),t._v(" "),s("tr",[s("td",[t._v("在")]),t._v(" "),s("td",[t._v("3")]),t._v(" "),s("td",[t._v('属于第3个单词 "在"')])]),t._v(" "),s("tr",[s("td",[t._v("厦")]),t._v(" "),s("td",[t._v("4")]),t._v(" "),s("td",[t._v('属于第4个单词 "厦门"')])]),t._v(" "),s("tr",[s("td",[t._v("门")]),t._v(" "),s("td",[t._v("4")]),t._v(" "),s("td",[t._v('属于第4个单词 "厦门"')])]),t._v(" "),s("tr",[s("td",[t._v("...")]),t._v(" "),s("td",[t._v("...")]),t._v(" "),s("td",[t._v("...")])]),t._v(" "),s("tr",[s("td",[t._v("[SEP]")]),t._v(" "),s("td",[t._v("None")]),t._v(" "),s("td",[t._v("特殊token，不属于任何单词")])])])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"实际应用场景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#实际应用场景"}},[t._v("#")]),t._v(" "),s("strong",[t._v("实际应用场景")])]),t._v(" "),s("h4",{attrs:{id:"_1-命名实体识别-ner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-命名实体识别-ner"}},[t._v("#")]),t._v(" "),s("strong",[t._v("1. 命名实体识别（NER）")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 假设我们有单词级别的标签")]),t._v("\nword_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"B-LOC"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将单词标签映射到token级别")]),t._v("\nword_ids "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntoken_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 特殊token")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Token labels:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" token_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"_2-对齐文本和分词结果"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-对齐文本和分词结果"}},[t._v("#")]),t._v(" "),s("strong",[t._v("2. 对齐文本和分词结果")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 重建原始单词与tokens的对应关系")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" defaultdict\n\nword_to_tokens "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokens_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convert_ids_to_tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nword_ids_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("token"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokens_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" word_ids_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        word_to_tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" token"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"单词到tokens的映射:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" token_info "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" word_to_tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    original_word "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"N/A"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"单词')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("original_word"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("): ")]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("token_info"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"_3-处理英文句子-更直观"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-处理英文句子-更直观"}},[t._v("#")]),t._v(" "),s("strong",[t._v("3. 处理英文句子（更直观）")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("english_sentence "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I love machine learning!"')]),t._v("\ntokenizer_en "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AutoTokenizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert-base-uncased"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntokens_en "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer_en"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("english_sentence"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"English tokens:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tokenizer_en"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convert_ids_to_tokens"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokens_en"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'input_ids'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"English word IDs:"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tokens_en"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出：")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# English tokens: ['[CLS]', 'i', 'love', 'machine', 'learning', '!', '[SEP]']")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# English word IDs: [None, 0, 1, 2, 2, 3, None]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注意：'machine learning' 被分成两个tokens，但属于同一个单词索引(2)")]),t._v("\n")])])]),s("hr"),t._v(" "),s("h3",{attrs:{id:"重要特性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#重要特性"}},[t._v("#")]),t._v(" "),s("strong",[t._v("重要特性")])]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("None 值")]),t._v("：特殊 tokens ([CLS], [SEP], [PAD]等) 的 word_id 为 None")]),t._v(" "),s("li",[s("strong",[t._v("相同数字")]),t._v("：属于同一个原始单词的所有 subwords 有相同的 word_id")]),t._v(" "),s("li",[s("strong",[t._v("连续编号")]),t._v("：word_id 从 0 开始按原始单词顺序编号")])]),t._v(" "),s("hr"),t._v(" "),s("h3",{attrs:{id:"在训练中的实际用途"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#在训练中的实际用途"}},[t._v("#")]),t._v(" "),s("strong",[t._v("在训练中的实际用途")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("align_labels_with_tokens")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""将单词级标签对齐到token级"""')]),t._v("\n    new_labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    current_word "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" word_ids"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            new_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 忽略特殊token（PyTorch中常用-100）")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" word_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" current_word"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 新单词的开始")]),t._v("\n            new_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            current_word "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" word_id\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 同一单词的后续subword（如##ing）")]),t._v("\n            new_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 或者使用相同的标签")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" new_labels\n")])])]),s("p",[s("strong",[t._v("总结")]),t._v("："),s("code",[t._v("word_ids()")]),t._v(" 是处理 "),s("strong",[t._v("分词对齐问题")]),t._v(" 的关键工具，特别在序列标注任务（NER、词性标注等）中不可或缺。")])])}),[],!1,null,null,null);s.default=e.exports}}]);