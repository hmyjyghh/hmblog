(window.webpackJsonp=window.webpackJsonp||[]).push([[147],{590:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("要计算出"),s("strong",[t._v("问答准确率提升 3.7%")]),t._v(" 这个量化结果，")]),t._v(" "),s("blockquote",[s("p",[t._v("核心是通过对比实验：在「数据优化前」和「数据优化后」")])]),t._v(" "),s("ol",[s("li",[t._v("分别测试"),s("strong",[t._v("相同的问答测试集")]),t._v("，")]),t._v(" "),s("li",[t._v("统计准确率，")]),t._v(" "),s("li",[t._v("再计算差值")])]),t._v(" "),s("p",[t._v("一、核心逻辑")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("准备标注好的测试集")]),t._v("：包含若干条真实用户 Query + 标准答案（Ground Truth），这是计算准确率的基准。")]),t._v(" "),s("li",[s("strong",[t._v("定义准确率计算规则")]),t._v("：明确 “回答正确” 的判定标准（如完全匹配、语义匹配、关键词匹配）。")]),t._v(" "),s("li",[s("strong",[t._v("对比实验")]),t._v("：\n"),s("ul",[s("li",[t._v("实验组：使用「Query 纠错 + 改写 + 扩展 + PDF 数据 DeepSeek 清洗」后的 RAG 系统；")]),t._v(" "),s("li",[t._v("对照组："),s("strong",[t._v("使用原始未优化数据的 RAG 系统")]),t._v("。")])])]),t._v(" "),s("li",[s("strong",[t._v("统计并计算提升率")]),t._v("：(实验组准确率 - 对照组准确率) = 提升百分比（如 3.7%）。")])]),t._v(" "),s("p",[t._v("二、关键前提")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("测试集要求")])])]),t._v(" "),s("ul",[s("li",[t._v("数量："),s("strong",[t._v("建议≥100 条")]),t._v("（样本太少会导致结果波动大）；")]),t._v(" "),s("li",[t._v("覆盖度：包含"),s("code",[t._v("常见 Query")]),t._v("、"),s("code",[t._v("边缘 Case")]),t._v("、"),s("code",[t._v("易解析错误的 PDF 相关 Query")]),t._v("；")]),t._v(" "),s("li",[t._v("标注：每条 Query 对应明确的 “标准答案”（可是文本、关键词列表、答案区间）。")])]),t._v(" "),s("ol",{attrs:{start:"2"}},[s("li",[s("strong",[t._v("准确率判定规则（可选其一，按需调整）")])])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("判定方式")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("适用场景")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("示例")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[t._v("完全匹配")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("答案固定（如事实性问题）")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("回答文本与 Ground Truth 完全一致")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[t._v("语义匹配")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("答案灵活（如解释类问题）")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("用 CosEmbedding/BERT 计算语义相似度≥0.8")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[t._v("关键词匹配")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("答案包含核心信息")]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("回答包含 Ground Truth 中 80% 以上关键词")])])])]),t._v(" "),s("p",[t._v("三、代码实现框架")]),t._v(" "),s("ol",[s("li",[t._v("安装依赖")])]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 基础依赖")]),t._v("\npip install pandas numpy scikit"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("learn\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 语义匹配可选（如用BERT计算相似度）")]),t._v("\npip install sentence"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("transformers\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[t._v("核心代码")])]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sentence_transformers "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SentenceTransformer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" util\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 1. 配置参数 =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载语义匹配模型（用于语义级准确率判定）")]),t._v("\nsemantic_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SentenceTransformer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'all-MiniLM-L6-v2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 测试集路径（需提前准备，格式：query, ground_truth）")]),t._v("\nTEST_SET_PATH "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"qa_test_set.csv"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 准确率判定阈值（语义匹配时使用）")]),t._v("\nSEMANTIC_THRESHOLD "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 2. 加载测试集 =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("load_test_set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""加载标注好的问答测试集"""')]),t._v("\n    df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 去重、过滤空值")]),t._v("\n    df "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("subset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'query'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ground_truth'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop_duplicates"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("subset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'query'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'records'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntest_set "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_test_set"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEST_SET_PATH"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"测试集加载完成，共')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_set"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('条样本"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 3. 定义准确率判定函数 =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("is_answer_correct")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ground_truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" judge_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"semantic"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    判断回答是否正确\n    :param pred_answer: RAG系统生成的回答\n    :param ground_truth: 标准答案\n    :param judge_type: 判定类型：exact（完全匹配）/ semantic（语义匹配）/ keyword（关键词匹配）\n    :return: True/False\n    """')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isna"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 完全匹配")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" judge_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"exact"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" ground_truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 语义匹配（推荐）")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" judge_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"semantic"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算语义相似度")]),t._v("\n        emb1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" semantic_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" convert_to_tensor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        emb2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" semantic_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("encode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ground_truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" convert_to_tensor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        similarity "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" util"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cos_sim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("emb1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" emb2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" similarity "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" SEMANTIC_THRESHOLD\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 关键词匹配")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" judge_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"keyword"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提取关键词（可自定义分词/去停用词）")]),t._v("\n        pred_keywords "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        gt_keywords "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ground_truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算关键词覆盖率")]),t._v("\n        coverage "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_keywords "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" gt_keywords"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gt_keywords"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gt_keywords"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" coverage "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 4. 定义RAG问答函数（核心：区分优化前/后） =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("rag_answer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" use_optimized_data"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    RAG系统问答逻辑\n    :param query: 用户问题\n    :param use_optimized_data: 是否使用优化后的数据（True=实验组，False=对照组）\n    :return: 生成的回答\n    """')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ---------------- 步骤1：Query处理（优化前/后） ----------------")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" use_optimized_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 优化后：Query纠错 + 改写 + 扩展（替换为你的实际逻辑）")]),t._v("\n        corrected_query "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" correct_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 纠错")]),t._v("\n        rewritten_query "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rewrite_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("corrected_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 改写")]),t._v("\n        expanded_query "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" expand_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rewritten_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 扩展")]),t._v("\n        final_query "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" expanded_query\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 优化前：原始Query")]),t._v("\n        final_query "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" query\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ---------------- 步骤2：文档检索（优化前/后） ----------------")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" use_optimized_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 优化后：使用DeepSeek清洗后的PDF文档库检索")]),t._v("\n        retrieved_docs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" retrieve_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("final_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc_lib"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"optimized_pdf_lib"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 优化前：使用原始PDF文档库检索")]),t._v("\n        retrieved_docs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" retrieve_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("final_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc_lib"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"original_pdf_lib"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ---------------- 步骤3：回答生成 ----------------")]),t._v("\n    answer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" generate_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("final_query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" retrieved_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 替换为你的LLM生成逻辑")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" answer\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 5. 替换为你的实际业务函数（需自定义） =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("correct_query")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Query纠错（如基于DeepSeek/拼写检查）"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 示例：替换为你的实际纠错逻辑")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" query  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 临时占位，需替换")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("rewrite_query")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Query改写（如更清晰、更完整）"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" query  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 临时占位，需替换")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("expand_query")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Query扩展（如同义词、相关问题）"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" query  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 临时占位，需替换")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("retrieve_docs")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" doc_lib"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""文档检索（替换为你的向量库检索逻辑）"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 临时占位，需替换")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("generate_answer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""LLM生成回答（替换为你的LLM调用逻辑）"""')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 临时占位，需替换")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 6. 对比实验：计算准确率 =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("calculate_accuracy")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("use_optimized_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""计算指定模式下的问答准确率"""')]),t._v("\n    correct_count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    total_count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_set"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" sample "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" test_set"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        query "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'query'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        ground_truth "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sample"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ground_truth'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用RAG获取回答")]),t._v("\n        pred_answer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rag_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("query"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" use_optimized_data"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("use_optimized_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断是否正确")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" is_answer_correct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_answer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ground_truth"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" judge_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"semantic"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            correct_count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    \n    accuracy "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("correct_count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" total_count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 转为百分比")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" accuracy\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算优化前准确率（对照组）")]),t._v("\naccuracy_before "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" calculate_accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("use_optimized_data"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算优化后准确率（实验组）")]),t._v("\naccuracy_after "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" calculate_accuracy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("use_optimized_data"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算提升率")]),t._v("\naccuracy_improvement "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" accuracy_after "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" accuracy_before\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===================== 7. 输出结果 =====================")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"优化前问答准确率：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy_before"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token format-spec"}},[t._v(".2f")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('%"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"优化后问答准确率：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy_after"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token format-spec"}},[t._v(".2f")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('%"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"问答准确率提升：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("accuracy_improvement"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token format-spec"}},[t._v(".2f")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('%"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 最终得到3.7%这类结果")]),t._v("\n")])])]),s("p",[t._v("四、关键细节说明")]),t._v(" "),s("ol",[s("li",[t._v("如何确保结果可信？")])]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("多次实验取平均")]),t._v("：单次实验可能受随机因素（如检索 TopK、LLM 随机性）影响，建议重复 3-5 次，取准确率平均值；")]),t._v(" "),s("li",[s("strong",[t._v("控制变量")]),t._v("：除了「数据优化」外，其他参数（如检索 TopK、LLM 温度、向量模型）保持一致；")]),t._v(" "),s("li",[s("strong",[t._v("分层统计")]),t._v("：可按 Query 类型（如事实类、推理类）、PDF 解析难度（如乱码、格式复杂）分层计算提升率，更精准。")])]),t._v(" "),s("ol",{attrs:{start:"2"}},[s("li",[t._v("若没有标注好的测试集？")])]),t._v(" "),s("ul",[s("li",[t._v("手动标注：选取 100-200 条真实用户 Query，"),s("code",[t._v("人工标注标准答案")]),t._v("；")]),t._v(" "),s("li",[t._v("自动标注（简易版）：对事实性问题，用权威文档中的答案作为 "),s("code",[t._v("Ground Truth")]),t._v("；")]),t._v(" "),s("li",[t._v("众包标注：通过平台获取标注数据（如 LabelStudio、百度众测）。")])]),t._v(" "),s("ol",{attrs:{start:"3"}},[s("li",[t._v("DeepSeek 数据清洗的代码集成")])]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("clean_pdf_docs_with_deepseek")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""使用DeepSeek清洗PDF解析后的杂乱文档"""')]),t._v("\n    cleaned_docs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    deepseek_api_key "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"你的DeepSeek API密钥"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" doc "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" raw_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造清洗提示词")]),t._v("\n        prompt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"""请清洗以下PDF解析后的文本，要求：\n        1. 修正解析错误（如乱码、错位、重复内容）；\n        2. 整理语句通顺性，删除无意义字符；\n        3. 保留核心信息，不丢失关键内容；\n        4. 输出清洗后的文本即可。\n        \n        待清洗文本：')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("doc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"""')])]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用DeepSeek API")]),t._v("\n        response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" requests"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("post"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            url"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://api.deepseek.com/v1/chat/completions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            headers"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Authorization"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string-interpolation"}},[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"Bearer ')]),s("span",{pre:!0,attrs:{class:"token interpolation"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("deepseek_api_key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            json"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"deepseek-chat"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"messages"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"role"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"user"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"content"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" prompt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"temperature"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 低温度保证稳定性")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        cleaned_doc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"choices"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"message"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"content"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        cleaned_docs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cleaned_doc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" cleaned_docs\n")])])]),s("p",[t._v("五、最终结果验证")]),t._v(" "),s("p",[t._v("假设你的实验结果为：")]),t._v(" "),s("ul",[s("li",[t._v("优化前准确率：85.0%")]),t._v(" "),s("li",[t._v("优化后准确率：88.7%")]),t._v(" "),s("li",[t._v("提升率：3.7%")])]),t._v(" "),s("p",[t._v("这就是「问答准确率提升 3.7%」的计算过程，核心是"),s("strong",[t._v("对比实验")]),t._v(" + "),s("strong",[t._v("标准化的准确率统计")]),t._v("。")]),t._v(" "),s("p",[t._v("总结：核心是"),s("strong",[t._v("控制变量的对比实验")]),t._v(" + "),s("strong",[t._v("明确的准确率判定规则")]),t._v("，")]),t._v(" "),s("blockquote",[s("p",[t._v("代码中需重点落地「Query 优化」「文档清洗」「准确率统计」三个环节，最终通过差值得到提升比例。")])])])}),[],!1,null,null,null);s.default=e.exports}}]);