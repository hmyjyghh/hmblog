(window.webpackJsonp=window.webpackJsonp||[]).push([[77],{559:function(t,v,_){"use strict";_.r(v);var e=_(3),n=Object(e.a)({},(function(){var t=this,v=t._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h3",{attrs:{id:"一句话总结"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#一句话总结"}},[t._v("#")]),t._v(" 一句话总结")]),t._v(" "),v("p",[v("strong",[t._v("vLLM 是一个专为大语言模型设计的高性能、低延迟的推理和服务化引擎。")]),t._v(" 它的核心目标就是："),v("strong",[t._v("用同样大小的显卡，让 LLM 服务同时处理更多用户的请求")]),t._v("，从而极大地降低成本、提高效率。")]),t._v(" "),v("p",[t._v("您可以把它想象成 LLM 世界的 "),v("strong",[t._v("“超级服务器”")]),t._v("。")]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"核心价值-解决-内存墙-问题"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#核心价值-解决-内存墙-问题"}},[t._v("#")]),t._v(" 核心价值：解决“内存墙”问题")]),t._v(" "),v("p",[t._v("在 vLLM 出现之前，像使用 Hugging Face Transformers 这样的库来部署 LLM 服务时，会遇到一个巨大的瓶颈——"),v("strong",[t._v("KV Cache 的内存管理效率极低")]),t._v("。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("什么是 KV Cache？")]),t._v(" LLM 在生成文本（如回答问题）时，是一个字一个字地蹦出来的。每生成下一个字，它都需要参考之前已经生成的所有字的信息。为了避免重复计算，系统会把中间计算结果（称为 Key 和 Value 向量）缓存起来，这个缓存就是 "),v("strong",[t._v("KV Cache")]),t._v("。")]),t._v(" "),v("li",[v("strong",[t._v("问题所在")]),t._v("：传统的系统会为每个用户请求"),v("strong",[t._v("预留一大块连续内存")]),t._v("来存放 KV Cache，以防生成过程很长。但实际上，大部分请求可能很短，这就导致了大量内存被白浪费（内部碎片）。同时，频繁的分配和释放也会产生内存空洞（外部碎片）。")])]),t._v(" "),v("p",[v("strong",[t._v("这就好比：")])]),t._v(" "),v("blockquote",[v("p",[t._v("一个停车场，每当来一辆车，就为它预留 10 个连续车位，因为它“可能”会来一群朋友。结果大部分车都只有司机一人，导致 9 个车位空着，其他车也停不进来。停车场看似满了，但实际利用率很低。")])]),t._v(" "),v("p",[t._v("vLLM 就是为了解决这个“停车场”效率问题而生的。")]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"vllm-的杀手锏-pagedattention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#vllm-的杀手锏-pagedattention"}},[t._v("#")]),t._v(" vLLM 的杀手锏：PagedAttention")]),t._v(" "),v("p",[t._v("vLLM 的核心技术是它创新的内存管理算法——"),v("strong",[t._v("PagedAttention")]),t._v("。这个技术的灵感来自于"),v("strong",[t._v("操作系统的虚拟内存和分页机制")]),t._v("。")]),t._v(" "),v("p",[v("strong",[t._v("它的工作方式如下：")])]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("分块")]),t._v("：vLLM 将 GPU 显存划分为许多个"),v("strong",[t._v("固定大小的内存块")]),t._v("，就像停车场划好了很多个标准车位。")]),t._v(" "),v("li",[v("strong",[t._v("按需分配")]),t._v("：当一个用户请求到来时，系统不再一次性预留大量空间，而是按实际需要，每次分配一个或多个"),v("strong",[t._v("内存块")]),t._v("给它。生成了多少内容，就用多少块。")]),t._v(" "),v("li",[v("strong",[t._v("非连续但逻辑连续")]),t._v("：这些块在物理上可能是不连续的，但 vLLM 内部通过一个“映射表”来管理，让模型在计算时感觉像是在访问一块连续的内存。")])]),t._v(" "),v("p",[v("strong",[t._v("继续用停车场的比喻：")])]),t._v(" "),v("blockquote",[v("p",[t._v("vLLM 管理的停车场不再为每辆车预留固定连续车位。来了一辆车，就给它一个车位。如果它需要接人（生成长文本），再来一辆车，就再给一个车位，这个车位可能在停车场的任何角落，但系统通过一个“智能管理员”（块表）记得所有这些车是属于同一个“车队”的。这样就最大限度地利用了每一个车位。")])]),t._v(" "),v("h3",{attrs:{id:"vllm-的主要特点和优势"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#vllm-的主要特点和优势"}},[t._v("#")]),t._v(" vLLM 的主要特点和优势")]),t._v(" "),v("ol",[v("li",[v("strong",[t._v("极高的吞吐量")]),t._v("：通过 PagedAttention，vLLM 的显存利用率接近 100%，几乎没有浪费。这意味着在同样大小的显卡上，它可以同时处理比传统方案（如 Hugging Face）多得多的高并发请求。官方数据显示，吞吐量"),v("strong",[t._v("最高可提升 24 倍")]),t._v("。")]),t._v(" "),v("li",[v("strong",[t._v("高效的内存共享")]),t._v("：对于需要生成多个候选结果（如并行采样、集束搜索）的场景，vLLM 可以让这些候选序列"),v("strong",[t._v("共享")]),t._v("它们共同前缀的 KV Cache，避免了重复存储，进一步节省了显存。")]),t._v(" "),v("li",[v("strong",[t._v("易于使用")]),t._v("：\n"),v("ul",[v("li",[t._v("它与流行的 Hugging Face 模型完全兼容，通常只需要改动几行代码就能将现有服务切换到 vLLM。")]),t._v(" "),v("li",[t._v("它提供了与 "),v("strong",[t._v("OpenAI API 完全兼容的接口")]),t._v("，这意味着任何可以使用 OpenAI API 的客户端应用都可以无缝切换到私有的 vLLM 服务。")])])]),t._v(" "),v("li",[v("strong",[t._v("支持连续批处理")]),t._v("：当一个批次中的某些请求先完成时，vLLM 会立即释放其资源并接入新的等待请求，让 GPU 时刻保持忙碌，进一步提升了计算效率。")])]),t._v(" "),v("hr"),t._v(" "),v("h3",{attrs:{id:"总结"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),v("p",[v("strong",[t._v("vLLM 是什么？")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("角色上")]),t._v("：它是一个 LLM 推理加速器和部署框架。")]),t._v(" "),v("li",[v("strong",[t._v("核心上")]),t._v("：它通过革命性的 "),v("strong",[t._v("PagedAttention")]),t._v(" 算法，极致地优化了 LLM 推理过程中最耗资源的 KV Cache 的内存管理。")]),t._v(" "),v("li",[v("strong",[t._v("效果上")]),t._v("：它极大地提升了 GPU 的利用率和服务的吞吐量，降低了单位请求的成本。")]),t._v(" "),v("li",[v("strong",[t._v("生态上")]),t._v("：它简单易用，兼容性强，是目前业界部署高性能 LLM 服务的"),v("strong",[t._v("事实标准")]),t._v("。")])]),t._v(" "),v("p",[t._v("无论是公司还是研究者，当他们需要将一个大模型（如 LLaMA, ChatGLM, Qwen 等）高效、低成本地提供给大量用户使用时，vLLM 通常是他们的首选方案。")]),t._v(" "),v("h2",{attrs:{id:"_2-它的底层原理-pagedattention"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-它的底层原理-pagedattention"}},[t._v("#")]),t._v(" 2. 它的底层原理: PagedAttention")]),t._v(" "),v("p",[t._v("好的，当然了解。vLLM的核心理念和巨大成功，很大程度上都归功于其创新的底层算法——"),v("strong",[t._v("PagedAttention")]),t._v("。")]),t._v(" "),v("p",[t._v("我们可以把它理解为一个“神来之笔”的类比："),v("strong",[t._v("就像操作系统（OS）管理物理内存一样，vLLM用PagedAttention来管理KV Cache。")])]),t._v(" "),v("p",[t._v("下面我为你详细拆解一下它的原理。")]),t._v(" "),v("h3",{attrs:{id:"_1-背景-llm推理的瓶颈-kv-cache"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-背景-llm推理的瓶颈-kv-cache"}},[t._v("#")]),t._v(" 1. 背景：LLM推理的瓶颈——KV Cache")]),t._v(" "),v("p",[t._v("在详细讲解PagedAttention之前，必须先理解它要解决的核心问题。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("自回归生成")]),t._v("：LLM（大语言模型）生成文本是一个一个token进行的。生成下一个token时，需要基于之前所有已生成的token。")]),t._v(" "),v("li",[v("strong",[t._v("KV Cache")]),t._v("：为了避免在生成每个新token时都重新计算所有之前token的Key和Value向量（Transformer Decoder层的核心），人们引入了KV Cache。它将之前所有token的K和V向量缓存起来，这样在计算下一个token时，只需要计算当前token的K、V，然后与缓存中的K、V一起进行Attention计算即可。")])]),t._v(" "),v("p",[v("strong",[t._v("这带来了巨大的性能提升，但也引入了一个新问题：内存管理。")])]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("内存浪费")]),t._v("：在传统的服务系统中，每个请求（序列）的KV Cache在内存中都是连续分配的。\n"),v("ul",[v("li",[v("strong",[t._v("内部碎片化")]),t._v("：由于序列生成长度不确定，你不得不为每个序列预留一个“可能的最大长度”的内存。比如，你预设最大生成长度为2048，但很多序列可能只生成了500个token，那么剩下的1548个token位置的内存就被浪费了。")]),t._v(" "),v("li",[v("strong",[t._v("外部碎片化")]),t._v("：序列有生有灭，不断分配和释放这些连续的大内存块，会在内存中产生很多“空洞”，导致即使总空闲内存足够，也无法分配一个新的连续大内存块。")])])])]),t._v(" "),v("p",[v("strong",[t._v("正是这个内存管理问题，严重限制了LLM服务的吞吐量")]),t._v("。")]),t._v(" "),v("h3",{attrs:{id:"_2-pagedattention-灵感来自操作系统的虚拟内存和分页"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-pagedattention-灵感来自操作系统的虚拟内存和分页"}},[t._v("#")]),t._v(" 2. PagedAttention：灵感来自操作系统的虚拟内存和分页")]),t._v(" "),v("p",[t._v("vLLM的开发者从操作系统的虚拟内存和分页机制中获得了灵感，并巧妙地将其应用到了KV Cache的管理上。")]),t._v(" "),v("p",[t._v("它的核心思想是：")]),t._v(" "),v("p",[v("strong",[t._v("将每个序列的KV Cache逻辑上视为一个连续空间，但物理上分割成多个非连续的、固定大小的“块”（Block）。")])]),t._v(" "),v("p",[t._v("我们来对照操作系统的概念来理解：")]),t._v(" "),v("table",[v("thead",[v("tr",[v("th",{staticStyle:{"text-align":"left"}},[t._v("概念")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("操作系统")]),t._v(" "),v("th",{staticStyle:{"text-align":"left"}},[t._v("vLLM (PagedAttention)")])])]),t._v(" "),v("tbody",[v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("虚拟内存")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("进程看到的连续、独立的地址空间")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("一个序列"),v("strong",[t._v("逻辑上")]),t._v("的KV Cache")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("物理内存")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("实际的、不连续的物理内存条")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("GPU的"),v("strong",[t._v("连续显存池")])])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("页")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("固定大小的内存块（如4KB）")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("固定大小的"),v("strong",[t._v("块")]),t._v("，可存储一定数量token的K和V向量（如16个token）")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("页表")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("记录虚拟页到物理页的映射关系")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("块表")]),t._v("，记录序列的逻辑块到物理块的映射")])]),t._v(" "),v("tr",[v("td",{staticStyle:{"text-align":"left"}},[v("strong",[t._v("缺页中断")])]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("访问的页不在物理内存中")]),t._v(" "),v("td",{staticStyle:{"text-align":"left"}},[t._v("（在vLLM中基本避免，因为管理在显存内）")])])])]),t._v(" "),v("h3",{attrs:{id:"_3-pagedattention-的工作流程"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-pagedattention-的工作流程"}},[t._v("#")]),t._v(" 3. PagedAttention 的工作流程")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("初始化内存池")]),t._v("：\nvLLM启动时，会向GPU显存申请一个大的、连续的内存池，并将其划分为大量"),v("strong",[t._v("大小相等的块")]),t._v("。每个块可以容纳固定数量的token（例如，"),v("code",[t._v("block_size=16")]),t._v("）的K和V向量。")])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("序列生成与块分配")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("当一个新的序列请求到来时，系统会为它创建一个"),v("strong",[t._v("逻辑上的“块表”")]),t._v("。")]),t._v(" "),v("li",[t._v("生成第一个token时，系统从"),v("strong",[t._v("空闲块列表")]),t._v("中分配一个"),v("strong",[t._v("物理块")]),t._v("给它，并将映射关系记录在块表中。这个块可以存储前16个token的KV Cache。")]),t._v(" "),v("li",[t._v("当序列长度超过16时，系统再分配一个新的物理块，并更新块表。这个新块在物理上可能与第一个块不连续，但在逻辑上，序列认为它们是连续的。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("Attention计算")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("当需要进行Attention计算时（例如，生成第17个token），vLLM的PagedAttention内核会根据该序列的"),v("strong",[t._v("块表")]),t._v("，去不同的物理块中 gathering（收集）所需的K和V向量。")]),t._v(" "),v("li",[t._v("然后，将这些收集来的K、V向量与当前token的Q向量一起进行Attention计算。")]),t._v(" "),v("li",[t._v("这个过程对模型来说是透明的，它仍然“感觉”自己在访问一个连续的KV Cache。")])])])]),t._v(" "),v("h3",{attrs:{id:"_4-pagedattention-带来的巨大优势"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-pagedattention-带来的巨大优势"}},[t._v("#")]),t._v(" 4. PagedAttention 带来的巨大优势")]),t._v(" "),v("ol",[v("li",[v("p",[v("strong",[t._v("近乎零内存浪费")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("内存分配单位从“整个序列可能的最大长度”变成了“一个小的块”。")]),t._v(" "),v("li",[t._v("序列需要多长，就分配多少个块，最后一个块没满也没关系，浪费的空间很小（内部碎片化极低）。")]),t._v(" "),v("li",[t._v("所有序列共享同一个物理块池，块可以被高效复用，避免了外部碎片化。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("实现高效的内存共享")]),t._v("：\n这是PagedAttention另一个杀手级特性，尤其对于"),v("strong",[t._v("并行采样")]),t._v("和"),v("strong",[t._v("Beam Search")]),t._v("。")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("场景")]),t._v("：多个序列可能共享同一个前缀（例如，同一个问题的多个回答）。")]),t._v(" "),v("li",[v("strong",[t._v("实现")]),t._v("：在PagedAttention中，这些序列可以"),v("strong",[t._v("直接共享存储前缀token的物理块")]),t._v("。系统只需在它们的块表中记录指向"),v("strong",[t._v("同一个物理块")]),t._v("的映射即可。")]),t._v(" "),v("li",[v("strong",[t._v("好处")]),t._v("：避免了重复存储共享前缀的KV Cache，节省了大量显存。这在传统连续分配方案中很难高效实现。")])])]),t._v(" "),v("li",[v("p",[v("strong",[t._v("高吞吐量")]),t._v("：\n正是由于极佳的内存利用率，vLLM可以在相同的GPU显存下，同时处理"),v("strong",[t._v("更多并发请求")]),t._v("，从而极大地提升了"),v("strong",[t._v("吞吐量")]),t._v("。官方数据显示，在某些场景下，吞吐量比HuggingFace Transformers高出24倍。")])])]),t._v(" "),v("h3",{attrs:{id:"总结-2"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结-2"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),v("p",[v("strong",[t._v("PagedAttention是vLLM的灵魂所在。")])]),t._v(" "),v("p",[t._v("它通过将操作系统成熟的内存分页管理思想引入到LLM推理中最耗资源的KV Cache管理中，巧妙地解决了内存碎片化和利用率低下的核心痛点。它不仅减少了浪费，还通过内存共享机制进一步优化，最终使得LLM服务能够在有限的GPU显存下实现前所未有的高吞吐量，成为了LLM部署领域一个里程碑式的技术。")])])}),[],!1,null,null,null);v.default=n.exports}}]);