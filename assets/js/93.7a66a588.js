(window.webpackJsonp=window.webpackJsonp||[]).push([[93],{535:function(t,_,v){"use strict";v.r(_);var r=v(3),e=Object(r.a)({},(function(){var t=this,_=t._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h3",{attrs:{id:"rnn-循环神经网络-是什么"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#rnn-循环神经网络-是什么"}},[t._v("#")]),t._v(" RNN（循环神经网络）是什么？")]),t._v(" "),_("p",[t._v("RNN是一类专门用于处理"),_("strong",[t._v("序列数据")]),t._v("的人工神经网络。所谓序列数据，就是指数据点之间存在顺序依赖关系，比如：")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("自然语言")]),t._v("：一句话中的单词是一个接一个的。")]),t._v(" "),_("li",[_("strong",[t._v("时间序列数据")]),t._v("：股票价格、传感器读数、音频波形等，每个数据点都与时间相关。")]),t._v(" "),_("li",[_("strong",[t._v("DNA序列")]),t._v("：碱基的排列顺序。")])]),t._v(" "),_("h4",{attrs:{id:"rnn的核心思想-循环与记忆"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#rnn的核心思想-循环与记忆"}},[t._v("#")]),t._v(" RNN的核心思想：循环与记忆")]),t._v(" "),_("p",[t._v("RNN的核心设计是一个"),_("strong",[t._v("循环连接")]),t._v("，这使得网络能够具备“记忆”能力。")]),t._v(" "),_("ol",[_("li",[_("strong",[t._v("隐藏状态")]),t._v("：RNN内部有一个“隐藏状态”（hidden state），你可以把它理解为网络的"),_("strong",[t._v("短期记忆")]),t._v("。这个状态会随着序列的每一步而更新。")]),t._v(" "),_("li",[_("strong",[t._v("循环过程")]),t._v("：在处理序列时，RNN不是独立地处理每个元素，而是"),_("strong",[t._v("按顺序一个一个地处理")]),t._v("。\n"),_("ul",[_("li",[t._v("在每一步 "),_("code",[t._v("t")]),t._v("，网络会接收"),_("strong",[t._v("两个输入")]),t._v("：\n"),_("ul",[_("li",[t._v("当前时间步的输入数据（例如，句子中的第 "),_("code",[t._v("t")]),t._v(" 个词）。")]),t._v(" "),_("li",[t._v("上一个时间步的隐藏状态 "),_("code",[t._v("h_{t-1}")]),t._v("（即处理上一个词时的记忆）。")])])]),t._v(" "),_("li",[t._v("网络结合这两个输入，计算出：\n"),_("ul",[_("li",[t._v("当前时间步的输出（如果需要）。")]),t._v(" "),_("li",[t._v("更新后的"),_("strong",[t._v("当前隐藏状态 "),_("code",[t._v("h_t")])]),t._v("，这个状态会被传递给下一个时间步 "),_("code",[t._v("t+1")]),t._v("。")])])])])])]),t._v(" "),_("h4",{attrs:{id:"rnn的简单公式表示"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#rnn的简单公式表示"}},[t._v("#")]),t._v(" RNN的简单公式表示：")]),t._v(" "),_("p",[_("code",[t._v("h_t = f(W * x_t + U * h_{t-1} + b)")]),t._v("\n其中：")]),t._v(" "),_("ul",[_("li",[_("code",[t._v("h_t")]),t._v(" 是当前时刻的隐藏状态（记忆）。")]),t._v(" "),_("li",[_("code",[t._v("x_t")]),t._v(" 是当前时刻的输入。")]),t._v(" "),_("li",[_("code",[t._v("h_{t-1}")]),t._v(" 是上一时刻的隐藏状态。")]),t._v(" "),_("li",[_("code",[t._v("W")]),t._v(", "),_("code",[t._v("U")]),t._v(" 是权重矩阵，"),_("code",[t._v("b")]),t._v(" 是偏置项。")]),t._v(" "),_("li",[_("code",[t._v("f")]),t._v(" 是一个激活函数（如tanh或ReLU）。")])]),t._v(" "),_("h3",{attrs:{id:"rnn的优势与致命缺陷"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#rnn的优势与致命缺陷"}},[t._v("#")]),t._v(" RNN的优势与致命缺陷")]),t._v(" "),_("p",[_("strong",[t._v("优势")]),t._v("：")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("天生为序列设计")]),t._v("：可以处理任意长度的序列。")]),t._v(" "),_("li",[_("strong",[t._v("共享参数")]),t._v("：在所有时间步共享相同的权重（"),_("code",[t._v("W")]),t._v(", "),_("code",[t._v("U")]),t._v("），大大减少了参数量。")])]),t._v(" "),_("p",[_("strong",[t._v("致命缺陷")]),t._v("：")]),t._v(" "),_("ol",[_("li",[_("strong",[t._v("梯度消失/爆炸问题")]),t._v("：在处理长序列时，梯度（用于更新权重的信号）在反向传播过程中需要通过许多时间步。这会导致梯度变得极小（消失）或极大（爆炸），使得网络难以学习到长距离的依赖关系（例如，句首的词对句尾的词的影响）。虽然LSTM和GRU等变体缓解了这个问题，但并未根本解决。")]),t._v(" "),_("li",[_("strong",[t._v("顺序计算，无法并行")]),t._v("：由于必须等待 "),_("code",[t._v("t-1")]),t._v(" 步的计算完成后才能计算第 "),_("code",[t._v("t")]),t._v(" 步，计算过程是串行的。这导致训练速度非常慢，无法充分利用现代GPU强大的并行计算能力。")])]),t._v(" "),_("h3",{attrs:{id:"回到transformer-由于不是循环-rnn-结构"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#回到transformer-由于不是循环-rnn-结构"}},[t._v("#")]),t._v(" 回到Transformer：“由于不是循环（RNN）结构”")]),t._v(" "),_("p",[t._v("现在再来看Transformer的这句话，它的含义就非常清晰了：")]),t._v(" "),_("blockquote",[_("p",[t._v("Transformer的作者完全摒弃了RNN的这种"),_("strong",[t._v("顺序处理")]),t._v("的循环结构，转而采用了一种全新的"),_("strong",[t._v("自注意力机制")]),t._v("来构建模型。")])]),t._v(" "),_("p",[_("strong",[t._v("Transformer是如何解决RNN问题的？")])]),t._v(" "),_("ol",[_("li",[_("strong",[t._v("完全并行化")]),t._v("：Transformer一次性将整个序列输入模型。通过"),_("strong",[t._v("自注意力机制")]),t._v("，"),_("code",[t._v("序列中的任何一个词都可以直接与任何另一个词进行交互和计算（“注意力”），")]),t._v("而不需要像RNN那样一步一步地传递。这极大地提高了训练速度。")]),t._v(" "),_("li",[_("strong",[t._v("高效捕获长距离依赖")]),t._v("：自注意力机制计算的是词与词之间的“直接关联”，无论它们在序列中相隔多远，这种关联的计算难度都是一样的。这彻底解决了RNN因长距离传递而导致的梯度消失问题。")])]),t._v(" "),_("p",[_("strong",[t._v("简单比喻")]),t._v("：")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("RNN")]),t._v("：像是一个"),_("strong",[t._v("传话游戏")]),t._v("。第一个人说给第二个人听，第二个人再结合自己的理解说给第三个人听……信息在一步步传递中很容易丢失或扭曲。")]),t._v(" "),_("li",[_("strong",[t._v("Transformer")]),t._v("：像是"),_("strong",[t._v("开小组会议")]),t._v("。每个人同时都能听到所有其他人的发言，然后直接基于所有人的信息形成自己的观点。信息交换是直接且并行的。")])]),t._v(" "),_("h3",{attrs:{id:"总结"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),_("table",[_("thead",[_("tr",[_("th",{staticStyle:{"text-align":"left"}},[t._v("特性")]),t._v(" "),_("th",{staticStyle:{"text-align":"left"}},[t._v("RNN (及LSTM/GRU)")]),t._v(" "),_("th",{staticStyle:{"text-align":"left"}},[t._v("Transformer")])])]),t._v(" "),_("tbody",[_("tr",[_("td",{staticStyle:{"text-align":"left"}},[_("strong",[t._v("核心机制")])]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("循环连接，顺序处理")]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("自注意力机制，并行处理")])]),t._v(" "),_("tr",[_("td",{staticStyle:{"text-align":"left"}},[_("strong",[t._v("计算方式")])]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("串行，无法并行化")]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("高度并行化，训练速度快")])]),t._v(" "),_("tr",[_("td",{staticStyle:{"text-align":"left"}},[_("strong",[t._v("长距离依赖")])]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("难以处理，存在梯度消失/爆炸")]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("轻松处理，直接计算任意两词关系")])]),t._v(" "),_("tr",[_("td",{staticStyle:{"text-align":"left"}},[_("strong",[t._v("记忆")])]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("通过隐藏状态传递，是"),_("strong",[t._v("压缩的、渐进式的")])]),t._v(" "),_("td",{staticStyle:{"text-align":"left"}},[t._v("通过注意力权重实现，是"),_("strong",[t._v("直接的、全局的")])])])])]),t._v(" "),_("p",[t._v("因此，“由于不是循环（RNN）结构”这句话，正是Transformer模型革命性的起点。它通过抛弃RNN的序列结构，转而利用自注意力，同时实现了"),_("strong",[t._v("更高的并行度、更快的训练速度以及更强大的捕获长距离依赖关系的能力")]),t._v("。")])])}),[],!1,null,null,null);_.default=e.exports}}]);