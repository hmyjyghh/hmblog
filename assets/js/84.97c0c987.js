(window.webpackJsonp=window.webpackJsonp||[]).push([[84],{528:function(t,s,a){"use strict";a.r(s);var n=a(3),r=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"手撕transformer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#手撕transformer"}},[t._v("#")]),t._v(" 手撕Transformer")]),t._v(" "),s("h2",{attrs:{id:"_1-add-norm-残差和标准化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-add-norm-残差和标准化"}},[t._v("#")]),t._v(" 1. Add & Norm (残差和标准化)")]),t._v(" "),s("p",[t._v("操作步骤：")]),t._v(" "),s("ul",[s("li",[t._v("残差连接：将输入直接与输出相加。")]),t._v(" "),s("li",[t._v("层归一化：对相加后的结果进行归一化。")])]),t._v(" "),s("h2",{attrs:{id:"_1-位置编码"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-位置编码"}},[t._v("#")]),t._v(" 1. 位置编码")]),t._v(" "),s("h3",{attrs:{id:"_1-为什么需要位置编码"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-为什么需要位置编码"}},[t._v("#")]),t._v(" 1. 为什么需要位置编码?")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("在 Transformer 模型中，由于不是循环（RNN）结构，模型本身无法捕捉输入序列中元素的位置信息。回顾一下注意力机制的计算过程，**得分（score）**是通过查询向量（query）和键向量（key）之间的内积得到的，生成的注意力权重（attention weights）也只是基于这些内积结果，这个操作不会捕捉到位置信息。")])]),t._v(" "),s("li",[s("p",[t._v("举个例子，把序列 "),s("code",[t._v('["A", "B", "C"]')]),t._v("改成 "),s("code",[t._v('["B", "A", "C"]')]),t._v("，得到的输出也会是原来的结果按同样顺序打乱后的形式，假设原输出为 "),s("code",[t._v("[Z_A, Z_B, Z_C]")]),t._v("，打乱后的输出将变为 "),s("code",[t._v("[Z_B, Z_A, Z_C]")]),t._v("。")])]),t._v(" "),s("li",[s("p",[t._v("所以如果嵌入向量本身不包含位置信息，就意味着输入元素的顺序不会影响输出的权重计算，模型无法从中捕捉到序列的顺序信息，换句话说，只是输出的位置跟着对应变化，但对应的计算结果不会改变。")])])]),t._v(" "),s("blockquote",[s("p",[t._v("为了解决这个问题，Transformer 引入了位置编码（Positional Encoding）：为每个位置生成一个向量，这个向量与对应的嵌入向量相加，从而在输入中嵌入位置信息。")])]),t._v(" "),s("h3",{attrs:{id:"_2-位置编码的代码实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-位置编码的代码实现"}},[t._v("#")]),t._v(" 2. 位置编码的代码实现")]),t._v(" "),s("div",{staticClass:"language-py extra-class"},[s("pre",{pre:!0,attrs:{class:"language-py"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" math\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("PositionalEncoding")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dropout"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_len"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        位置编码，为输入序列中的每个位置添加唯一的位置表示，以引入位置信息。\n\n        参数:\n            d_model: 嵌入维度，即每个位置的编码向量的维度。\n            dropout: 位置编码后应用的 Dropout 概率。\n            max_len: 位置编码的最大长度，适应不同长度的输入序列。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("PositionalEncoding"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 正如论文 5.4 节所提到的，需要将 Dropout 应用在 embedding 和 positional encoding 相加的时候")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建位置编码矩阵，形状为 (max_len, d_model)")]),t._v("\n        pe "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("max_len"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        position "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_len"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 位置索引 (max_len, 1)")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算每个维度对应的频率")]),t._v("\n        div_term "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("math"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" d_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将位置和频率结合，计算 sin 和 cos")]),t._v("\n        pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sin"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" div_term"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 偶数维度")]),t._v("\n        pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("position "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" div_term"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 奇数维度")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增加一个维度，方便后续与输入相加，形状变为 (1, max_len, d_model)")]),t._v("\n        pe "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将位置编码注册为模型的缓冲区，不作为参数更新")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register_buffer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pe'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        前向传播函数。\n\n        参数:\n            x: 输入序列的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n\n        返回:\n            加入位置编码和 Dropout 后的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n        """')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 取出与输入序列长度相同的部分位置编码，并与输入相加")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        \n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 应用 dropout")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"_2-多头注意力-multi-head-attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-多头注意力-multi-head-attention"}},[t._v("#")]),t._v(" 2. 多头注意力 Multi-Head Attention")]),t._v(" "),s("h3",{attrs:{id:"_1-多头注意力的实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-多头注意力的实现"}},[t._v("#")]),t._v(" 1. 多头注意力的实现")]),t._v(" "),s("h3",{attrs:{id:"_2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2"}},[t._v("#")]),t._v(" 2.")]),t._v(" "),s("h2",{attrs:{id:"_3-前馈神经网络-feed-forward-network"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-前馈神经网络-feed-forward-network"}},[t._v("#")]),t._v(" 3. 前馈神经网络 Feed Forward Network")]),t._v(" "),s("p",[t._v("前馈神经网络接收"),s("strong",[t._v("来自注意力机制层的输出结果")]),t._v("，随后对该输出执行进一步的线性变换.")]),t._v(" "),s("ul",[s("li",[t._v("通过这种方式，网络能够"),s("strong",[t._v("深入挖掘并捕获更为复杂、抽象的特征")]),t._v(".")])]),t._v(" "),s("h3",{attrs:{id:"_1-前馈神经网络的实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-前馈神经网络的实现"}},[t._v("#")]),t._v(" 1. 前馈神经网络的实现")]),t._v(" "),s("blockquote",[s("p",[t._v("前馈神经网络的实现比较简单，具体可以参考 "),s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/34871928",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer 前馈神经网络"),s("OutboundLink")],1),t._v("。")])]),t._v(" "),s("h3",{attrs:{id:"_2-前馈神经网络的作用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-前馈神经网络的作用"}},[t._v("#")]),t._v(" 2. 前馈神经网络的作用")]),t._v(" "),s("p",[t._v("前馈神经网络的作用是对输入进行非线性变换，从而提取输入的特征。在 Transformer 模型中，前馈神经网络的作用是对每个位置的嵌入向量进行变换，从而提取位置信息。")]),t._v(" "),s("blockquote",[s("p",[t._v("Transformer 已成为语言模型领域的奠基之作，大幅推动了自然语言处理（NLP，Natural Language Processing）的发展。")])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/PaperNotes/Transformer%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.md#rnn-%E7%9A%84%E9%80%92%E5%BD%92%E5%8E%9F%E7%90%86",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transfromer 论文精读"),s("OutboundLink")],1)]),t._v(" "),s("blockquote",[s("p",[t._v("Transformer 直接放弃 "),s("code",[t._v("RNN")]),t._v(" 的递归结构，只使用注意力机制来编码和解码序列信息。这相当于，大家原本都端着碗（RNN）拿着筷子（Attention）吃饭，而 Transformer 直接把“碗”扔掉，表示只用筷子也能直接吃，而且吃得又快又好，还不用装饭。展示了一种全新的思路：Attention Is All You Need。")])]),t._v(" "),s("p",[t._v("Transformer 的主要贡献如下：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("取消递归结构，实现并行计算")])]),t._v(" "),s("li",[s("p",[t._v("通过采用自注意力机制（Self-Attention），Transformer 可以同时处理多个输入序列，极大提高了计算的并行度和训练速度。")])]),t._v(" "),s("li",[s("p",[t._v("引入位置编码（Positional Encoding）并结合 Attention 机制巧妙地捕捉位置信息")])]),t._v(" "),s("li",[s("p",[t._v("在不依赖 RNN 结构的情况下，通过位置编码为序列中的每个元素嵌入位置信息，从而使模型能够感知输入的顺序。")])])]),t._v(" "),s("p",[t._v("Transformer 的核心是多头注意力机制（Multi-Head Attention），它将输入序列的每个位置的嵌入向量映射到多个不同的空间表示，每个空间表示对应一个注意力头（Attention Head）。每个注意力头都有自己的查询向量（Query）、键向量（Key）和值向量（Value），通过计算查询向量与键向量的内积，得到注意力权重（Attention Weight），再将权重与值向量进行加权求和，得到注意力输出（Attention Output）。")]),t._v(" "),s("p",[t._v("多头注意力机制的作用是允许模型同时关注输入序列中的不同位置，从而提取更丰富的特征表示。")]),t._v(" "),s("p",[t._v("前馈神经网络的作用是对每个位置的嵌入向量进行变换，从而提取位置信息。")]),t._v(" "),s("p",[t._v("位置编码的作用是为每个位置添加唯一的位置表示，以引入位置信息。")]),t._v(" "),s("p",[t._v("通过将多头注意力机制、前馈神经网络和位置编码结合起来，Transformer 模型能够同时考虑输入序列的内容和位置信息，从而实现高效的序列建模。")]),t._v(" "),s("h1",{attrs:{id:"注意要看-速览疑问"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#注意要看-速览疑问"}},[t._v("#")]),t._v(" 注意要看  速览疑问")])])}),[],!1,null,null,null);s.default=r.exports}}]);